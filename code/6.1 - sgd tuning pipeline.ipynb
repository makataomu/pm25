{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import njit\n",
    "from window_ops.expanding import expanding_mean\n",
    "from window_ops.rolling import rolling_mean\n",
    "\n",
    "@njit\n",
    "def rolling_mean_14(x):\n",
    "    return rolling_mean(x, window_size=14)\n",
    "@njit\n",
    "def rolling_mean_30(x):\n",
    "    return rolling_mean(x, window_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_df_to_mlforecast(df, date_col, target_col, unique_id='mean'):\n",
    "    df_ = df.rename({\n",
    "        date_col: \"ds\",\n",
    "        # target_col: 'y',\n",
    "    }, axis=1)\n",
    "\n",
    "    df_['ds'] = pd.to_datetime(df_['ds'])\n",
    "\n",
    "    df_['y'] = df_[target_col].copy()\n",
    "    # df_.drop(columns=target_col)\n",
    "\n",
    "    df_['unique_id'] = unique_id\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_sensors_df = pd.read_csv(\"../data/selected_sensors2_cleaned.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_START_DATE = \"2019-04-02\"\n",
    "scenarios_sensors = {\n",
    "    # 0: 1, 4372603\n",
    "    # \"0_12M_train_7M_test\": {\"train_start\": \"2017-03-25\", \"train_end\": \"2018-03-25\", \"test_start\": \"2018-03-26\", \"test_end\": \"2018-10-10\"},\n",
    "    '2': {\n",
    "        # \"18M_train\":  {\"train_start\": \"2017-04-01\", \"train_end\": \"2018-10-01\"},\n",
    "        # \"12M_train\":  {\"train_start\": \"2017-04-01\", \"train_end\": \"2018-04-01\", \"val_start\": \"2017-04-01\", \"val_end\": \"2018-04-01\"},\n",
    "        # \"12M_train_3M_val\":  {\"train_start\": \"2017-04-01\", \"train_end\": \"2018-04-01\", \"val_start\": \"2018-04-01\", \"val_end\": \"2018-07-01\"},\n",
    "        \"12M_train_6M_val\":  {\"train_start\": \"2017-04-01\", \"train_end\": \"2018-04-01\", \"val_start\": \"2018-04-01\", \"val_end\": \"2018-10-01\"},\n",
    "        \"12M_train_9M_val\":  {\"train_start\": \"2017-04-01\", \"train_end\": \"2018-04-01\", \"val_start\": \"2018-04-01\", \"val_end\": \"2019-01-01\"},\n",
    "        \"12M_train_12M_val\":  {\"train_start\": \"2017-04-01\", \"train_end\": \"2018-04-01\", \"val_start\": \"2018-04-01\", \"val_end\": \"2019-04-01\"},\n",
    "        },\n",
    "}\n",
    "scenarios_sensors['5'] = scenarios_sensors['2'].copy()\n",
    "scenarios_sensors['6'] = scenarios_sensors['2'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MLForecastPipeline import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_split_data(df, scenario, test_start_date=TEST_START_DATE, date_col=\"ds\"):\n",
    "    train_data = df[df[date_col] <= scenario['train_end']]\n",
    "    val_data = df[(df[date_col] > scenario['val_start']) & (df[date_col] <= scenario['val_end'])]\n",
    "    test_data = df[df[date_col] >= test_start_date]\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "def split_data(df, scenario, date_col=\"ds\"):\n",
    "    \"\"\"Extracts train and test data based on train end date.\"\"\"\n",
    "    train_data = df[df[date_col] <= scenario['train_end']]\n",
    "    test_start = pd.to_datetime(scenario['train_end']) + pd.Timedelta(days=1)\n",
    "    test_data = df[df[date_col] >= test_start]\n",
    "    return train_data, test_data\n",
    "\n",
    "models = {\n",
    "    \"SGD_Optuna\": SGDRegressor( penalty='elasticnet', l1_ratio=0.5, alpha=0.001, random_state=42 ),\n",
    "    # \"SGDRegressor\": SGDRegressor(random_state=42),\n",
    "    # \"SGD_ElasticNet\": SGDRegressor( penalty='elasticnet', l1_ratio=0.5, alpha=0.001, random_state=42 ),\n",
    "}\n",
    "\n",
    "# Define lag transformations\n",
    "from mlforecast.lag_transforms import *\n",
    "lag_transforms_options = [\n",
    "    # {},\n",
    "    {1: [rolling_mean_14], 7: [rolling_mean_30], 30: [expanding_mean]},\n",
    "    {1: [expanding_mean], 7: [rolling_mean_14], 30: [expanding_mean]},\n",
    "    # {7: [RollingMean(window_size=7)], 30: [RollingMean(window_size=30)], 60: [RollingMean(window_size=60)], },\n",
    "    {7: [RollingMean(7), RollingStd(7)], 30: [RollingMean(30)], 60: [ExpandingMean()], 14: [ExponentiallyWeightedMean(alpha=0.3)],},\n",
    "    {7: [RollingMean(7), RollingStd(7), ExpandingStd()], 14: [RollingMean(14), ExpandingStd(), ExponentiallyWeightedMean(alpha=0.3)], 30: [RollingMean(30)], 60: [ExpandingMean()],},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping to MLForecast format\n",
    "def format_multi_df_to_mlforecast(df):\n",
    "    df_melted = df.melt(id_vars=['full_date'], var_name='unique_id', value_name='y')\n",
    "    return df_melted.rename(columns={'full_date': 'ds'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optuna_objective(trial, train_df, test_df, transforms, lags, lag_transforms):\n",
    "    alpha = trial.suggest_float('alpha', 1e-6, 1, log=True)\n",
    "    l1_ratio = trial.suggest_float('l1_ratio', 0.0, 1.0)\n",
    "    max_iter = trial.suggest_int('max_iter', 300, 1000, step=100)  # Optimizing max_iter (number of iterations)\n",
    "    eta0 = trial.suggest_float('eta0', 1e-6, 1, log=True)\n",
    "    tol = trial.suggest_loguniform('tol', 1e-6, 1e-3)\n",
    "\n",
    "    model = SGDRegressor(alpha=alpha, l1_ratio=l1_ratio, max_iter=max_iter, eta0=eta0, tol=tol, penalty='elasticnet', random_state=42)\n",
    "\n",
    "    try:\n",
    "        fcst = MLForecast(\n",
    "            models=[model],\n",
    "            freq='D',\n",
    "            lags=lags,\n",
    "            target_transforms=transforms,\n",
    "            lag_transforms=lag_transforms,\n",
    "            num_threads=1,\n",
    "        )\n",
    "        fcst.fit(train_df)\n",
    "        predictions = fcst.predict(h=len(test_df))\n",
    "        mape = mape_met(test_df['y'].values, predictions['SGDRegressor'].values)\n",
    "        return mape\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return float('inf')\n",
    "    \n",
    "import optuna\n",
    "\n",
    "def run_optuna_search(train_df, test_df, transforms, lags, lag_transforms, n_trials=30):\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(lambda trial: optuna_objective(trial, train_df, test_df, transforms, lags, lag_transforms), n_trials=n_trials)\n",
    "    return study.best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import time\n",
    "\n",
    "def process_scenario(sensor_name, scenario_name, scenario, selected_sensors_df, models, lag_transforms_options, ratios=[0.33, 0.66, 1]):\n",
    "    \"\"\" Process each scenario independently and save results. \"\"\"\n",
    "    print(f'{sensor_name}_{scenario_name}')\n",
    "    formatted_df = format_df_to_mlforecast(selected_sensors_df[['full_date', sensor_name]], 'full_date', sensor_name, unique_id=sensor_name)\n",
    "    formatted_df = formatted_df[['ds', 'y', 'unique_id']]\n",
    "    \n",
    "    _, val_test_df = split_data(formatted_df, scenario) # everything after train\n",
    "    train_df, val_df, test_df = full_split_data(formatted_df, scenario) # here different validation set but same test \n",
    "    optimal_lags_list = get_optimal_lags(train_df, 'y', ratios=ratios, low_resources=True)\n",
    "    target_transforms = get_dynamic_transforms(train_df, remove_boxcox=True)\n",
    "\n",
    "    print(len(target_transforms))\n",
    "    print(len(optimal_lags_list))\n",
    "    print(len(models))\n",
    "    print(len(lag_transforms_options))\n",
    "\n",
    "    results = evaluate_models_sgd_tune(train_df, val_df, val_test_df, models, target_transforms, lag_transforms_options, optimal_lags_list, n_jobs=-1, n_trials=30)\n",
    "\n",
    "    # Save results\n",
    "    save_results(results, f\"results/run_18/{sensor_name}_{scenario_name}.csv\")\n",
    "\n",
    "    return results\n",
    "\n",
    "def run_all_scenarios_parallel(scenarios_sensors, selected_sensors_df, models, lag_transforms_options, ratios=[0.33, 0.66, 1]):\n",
    "    # don't use all cpus (instead all but one)\n",
    "    results = Parallel(n_jobs=15)( \n",
    "        delayed(process_scenario)(sensor_name, scenario_name, scenario, selected_sensors_df, models, lag_transforms_options, ratios=ratios)\n",
    "        for sensor_name, scenarios in scenarios_sensors.items()\n",
    "        for scenario_name, scenario in scenarios.items()\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "def sgd_optuna_objective(trial, train_df, test_df, transforms, lags, lag_transforms):\n",
    "    alpha = trial.suggest_float('alpha', 1e-6, 1, log=True)\n",
    "    l1_ratio = trial.suggest_float('l1_ratio', 0.0, 1.0)\n",
    "    max_iter = trial.suggest_int('max_iter', 300, 1000, step=100)  # Optimizing max_iter (number of iterations)\n",
    "    eta0 = trial.suggest_float('eta0', 1e-6, 1, log=True)\n",
    "    tol = trial.suggest_float('tol', 1e-6, 1e-3, log=True)\n",
    "\n",
    "    model = SGDRegressor(alpha=alpha, l1_ratio=l1_ratio, max_iter=max_iter, eta0=eta0, tol=tol, penalty='elasticnet', random_state=42)\n",
    "\n",
    "    try:\n",
    "        fcst = MLForecast(\n",
    "            models=[model],\n",
    "            freq='D',\n",
    "            lags=lags,\n",
    "            target_transforms=transforms,\n",
    "            lag_transforms=lag_transforms,\n",
    "            num_threads=1,\n",
    "        )\n",
    "        fcst.fit(train_df)\n",
    "        predictions = fcst.predict(h=len(test_df))\n",
    "        mape = mape_met(test_df['y'].values, predictions['SGDRegressor'].values)\n",
    "        return mape\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return float('inf')\n",
    "    \n",
    "import optuna\n",
    "def run_optuna_search(train_df, test_df, transforms, lags, lag_transforms, n_trials=30, n_jobs=-1):\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(lambda trial: sgd_optuna_objective(trial, train_df, test_df, transforms, lags, lag_transforms), n_trials=n_trials, n_jobs=n_jobs)\n",
    "    return study.best_params\n",
    "\n",
    "def evaluate_model_for_configuration(lag_name, optimal_lags, transform_combination, lag_transforms, model_name, model, train_df, val_df, val_test_df, date_features, max_test_length, transforms, n_trials=42):\n",
    "    best_params = run_optuna_search(train_df, val_df, list(transform_combination), optimal_lags, lag_transforms, n_trials=n_trials)\n",
    "    optuna_model = SGDRegressor(**best_params, random_state=42)\n",
    "    test_lengths = list(range(30, 181, 30)) + [240, 300, 360, 480, 600, 720, max_test_length]\n",
    "\n",
    "    try:\n",
    "        fcst = MLForecast(\n",
    "            models=[optuna_model],\n",
    "            freq='D',\n",
    "            lags=optimal_lags,\n",
    "            target_transforms=list(transform_combination),\n",
    "            date_features=date_features,\n",
    "            num_threads=4,  # Multi-threading during model fitting\n",
    "            lag_transforms=lag_transforms,\n",
    "        )\n",
    "\n",
    "        fcst.fit(train_df)\n",
    "        predictions = fcst.predict(h=max_test_length)\n",
    "        \n",
    "        test_df_copy = val_test_df.copy()\n",
    "        test_df_copy['forecast'] = predictions['SGDRegressor'].values\n",
    "\n",
    "        error_dict = {}\n",
    "        for test_length in test_lengths:  # Define test segment lengths\n",
    "            eval_subset = test_df_copy.iloc[:test_length]  # Take subset for evaluation\n",
    "            error_dict[f\"test_{test_length}_days\"] = mape_met(eval_subset['y'].values, eval_subset['forecast'].values)\n",
    "\n",
    "        return {\n",
    "            \"Model\": model_name,\n",
    "            \"Transforms\": stringify_transform(list(transform_combination)),\n",
    "            \"Lags\": optimal_lags,\n",
    "            \"Lag Name\": lag_name,\n",
    "            \"Lag Transforms\": str(lag_transforms),\n",
    "            **error_dict,\n",
    "            \"preds\": test_df_copy['forecast'].values,\n",
    "            \"params\": best_params \n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating model {model_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "from itertools import combinations, chain\n",
    "def evaluate_models_sgd_tune_parallel(train_df, val_df, val_test_df, models, target_transforms, lag_transforms_options, optimal_lags_list, date_features=['dayofweek', 'month'], n_trials=42, n_jobs=-1):\n",
    "    results = []\n",
    "    print(target_transforms)\n",
    "    valid_transform_combinations = [()] + list(chain(combinations(target_transforms, 1), combinations(target_transforms, 2)))\n",
    "    valid_transform_combinations = [tc for tc in valid_transform_combinations if filter_conflicting_transforms(tc)]\n",
    "\n",
    "    # Generate all combinations of the parameters to evaluate\n",
    "    configs = [\n",
    "        (lag_name, optimal_lags, transform_combination, lag_transforms, model_name, model)\n",
    "        for lag_name, optimal_lags in optimal_lags_list.items()\n",
    "        for transform_combination in valid_transform_combinations\n",
    "        for lag_transforms in lag_transforms_options\n",
    "        for model_name, model in models.items()\n",
    "    ]\n",
    "\n",
    "    print(len(configs))\n",
    "\n",
    "    # Run evaluations in parallel using Joblib\n",
    "    results = Parallel(n_jobs=n_jobs, verbose=30)(\n",
    "        delayed(evaluate_model_for_configuration)(\n",
    "            lag_name, optimal_lags, transform_combination, lag_transforms, model_name, model, train_df, val_df, val_test_df, date_features, len(val_test_df), target_transforms, n_trials)\n",
    "        for lag_name, optimal_lags, transform_combination, lag_transforms, model_name, model in configs\n",
    "    )\n",
    "\n",
    "    results = [res for res in results if res is not None]\n",
    "    \n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sensor_name, scenarios in scenarios_sensors.items():\n",
    "    for scenario_name, scenario in scenarios.items():\n",
    "        formatted_df = format_df_to_mlforecast(selected_sensors_df[['full_date', sensor_name]], 'full_date', sensor_name, unique_id=sensor_name)\n",
    "        formatted_df = formatted_df[['ds', 'y', 'unique_id']]\n",
    "        \n",
    "        _, val_test_df = split_data(formatted_df, scenario) # everything after train\n",
    "        train_df, val_df, test_df = full_split_data(formatted_df, scenario) # here different validation set but same test \n",
    "        optimal_lags_list = get_optimal_lags(train_df, 'y', low_resources=True)\n",
    "        target_transforms = get_dynamic_transforms(train_df, remove_boxcox=True)\n",
    "\n",
    "        results = evaluate_models_sgd_tune_parallel(train_df, val_df, val_test_df, models, target_transforms, lag_transforms_options, optimal_lags_list, n_trials=32)\n",
    "\n",
    "        save_results(results, f\"results/run_18/{sensor_name}_{scenario_name}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_all_scenarios_parallel(scenarios_sensors, selected_sensors_df, models, lag_transforms_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sensor_name, scenarios in scenarios_sensors.items():\n",
    "    for scenario_name, scenario in scenarios.items():\n",
    "        process_scenario(sensor_name, scenario_name, scenario, selected_sensors_df, models, lag_transforms_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
