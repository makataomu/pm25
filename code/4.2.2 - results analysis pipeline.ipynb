{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MLForecastPipeline import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_sensors_df = pd.read_csv(\"../data/selected_sensors2_cleaned.csv\", index_col=0)\n",
    "\n",
    "scenarios_sensors = {\n",
    "    # 0: 1, 4372603\n",
    "    # \"0_12M_train_7M_test\": {\"train_start\": \"2017-03-25\", \"train_end\": \"2018-03-25\", \"test_start\": \"2018-03-26\", \"test_end\": \"2018-10-10\"},\n",
    "    '2': {\n",
    "        \"26M_train\":  {\"train_start\": \"2017-04-01\", \"train_end\": \"2019-06-01\"},\n",
    "        \"24M_train\":  {\"train_start\": \"2017-04-01\", \"train_end\": \"2019-04-01\"},\n",
    "        \"22M_train\":  {\"train_start\": \"2017-04-01\", \"train_end\": \"2019-02-01\"},\n",
    "        \"20M_train\":  {\"train_start\": \"2017-04-01\", \"train_end\": \"2018-12-01\"},\n",
    "        \"18M_train\":  {\"train_start\": \"2017-04-01\", \"train_end\": \"2018-10-01\"},\n",
    "        \"12M_train\":  {\"train_start\": \"2017-04-01\", \"train_end\": \"2018-04-01\"},\n",
    "        \"10M_train\":  {\"train_start\": \"2017-04-01\", \"train_end\": \"2018-01-25\"},\n",
    "        \"8M_train\":   {\"train_start\": \"2017-04-01\", \"train_end\": \"2017-10-25\"},\n",
    "        \n",
    "        # Non-Heating Periods\n",
    "        \"NH_3M_train\":  {\"train_start\": \"2017-04-15\", \"train_end\": \"2017-07-15\"},\n",
    "        \"NH_4M_train\":  {\"train_start\": \"2017-04-15\", \"train_end\": \"2017-08-15\"},\n",
    "        \"NH_2M_train\":  {\"train_start\": \"2017-04-15\", \"train_end\": \"2017-06-15\"},\n",
    "        \"NH_1M_train\":  {\"train_start\": \"2017-04-15\", \"train_end\": \"2017-05-15\"},\n",
    "        \"NH_15D_train\": {\"train_start\": \"2017-04-15\", \"train_end\": \"2017-04-30\"},\n",
    "        \"NH_feb_2M_train\": {\"train_start\": \"2017-02-15\", \"train_end\": \"2017-04-15\"},\n",
    "        \"NH_feb_1M_train\": {\"train_start\": \"2017-02-15\", \"train_end\": \"2017-04-15\"},\n",
    "        \"NH_mar_2M_train\": {\"train_start\": \"2017-03-15\", \"train_end\": \"2017-05-15\"},\n",
    "        \"NH_mar_1M_train\": {\"train_start\": \"2017-03-15\", \"train_end\": \"2017-04-15\"},\n",
    "\n",
    "        # Heating Periods\n",
    "        \"H_5M_train\":     {\"train_start\": \"2017-06-01\", \"train_end\": \"2017-11-01\"},\n",
    "        \"H_3M_jul_train\": {\"train_start\": \"2017-07-01\", \"train_end\": \"2017-10-10\"},\n",
    "        \"H_3M_sep_train\": {\"train_start\": \"2017-09-01\", \"train_end\": \"2017-12-10\"},\n",
    "        \"H_3M_nov_train\": {\"train_start\": \"2017-11-01\", \"train_end\": \"2018-02-10\"},\n",
    "        },\n",
    "}\n",
    "scenarios_sensors['5'] = scenarios_sensors['2'].copy()\n",
    "scenarios_sensors['6'] = scenarios_sensors['2'].copy()\n",
    "\n",
    "def split_data(df, scenario, date_col=\"ds\"):\n",
    "    \"\"\"Extracts train and test data based on train end date.\"\"\"\n",
    "    train_data = df[df[date_col] <= scenario['train_end']]\n",
    "    test_start = pd.to_datetime(scenario['train_end']) + pd.Timedelta(days=1)\n",
    "    test_data = df[df[date_col] >= test_start]\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "LAG_TRANSFORMS_MAP = {\n",
    "    \"expanding_mean_rolling_14_rolling_30\": {1: 'expanding_mean', 7: 'expanding_mean', 30: 'rolling_mean_30'},\n",
    "    \"expanding_mean_rolling_14\": {1: 'expanding_mean', 7: 'rolling_mean_14', 30: 'expanding_mean'},\n",
    "    \"rolling_14_rolling_30_expanding\": {1: 'rolling_mean_14', 7: 'rolling_mean_30', 30: 'expanding_mean'},\n",
    "    \"rolling_14_expanding\": {1: 'rolling_mean_14', 30: 'expanding_mean'},\n",
    "    \"rolling_14_only\": {1: 'rolling_mean_14'},\n",
    "    \"no_transform\": {},\n",
    "}\n",
    "\n",
    "def map_lag_transforms(lag_transform_dict, lag_transforms_map=LAG_TRANSFORMS_MAP):\n",
    "    for name, transform in lag_transforms_map.items():\n",
    "        if lag_transform_dict == transform:\n",
    "            return name\n",
    "    return \"unknown\"\n",
    "\n",
    "def analyze_results(df, lag_transforms_map=LAG_TRANSFORMS_MAP, mape_threshold=40, model_filter=None):\n",
    "    df = df.copy()\n",
    "    df['Lag Transform Name'] = df['Lag Transforms'].apply(lambda x: map_lag_transforms(x, lag_transforms_map))\n",
    "    df['Lag_Set_Name'] = df['Lag Name']\n",
    "    # Identify MAPE columns dynamically\n",
    "    mape_columns = [col for col in df.columns if col.startswith(\"test_\") and col.endswith(\"_days\")]\n",
    "    \n",
    "    # Compute mean MAPE across all test periods\n",
    "    df['MAPE'] = df[mape_columns].mean(axis=1)\n",
    "    \n",
    "    # Apply filtering\n",
    "    top_df = df[df[\"MAPE\"] < mape_threshold].copy()\n",
    "    if model_filter:\n",
    "        top_df = top_df[top_df['Model'] == model_filter].copy()\n",
    "    \n",
    "    # Compute groupings\n",
    "    top_models = top_df.groupby(\"Model\")[\"MAPE\"].mean().sort_values().reset_index()\n",
    "    top_transforms = top_df.groupby(\"Transforms\")[\"MAPE\"].mean().sort_values().reset_index()\n",
    "    top_lag_transforms = top_df.groupby(\"Lag Transform Name\")[\"MAPE\"].mean().sort_values().reset_index()\n",
    "    top_lags = top_df.groupby(\"Lag_Set_Name\")[\"MAPE\"].mean().sort_values().reset_index()\n",
    "    \n",
    "    # Compute MAPE trends over different forecasting horizons\n",
    "    mape_trends = top_df.groupby(\"Model\")[mape_columns].mean().reset_index()\n",
    "    \n",
    "    return top_models, top_transforms, top_lag_transforms, top_lags, mape_trends\n",
    "\n",
    "def plot_results(top_models, top_transforms, top_lag_transforms, top_lags, mape_trends):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    sns.barplot(x=top_models[\"Model\"], y=top_models[\"MAPE\"], palette=\"viridis\", hue=top_models[\"Model\"])\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title(\"Average MAPE per Model\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    sns.barplot(x=top_transforms[\"Transforms\"], y=top_transforms[\"MAPE\"], palette=\"coolwarm\", hue=top_transforms[\"Transforms\"])\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(\"Average MAPE per Transform\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    sns.barplot(x=top_lag_transforms[\"Lag Transform Name\"], y=top_lag_transforms[\"MAPE\"], palette=\"Blues\", hue=top_lag_transforms[\"Lag Transform Name\"])\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(\"Average MAPE per Lag Transform\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    sns.barplot(x=top_lags[\"Lag_Set_Name\"], y=top_lags[\"MAPE\"], palette=\"Blues\", hue=top_lags[\"Lag_Set_Name\"])\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(\"MAPE vs Number of Lags\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot MAPE trends across different forecasting horizons\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    for model in mape_trends[\"Model\"]:\n",
    "        plt.plot(mape_trends.columns[1:], mape_trends[mape_trends[\"Model\"] == model].values[0][1:], label=model)\n",
    "    plt.xlabel(\"Forecasting Horizon (Days)\")\n",
    "    plt.ylabel(\"MAPE\")\n",
    "    plt.title(\"MAPE Trends Across Forecast Horizons\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# top_models, top_transforms, top_lag_transforms, top_lags, mape_trends = analyze_results(df, lag_transforms_map, optimal_lags_map)\n",
    "# plot_results(top_models, top_transforms, top_lag_transforms, top_lags, mape_trends)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import glob\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# # Load all results\n",
    "# results = {}\n",
    "# for file in glob.glob(\"results/run_3/*.csv\"):\n",
    "#     dataset_name = file.split(\"/\")[-1].replace(\".csv\", \"\")\n",
    "#     results[dataset_name] = pd.read_csv(file)\n",
    "\n",
    "# # Combine all datasets into a single DataFrame\n",
    "# df = pd.concat([df.assign(Dataset=name) for name, df in results.items()], ignore_index=True)\n",
    "\n",
    "# # Define threshold for MAPE\n",
    "# mape_threshold = 35  \n",
    "\n",
    "# # Identify best models per test period\n",
    "# best_models_per_period = {}\n",
    "# for test_n in [30, 60, 90, 120, 150, 180, 240, 300, 360, 480, 600, 720, 737]:\n",
    "#     col = f\"test_{test_n}_days\"\n",
    "#     best_models_per_period[test_n] = df[df[col] < mape_threshold].nsmallest(1, col)\n",
    "\n",
    "# # Calculate variance across all test columns\n",
    "# test_cols = [col for col in df.columns if \"test_\" in col]\n",
    "# df[\"stability\"] = df[test_cols].std(axis=1)\n",
    "\n",
    "# # Select most stable model\n",
    "# most_stable_model = df.nsmallest(1, \"stability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "results = {}\n",
    "for file in glob.glob(\"results/run_3/*.csv\"):\n",
    "    dataset_name = file.split(\"\\\\\")[-1].replace(\".csv\", \"\")\n",
    "    results[dataset_name] = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6_NH_15D_train → (6, 0.5, 'NH_15D_train')\n",
      "6_H_5M_train → (6, 5, 'H_5M_train')\n",
      "2_18M_train → (2, 18, '18M_train')\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_train_info(dataset_name):\n",
    "    \"\"\"\n",
    "    Extracts sensor ID, training length in months, and a standardized train label.\n",
    "    \n",
    "    Example Inputs:\n",
    "        - \"6_NH_15D_train\"  → (6, 0.5, \"NH_15D_train\")\n",
    "        - \"6_H_5M_train\"    → (6, 5, \"H_5M_train\")\n",
    "        - \"2_18M_train\"     → (2, 18, \"18M_train\")\n",
    "    \n",
    "    Returns:\n",
    "        - sensor (int): Sensor ID\n",
    "        - train_months (float): Training length in months\n",
    "        - train_label (str): Everything after the sensor ID (used for finding comparable datasets)\n",
    "    \"\"\"\n",
    "    name_parts = dataset_name.split('_')\n",
    "\n",
    "    # Extract sensor ID\n",
    "    sensor = int(name_parts[0])  # First part is always the sensor number\n",
    "\n",
    "    # Reconstruct the label for easy dataset comparison\n",
    "    train_label = '_'.join(name_parts[1:])\n",
    "\n",
    "    # Extract training length (2nd to last part contains number + unit)\n",
    "    train_info = name_parts[-2]  # Example: \"15D\" or \"5M\"\n",
    "    match = re.match(r\"(\\d+)([MD])\", train_info)  # Extract number and unit\n",
    "\n",
    "    if match:\n",
    "        train_length = int(match.group(1))\n",
    "        unit = match.group(2)\n",
    "\n",
    "        # Convert days to months (approximate)\n",
    "        train_months = train_length / 30 if unit == \"D\" else train_length\n",
    "    else:\n",
    "        return None, None, None  # Invalid format, return None values\n",
    "\n",
    "    return sensor, train_months, train_label\n",
    "\n",
    "# Example Usage\n",
    "datasets = [\"6_NH_15D_train\", \"6_H_5M_train\", \"2_18M_train\"]\n",
    "for ds in datasets:\n",
    "    print(f\"{ds} → {extract_train_info(ds)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String Input: {1: [<function expanding_mean at 0x0000021CAC0F2840>], 7: [<function rolling_mean_14 at 0x0000021CA47C3C40>], 30: [<function expanding_mean at 0x0000021CAC0F2840>]}\n",
      "Parsed Back: {1: [<function expanding_mean at 0x000001F254157EC0>], 7: [<function rolling_mean_14 at 0x000001F25BB6F9C0>], 30: [<function expanding_mean at 0x000001F254157EC0>]}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numba import njit\n",
    "from window_ops.expanding import expanding_mean\n",
    "from window_ops.rolling import rolling_mean\n",
    "\n",
    "@njit\n",
    "def rolling_mean_14(x):\n",
    "    return rolling_mean(x, window_size=14)\n",
    "\n",
    "@njit\n",
    "def rolling_mean_30(x):\n",
    "    return rolling_mean(x, window_size=30)\n",
    "\n",
    "# Define function mapping\n",
    "lag_transforms_mapping = {\n",
    "    'expanding_mean': expanding_mean,\n",
    "    'rolling_mean_14': rolling_mean_14,\n",
    "    'rolling_mean_30': rolling_mean_30,\n",
    "}\n",
    "\n",
    "# Function to convert lag_transforms dictionary to a string\n",
    "def stringify_lag_transforms(lag_transforms):\n",
    "    \"\"\"Converts lag_transforms dictionary to a clean string format for storage.\"\"\"\n",
    "    return str({key: [func.__name__ for func in funcs] for key, funcs in lag_transforms.items()})\n",
    "\n",
    "# Function to parse lag_transforms safely from an invalid dictionary-like string\n",
    "def parse_lag_transforms(lag_transforms_str):\n",
    "    \"\"\"Parses a raw function dictionary string and converts it back to a proper dictionary with function references.\"\"\"\n",
    "    try:\n",
    "        # Extract function names using regex pattern: `<function function_name at 0x...>`\n",
    "        cleaned_str = re.sub(r'<function (\\w+) at 0x[0-9A-Fa-f]+>', r'\"\\1\"', lag_transforms_str)\n",
    "\n",
    "        # Convert cleaned string into a valid Python dictionary\n",
    "        temp_dict = eval(cleaned_str)  # Evaluates after function names are fixed\n",
    "\n",
    "        # Map function names back to actual function references\n",
    "        return {key: [lag_transforms_mapping[func_name] for func_name in funcs] for key, funcs in temp_dict.items()}\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing lag_transforms {lag_transforms_str}: {e}\")\n",
    "        return {}\n",
    "\n",
    "# Example input with incorrect function references\n",
    "initial_lag_transforms_str = \"{1: [<function expanding_mean at 0x0000021CAC0F2840>], 7: [<function rolling_mean_14 at 0x0000021CA47C3C40>], 30: [<function expanding_mean at 0x0000021CAC0F2840>]}\"\n",
    "\n",
    "# Convert to proper lag_transforms dictionary\n",
    "parsed_lag_transforms = parse_lag_transforms(initial_lag_transforms_str)\n",
    "\n",
    "print(\"String Input:\", initial_lag_transforms_str)\n",
    "print(\"Parsed Back:\", parsed_lag_transforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from numba import njit\n",
    "from window_ops.expanding import expanding_mean\n",
    "from window_ops.rolling import rolling_mean\n",
    "from mlforecast import MLForecast\n",
    "# from my_custom_utils import format_df_to_mlforecast, split_data  # Assuming these are in a module\n",
    "import ast\n",
    "\n",
    "# Define parameters\n",
    "HORIZONS = [30, 60, 90, 120, 150, 180, 240, 300, 360, 480, 600, 720, 737]\n",
    "MAPE_COLUMNS = [f\"test_{h}_days\" for h in HORIZONS]\n",
    "\n",
    "# Thresholds for different filtering criteria\n",
    "EARLY_HORIZON_THRESHOLD = 30  # MAPE threshold for early horizons\n",
    "ONE_THIRD_THRESHOLD = 30       # Threshold for MAPE at 1/3 of training length\n",
    "ONE_THIRD_THRESHOLD_GENERAL = 35       # Threshold for MAPE at 1/3 of training length\n",
    "\n",
    "\n",
    "EARLY_HORIZONS = [30, 60, 90, 120]  # Horizons we check for early filtering\n",
    "SENSORS_TO_COMPARE = [2, 5, 6]  # Sensors that share training length criteria\n",
    "\n",
    "# Define available lag transforms\n",
    "lag_transforms_options = [\n",
    "    {1: [expanding_mean], 7: [rolling_mean], 30: [expanding_mean]},\n",
    "    {1: [rolling_mean], 7: [rolling_mean], 30: [expanding_mean]},\n",
    "]\n",
    "\n",
    "model_mapping = {\n",
    "    \"XGBRegressor\": XGBRegressor(),\n",
    "    \"SGDRegressor_42\": SGDRegressor(random_state=42),\n",
    "    \"Ridge\": Ridge(),\n",
    "    \"Lasso\": Lasso()\n",
    "}\n",
    "\n",
    "transform_mapping = {\n",
    "    \"AutoDifferences\" : AutoDifferences, \n",
    "    \"AutoSeasonalDifferences\" : AutoSeasonalDifferences, \n",
    "    \"AutoSeasonalityAndDifferences\" : AutoSeasonalityAndDifferences,\n",
    "    \"LocalStandardScaler\" : LocalStandardScaler, \n",
    "    \"LocalMinMaxScaler\" : LocalMinMaxScaler, \n",
    "    \"LocalBoxCox\" : LocalBoxCox\n",
    "}\n",
    "\n",
    "@njit\n",
    "def rolling_mean_14(x):\n",
    "    return rolling_mean(x, window_size=14)\n",
    "\n",
    "@njit\n",
    "def rolling_mean_30(x):\n",
    "    return rolling_mean(x, window_size=30)\n",
    "\n",
    "# Function to extract sensor ID and train length\n",
    "def extract_train_info(dataset_name):\n",
    "    name_parts = dataset_name.split('_')\n",
    "    sensor = int(name_parts[0])  \n",
    "    train_label = '_'.join(name_parts[1:])\n",
    "\n",
    "    train_info = name_parts[-2]  \n",
    "    match = re.match(r\"(\\d+)([MD])\", train_info)  \n",
    "\n",
    "    if match:\n",
    "        train_length = int(match.group(1))\n",
    "        unit = match.group(2)\n",
    "        train_months = train_length / 30 if unit == \"D\" else train_length\n",
    "    else:\n",
    "        return None, None, None  \n",
    "\n",
    "    return sensor, train_months, train_label\n",
    "\n",
    "# Function to reverse `stringify_transform`\n",
    "def parse_transform_string(transform_str):\n",
    "    transform_str = transform_str.strip()\n",
    "    if \"(\" in transform_str:\n",
    "        class_name, params = transform_str.split(\"(\", 1)\n",
    "        params = params.rstrip(\")\")\n",
    "        param_dict = {}\n",
    "        if params != \"NoParams\" and class_name != \"LocalBoxCox\":\n",
    "            for param in params.split(\", \"):\n",
    "                key, value = param.split(\"=\")\n",
    "                if key in (\"max_diffs\", \"season_length\", \"max_season_length\"):\n",
    "                    param_dict[key] = int(value) if value.replace(\".\", \"\").isdigit() else value\n",
    "        return class_name, param_dict\n",
    "    return transform_str, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define patterns to match and their correct format\n",
    "rename_patterns = {\n",
    "    \"H_3M_jul_train\": \"H_jul_3M_train\",\n",
    "    \"H_3M_sep_train\": \"H_sep_3M_train\",\n",
    "    \"H_3M_nov_train\": \"H_nov_3M_train\",\n",
    "}\n",
    "\n",
    "# Create a new dictionary with updated keys\n",
    "updated_results = {}\n",
    "\n",
    "for dataset_name, df in results.items():\n",
    "    new_name = dataset_name  # Default: keep the same name\n",
    "\n",
    "    for old_pattern, new_pattern in rename_patterns.items():\n",
    "        if old_pattern in dataset_name:\n",
    "            new_name = dataset_name.replace(old_pattern, new_pattern)\n",
    "            break  # Stop checking once renamed\n",
    "\n",
    "    updated_results[new_name] = df  # Preserve the dataset content\n",
    "\n",
    "results = updated_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATASET: 2_10M_train\n",
      "================================================================================\n",
      "================================================================================\n",
      "DATASET: 2_12M_train\n",
      "================================================================================\n",
      "Model Ridge rejected: Poor performance on sensor 5 at one_third_horizon\n",
      "Model Lasso rejected: Poor performance on sensor 5 at one_third_horizon\n",
      "Model Ridge rejected: Poor performance on sensor 5 at one_third_horizon\n",
      "Model Ridge rejected: Poor performance on sensor 5 at one_third_horizon\n",
      "Model 2_12M_train_Ridge_48ac90c4 passed: Good performance across sensors\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'saved_models/2_12M_train_Ridge_48ac90c4.joblib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 135\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;66;03m# --- Save Passed Model ---\u001b[39;00m\n\u001b[0;32m    134\u001b[0m model_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaved_models/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00munique_model_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.joblib\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 135\u001b[0m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfcst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_filename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;66;03m# Save metadata\u001b[39;00m\n\u001b[0;32m    138\u001b[0m passed_models_info[unique_model_id] \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msensor\u001b[39m\u001b[38;5;124m\"\u001b[39m: sensor_id,\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_label\u001b[39m\u001b[38;5;124m\"\u001b[39m: train_label,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile_path\u001b[39m\u001b[38;5;124m\"\u001b[39m: model_filename,\n\u001b[0;32m    147\u001b[0m }\n",
      "File \u001b[1;32mc:\\Users\\77019\\pyver\\py312\\Lib\\site-packages\\joblib\\numpy_pickle.py:552\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(value, filename, compress, protocol, cache_size)\u001b[0m\n\u001b[0;32m    550\u001b[0m         NumpyPickler(f, protocol\u001b[38;5;241m=\u001b[39mprotocol)\u001b[38;5;241m.\u001b[39mdump(value)\n\u001b[0;32m    551\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_filename:\n\u001b[1;32m--> 552\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    553\u001b[0m         NumpyPickler(f, protocol\u001b[38;5;241m=\u001b[39mprotocol)\u001b[38;5;241m.\u001b[39mdump(value)\n\u001b[0;32m    554\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'saved_models/2_12M_train_Ridge_48ac90c4.joblib'"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "import joblib\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "RUN_NUM = \"run_3\"\n",
    "\n",
    "failed_models_log = f\"results_of_results/{RUN_NUM}_failed_models_log.txt\"\n",
    "\n",
    "# Ensure directory exists\n",
    "save_dir = f\"results_of_results/{RUN_NUM}\"\n",
    "os.makedirs(save_dir, exist_ok=True)  # Creates the directory if it doesn't exist\n",
    "\n",
    "# Function to create a unique hash based on model parameters\n",
    "def generate_model_hash(model_name, transforms, lags, lag_transforms):\n",
    "    \"\"\"Creates a short unique hash for the model based on its features & transforms.\"\"\"\n",
    "    hash_input = f\"{model_name}_{transforms}_{lags}_{lag_transforms}\"\n",
    "    return hashlib.md5(hash_input.encode()).hexdigest()[:8]  # Take only the first 8 characters\n",
    "\n",
    "# Dictionary to store information about passed models\n",
    "passed_models_info = {}\n",
    "\n",
    "# Process each dataset\n",
    "for dataset_name, df in results.items():\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"DATASET: {dataset_name}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    sensor_id, train_months, train_label = extract_train_info(dataset_name)\n",
    "    if sensor_id is None:\n",
    "        print(\"Skipping dataset: Cannot determine sensor ID or train length.\")\n",
    "        continue\n",
    "\n",
    "    mape_columns_local = [c for c in MAPE_COLUMNS if c in df.columns]\n",
    "\n",
    "    # --- (a) Filtering Criteria ---\n",
    "\n",
    "    # 1. Remove models exceeding the early horizon threshold\n",
    "    early_horizon_cols = [f\"test_{h}_days\" for h in EARLY_HORIZONS if f\"test_{h}_days\" in df.columns]\n",
    "    mask_early_horizon = (df[early_horizon_cols] <= EARLY_HORIZON_THRESHOLD).all(axis=1)\n",
    "\n",
    "    # 2. Ensure MAPE is good at 1/3 of train length\n",
    "    one_third_horizon = int(train_months * 30 // 3)\n",
    "    closest_horizon = min(HORIZONS, key=lambda x: abs(x - one_third_horizon))\n",
    "\n",
    "    if f\"test_{closest_horizon}_days\" in df.columns:\n",
    "        mask_one_third = df[f\"test_{closest_horizon}_days\"] <= ONE_THIRD_THRESHOLD\n",
    "    else:\n",
    "        mask_one_third = True\n",
    "\n",
    "    # Apply filters\n",
    "    df_filtered = df[mask_early_horizon & mask_one_third].copy()\n",
    "\n",
    "    # --- (b) Extract best models and reconstruct `MLForecast` ---\n",
    "    for _, row in df_filtered.iterrows():\n",
    "        model_name = row[\"Model\"]\n",
    "        transforms_str = row[\"Transforms\"]\n",
    "        lags = row[\"Lags\"]\n",
    "        lag_transforms_str = row[\"Lag Transforms\"]\n",
    "\n",
    "        # Reverse transforms\n",
    "        transform_objects = []\n",
    "        if isinstance(transforms_str, str):\n",
    "            for transform_str in transforms_str.split(\" | \"):\n",
    "                class_name, params = parse_transform_string(transform_str)\n",
    "                \n",
    "                # transform_objects.append(eval(class_name)(**params) if params else eval(class_name)())\n",
    "                if class_name in transform_mapping:\n",
    "                    transform_objects.append(transform_mapping[class_name](**params) if params else transform_mapping[class_name]())\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown transform: {class_name}\")\n",
    "\n",
    "        # Reverse lag transforms\n",
    "        lag_transforms = parse_lag_transforms(lag_transforms_str)\n",
    "\n",
    "        # Prepare dataset\n",
    "        sensor_id = str(sensor_id)\n",
    "        scenario = scenarios_sensors[sensor_id][train_label]\n",
    "\n",
    "        if sensor_id not in scenarios_sensors:\n",
    "            continue\n",
    "\n",
    "        formatted_df = format_df_to_mlforecast(\n",
    "            selected_sensors_df[['full_date', sensor_id]], 'full_date', sensor_id, unique_id=sensor_id\n",
    "        )\n",
    "        formatted_df = formatted_df[['ds', 'y', 'unique_id']]\n",
    "        train_df, test_df = split_data(formatted_df, scenario)\n",
    "\n",
    "        # Initialize MLForecast\n",
    "        model_instance = model_mapping.get(model_name)\n",
    "        if model_instance is None:\n",
    "            raise ValueError(f\"Model {model_name} not found in model_mapping\")\n",
    "\n",
    "        fcst = MLForecast(\n",
    "            models=[model_instance],\n",
    "            freq='D',\n",
    "            lags=ast.literal_eval(lags),\n",
    "            target_transforms=transform_objects,\n",
    "            date_features=['dayofweek', 'month'],\n",
    "            num_threads=1,\n",
    "            lag_transforms=lag_transforms,\n",
    "        )\n",
    "\n",
    "        fcst.fit(train_df)\n",
    "        predictions = fcst.predict(h=test_df.shape[0])\n",
    "        test_df_copy = test_df.copy()\n",
    "        test_df_copy['forecast'] = predictions[model_name].values\n",
    "\n",
    "        # Compute one_third_horizon MAPE **before validation**\n",
    "        eval_subset = test_df_copy.iloc[:closest_horizon]\n",
    "        one_third_mape = mape_met(eval_subset['y'].values, eval_subset['forecast'].values)\n",
    "\n",
    "        # --- (c) Train on Other Sensors and Validate at `one_third_horizon` ---\n",
    "        valid = True\n",
    "        for other_sensor in SENSORS_TO_COMPARE:\n",
    "            if other_sensor == int(sensor_id):\n",
    "                continue  \n",
    "\n",
    "            other_sensor_id = str(other_sensor)\n",
    "            if other_sensor_id in scenarios_sensors and train_label in scenarios_sensors[other_sensor_id]:\n",
    "                other_scenario = scenarios_sensors[other_sensor_id][train_label]\n",
    "\n",
    "                formatted_df = format_df_to_mlforecast(\n",
    "                    selected_sensors_df[['full_date', other_sensor_id]], 'full_date', other_sensor_id, unique_id=other_sensor_id\n",
    "                )\n",
    "                formatted_df = formatted_df[['ds', 'y', 'unique_id']]\n",
    "                train_df, test_df = split_data(formatted_df, other_scenario)\n",
    "\n",
    "                fcst.fit(train_df)\n",
    "\n",
    "                try:\n",
    "                    if test_df.shape[0] > 0:\n",
    "                        predictions = fcst.predict(h=test_df.shape[0])\n",
    "                        test_df_copy = test_df.copy()\n",
    "                        test_df_copy['forecast'] = predictions[model_name].values\n",
    "                    else:\n",
    "                        with open(failed_models_log, \"a\") as f:\n",
    "                            f.write(f\"Skipping prediction: test_df is empty for {model_name} {dataset_name}\\n\")\n",
    "                        continue  # Skip this iteration\n",
    "                except Exception as e:\n",
    "                    with open(failed_models_log, \"a\") as f:\n",
    "                        f.write(f\"Prediction failed for {model_name} {dataset_name}: {e}\\n\")\n",
    "                    continue  # Skip model saving if prediction fails\n",
    "\n",
    "                eval_subset = test_df_copy.iloc[:closest_horizon]\n",
    "                other_mape = mape_met(eval_subset['y'].values, eval_subset['forecast'].values)\n",
    "\n",
    "                if other_mape > ONE_THIRD_THRESHOLD_GENERAL:\n",
    "                    valid = False\n",
    "                    with open(failed_models_log, \"a\") as f:\n",
    "                        f.write(f\"Model {model_name} {one_third_mape} rejected: {other_mape} on sensor {other_sensor_id} at one_third_horizon\")\n",
    "                    break\n",
    "\n",
    "        if valid:\n",
    "            model_hash = generate_model_hash(model_name, transforms_str, lags, lag_transforms_str)\n",
    "            unique_model_id = f\"{sensor_id}_{train_label}_{model_name}_{model_hash}\"\n",
    "\n",
    "            print(f\"Model {unique_model_id} passed: {other_mape} across sensors\")\n",
    "\n",
    "            # --- Save Passed Model ---\n",
    "            model_filename = f\"results_of_results/{RUN_NUM}/{unique_model_id}.joblib\"\n",
    "            joblib.dump(fcst, model_filename)\n",
    "\n",
    "            # Save metadata\n",
    "            passed_models_info[unique_model_id] = {\n",
    "                \"sensor\": sensor_id,\n",
    "                \"train_label\": train_label,\n",
    "                \"model_name\": model_name,\n",
    "                \"transforms\": transforms_str,\n",
    "                \"lags\": lags,\n",
    "                \"lag_transforms\": lag_transforms_str,\n",
    "                \"one_third_mape\": one_third_mape,  \n",
    "                \"file_path\": model_filename,\n",
    "            }\n",
    "\n",
    "# --- Save Metadata ---\n",
    "with open(f\"results_of_results/{RUN_NUM}_passed_models_metadata.json\", \"w\") as f:\n",
    "    json.dump(passed_models_info, f, indent=4)\n",
    "\n",
    "# Also save as CSV\n",
    "pd.DataFrame.from_dict(passed_models_info, orient=\"index\").to_csv(f\"results_of_results/{RUN_NUM}_passed_models_metadata.csv\")\n",
    "\n",
    "print(\"All passed models and metadata saved successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
