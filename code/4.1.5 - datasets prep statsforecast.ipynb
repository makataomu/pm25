{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import njit\n",
    "from window_ops.expanding import expanding_mean\n",
    "from window_ops.rolling import rolling_mean\n",
    "\n",
    "@njit\n",
    "def rolling_mean_14(x):\n",
    "    return rolling_mean(x, window_size=14)\n",
    "@njit\n",
    "def rolling_mean_30(x):\n",
    "    return rolling_mean(x, window_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_df_to_mlforecast(df, date_col, target_col, unique_id='mean'):\n",
    "    df_ = df.rename({\n",
    "        date_col: \"ds\",\n",
    "        # target_col: 'y',\n",
    "    }, axis=1)\n",
    "\n",
    "    df_['ds'] = pd.to_datetime(df_['ds'])\n",
    "\n",
    "    df_['y'] = df_[target_col].copy()\n",
    "    # df_.drop(columns=target_col)\n",
    "\n",
    "    df_['unique_id'] = unique_id\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_sensors_df = pd.read_csv(\"../data/selected_sensors2_cleaned.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios_sensors = {\n",
    "    # 0: 1, 4372603\n",
    "    # \"0_12M_train_7M_test\": {\"train_start\": \"2017-03-25\", \"train_end\": \"2018-03-25\", \"test_start\": \"2018-03-26\", \"test_end\": \"2018-10-10\"},\n",
    "    '2': {\n",
    "        \"26M_train\":  {\"train_start\": \"2017-04-01\", \"train_end\": \"2019-06-01\"},\n",
    "        \"24M_train\":  {\"train_start\": \"2017-04-01\", \"train_end\": \"2019-04-01\"},\n",
    "        \"22M_train\":  {\"train_start\": \"2017-04-01\", \"train_end\": \"2019-02-01\"},\n",
    "        \"20M_train\":  {\"train_start\": \"2017-04-01\", \"train_end\": \"2018-12-01\"},\n",
    "        \"18M_train\":  {\"train_start\": \"2017-04-01\", \"train_end\": \"2018-10-01\"},\n",
    "        \"12M_train\":  {\"train_start\": \"2017-04-01\", \"train_end\": \"2018-04-01\"},\n",
    "        \"10M_train\":  {\"train_start\": \"2017-04-01\", \"train_end\": \"2018-01-25\"},\n",
    "        \"8M_train\":   {\"train_start\": \"2017-04-01\", \"train_end\": \"2017-10-25\"},\n",
    "        \n",
    "        # Non-Heating Periods\n",
    "        \"NH_3M_train\":  {\"train_start\": \"2017-04-15\", \"train_end\": \"2017-07-15\"},\n",
    "        \"NH_4M_train\":  {\"train_start\": \"2017-04-15\", \"train_end\": \"2017-08-15\"},\n",
    "        \"NH_2M_train\":  {\"train_start\": \"2017-04-15\", \"train_end\": \"2017-06-15\"},\n",
    "        \"NH_1M_train\":  {\"train_start\": \"2017-04-15\", \"train_end\": \"2017-05-15\"},\n",
    "        \"NH_15D_train\": {\"train_start\": \"2017-04-15\", \"train_end\": \"2017-04-30\"},\n",
    "        \"NH_feb_2M_train\": {\"train_start\": \"2017-02-15\", \"train_end\": \"2017-04-15\"},\n",
    "        \"NH_feb_1M_train\": {\"train_start\": \"2017-02-15\", \"train_end\": \"2017-04-15\"},\n",
    "        \"NH_mar_2M_train\": {\"train_start\": \"2017-03-15\", \"train_end\": \"2017-05-15\"},\n",
    "        \"NH_mar_1M_train\": {\"train_start\": \"2017-03-15\", \"train_end\": \"2017-04-15\"},\n",
    "\n",
    "        # Heating Periods\n",
    "        \"H_5M_train\":     {\"train_start\": \"2017-06-01\", \"train_end\": \"2017-11-01\"},\n",
    "        \"H_3M_jul_train\": {\"train_start\": \"2017-07-01\", \"train_end\": \"2017-10-10\"},\n",
    "        \"H_3M_sep_train\": {\"train_start\": \"2017-09-01\", \"train_end\": \"2017-12-10\"},\n",
    "        \"H_3M_nov_train\": {\"train_start\": \"2017-11-01\", \"train_end\": \"2018-02-10\"},\n",
    "        },\n",
    "}\n",
    "scenarios_sensors['5'] = scenarios_sensors['2'].copy()\n",
    "scenarios_sensors['6'] = scenarios_sensors['2'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MLForecastPipeline import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df, scenario, date_col=\"ds\"):\n",
    "    \"\"\"Extracts train and test data based on train end date.\"\"\"\n",
    "    train_data = df[df[date_col] <= scenario['train_end']]\n",
    "    test_start = pd.to_datetime(scenario['train_end']) + pd.Timedelta(days=1)\n",
    "    test_data = df[df[date_col] >= test_start]\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsforecast import StatsForecast\n",
    "from statsforecast.models import (\n",
    "    AutoARIMA, AutoETS, AutoCES, AutoTheta, CrostonClassic, CrostonSBA,\n",
    "    AutoTBATS, SeasonalExponentialSmoothingOptimized, HoltWinters, Holt,\n",
    "    SeasonalWindowAverage, ADIDA, CrostonOptimized, IMAPA, TSB,\n",
    "    MSTL, MFLES, OptimizedTheta, DynamicOptimizedTheta, GARCH\n",
    ")\n",
    "models_dict = {\n",
    "    \"AutoARIMA\": AutoARIMA(season_length=365),\n",
    "    \"AutoETS\": AutoETS(season_length=365),\n",
    "    \"AutoCES\": AutoCES(season_length=365),\n",
    "    \"AutoTheta\": AutoTheta(season_length=365),\n",
    "    \"CrostonClassic\": CrostonClassic(),\n",
    "    \"CrostonSBA\": CrostonSBA(),\n",
    "    \"AutoTBATS\": AutoTBATS(season_length=365),\n",
    "    \"SeasonalExponentialSmoothingOptimized\": SeasonalExponentialSmoothingOptimized(season_length=365),\n",
    "    \"HoltWinters\": HoltWinters(season_length=365),\n",
    "    \"Holt\": Holt(),\n",
    "    \"SeasonalWindowAverage1\": SeasonalWindowAverage(season_length=365, window_size=30),\n",
    "    \"SeasonalWindowAverage2\": SeasonalWindowAverage(season_length=365, window_size=60),\n",
    "    \"SeasonalWindowAverage3\": SeasonalWindowAverage(season_length=365, window_size=15),\n",
    "    \"ADIDA\": ADIDA(),\n",
    "    \"CrostonOptimized\": CrostonOptimized(),\n",
    "    \"IMAPA\": IMAPA(),\n",
    "    # \"TSB\": TSB(),\n",
    "    \"MSTL_AdditiveB\": MSTL(season_length=[7, 30, 365], trend_forecaster=AutoARIMA()),\n",
    "    \"MSTL_Additive\": MSTL(season_length=[7, 365], trend_forecaster=AutoARIMA()),\n",
    "    \"MSTL_MultiplicativeB\": MSTL(season_length=[7, 30, 365], trend_forecaster=AutoARIMA()),\n",
    "    \"MSTL_Multiplicative\": MSTL(season_length=[30, 365], trend_forecaster=AutoARIMA()),\n",
    "    \"MFLES1\": MFLES(season_length=365),\n",
    "    \"MFLES2\": MFLES(season_length=365, fourier_order=3),\n",
    "    \"OptimizedTheta\": OptimizedTheta(season_length=365),\n",
    "    \"DynamicOptimizedTheta1\": DynamicOptimizedTheta(season_length=365),\n",
    "    \"DynamicOptimizedTheta2\": DynamicOptimizedTheta(season_length=365, decomposition_type='additive'),\n",
    "    \"GARCH\": GARCH()\n",
    "}\n",
    "\n",
    "class LogTransform:\n",
    "    def fit_transform(self, y):\n",
    "        self.min_ = y.min()\n",
    "        if self.min_ <= 0:\n",
    "            y = y - self.min_ + 1\n",
    "        self.transformed_ = np.log(y)\n",
    "        return self.transformed_\n",
    "\n",
    "    def transform(self, y):\n",
    "        if self.min_ <= 0:\n",
    "            y = y - self.min_ + 1\n",
    "        return np.log(y)\n",
    "\n",
    "    def inverse_transform(self, y):\n",
    "        return np.exp(y) + self.min_ - 1 if self.min_ <= 0 else np.exp(y)\n",
    "\n",
    "def evaluate_models_statsforecast(train_df, test_df, target_transforms):\n",
    "    results = []\n",
    "\n",
    "    log_transform = LogTransform()\n",
    "    target_transforms.append(log_transform)\n",
    "\n",
    "    valid_transform_combinations = [()] + list(chain(combinations(target_transforms, 1), combinations(target_transforms, 2)))\n",
    "    valid_transform_combinations = [tc for tc in valid_transform_combinations if filter_conflicting_transforms(tc)]\n",
    "    max_test_length = len(test_df)\n",
    "    test_lengths = [t for t in range(30, 181, 30)] + [240, 300, 360, 480, 600, 720, max_test_length]\n",
    "    test_lengths = [t for t in test_lengths if t <= max_test_length]\n",
    "    total_fits = len(valid_transform_combinations) * len(models_dict)\n",
    "    print(f\"Total model fits to run: {total_fits}\")\n",
    "    fit_num = 0\n",
    "\n",
    "    for model_name, model in models_dict.items():\n",
    "        for transform_combination in valid_transform_combinations:\n",
    "            print(f\"{fit_num + 1}/{total_fits} Training {model_name} with transforms: {stringify_transform(transform_combination)}...\")\n",
    "            try:\n",
    "                train_ga = dataframe_to_grouped_array(train_df, 'unique_id', 'y')\n",
    "                transformed_train_ga = apply_transformations(train_ga, transform_combination)\n",
    "                transformed_train_df = train_df.copy()\n",
    "                transformed_train_df['y'] = transformed_train_ga.data\n",
    "                sf = StatsForecast(models=[model], freq='D', n_jobs=-1)\n",
    "                sf.fit(transformed_train_df)\n",
    "                forecasts = sf.predict(h=max_test_length)\n",
    "                transformed_forecast_ga = dataframe_to_grouped_array(forecasts, 'unique_id', 'y')\n",
    "                inverse_transformed_forecast_ga = inverse_transformations(transformed_forecast_ga, transform_combination)\n",
    "                test_df_copy = test_df.copy()\n",
    "                test_df_copy['forecast'] = inverse_transformed_forecast_ga.data\n",
    "                error_dict = {}\n",
    "                for test_length in test_lengths:\n",
    "                    eval_subset = test_df_copy.iloc[:test_length]\n",
    "                    error_dict[f\"test_{test_length}_days\"] = mape_met(eval_subset['y'].values, eval_subset['forecast'].values)\n",
    "                results.append({\n",
    "                    \"Model\": model_name,\n",
    "                    \"Transforms\": stringify_transform(list(transform_combination)),\n",
    "                    **error_dict\n",
    "                })\n",
    "                print(f\"{model_name} MAPE: {error_dict[f'test_{max_test_length}_days']:.2f}% with transforms {stringify_transform(list(transform_combination))}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping combination {fit_num + 1} due to error: {e}\")\n",
    "            fit_num += 1\n",
    "\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "import time\n",
    "\n",
    "def process_scenario(sensor_name, scenario_name, scenario, selected_sensors_df):\n",
    "    \"\"\" Process each scenario independently and save results. \"\"\"\n",
    "    print(f'{sensor_name}_{scenario_name}')\n",
    "    formatted_df = format_df_to_mlforecast(selected_sensors_df[['full_date', sensor_name]], 'full_date', sensor_name, unique_id=sensor_name)\n",
    "    formatted_df = formatted_df[['ds', 'y', 'unique_id']]\n",
    "    \n",
    "    train_df, test_df = split_data(formatted_df, scenario)\n",
    "    target_transforms = get_dynamic_transforms(train_df)\n",
    "\n",
    "    results = evaluate_models_statsforecast(train_df, test_df, target_transforms)\n",
    "\n",
    "    save_results(results, f\"results/run_5/{sensor_name}_{scenario_name}.csv\")\n",
    "\n",
    "    return results\n",
    "\n",
    "def run_all_scenarios_parallel(scenarios_sensors, selected_sensors_df):\n",
    "    # don't use all cpus (instead all but one)\n",
    "    n_jobs = max(1, multiprocessing.cpu_count() - 1)\n",
    "    results = Parallel(n_jobs=n_jobs)( \n",
    "        delayed(process_scenario)(sensor_name, scenario_name, scenario, selected_sensors_df )\n",
    "        for sensor_name, scenarios in scenarios_sensors.items()\n",
    "        for scenario_name, scenario in scenarios.items()\n",
    "    )\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_all_scenarios_parallel(scenarios_sensors, selected_sensors_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
