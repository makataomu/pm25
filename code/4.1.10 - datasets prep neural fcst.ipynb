{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import njit\n",
    "from window_ops.expanding import expanding_mean\n",
    "from window_ops.rolling import rolling_mean\n",
    "\n",
    "@njit\n",
    "def rolling_mean_14(x):\n",
    "    return rolling_mean(x, window_size=14)\n",
    "@njit\n",
    "def rolling_mean_30(x):\n",
    "    return rolling_mean(x, window_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_df_to_mlforecast(df, date_col, target_col, unique_id='mean'):\n",
    "    df_ = df.rename({\n",
    "        date_col: \"ds\",\n",
    "        # target_col: 'y',\n",
    "    }, axis=1)\n",
    "\n",
    "    df_['ds'] = pd.to_datetime(df_['ds'])\n",
    "\n",
    "    df_['y'] = df_[target_col].copy()\n",
    "    # df_.drop(columns=target_col)\n",
    "\n",
    "    df_['unique_id'] = unique_id\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_sensors_df = pd.read_csv(\"../data/selected_sensors2_cleaned.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios_sensors = {\n",
    "    # 0: 1, 4372603\n",
    "    # \"0_12M_train_7M_test\": {\"train_start\": \"2017-03-25\", \"train_end\": \"2018-03-25\", \"test_start\": \"2018-03-26\", \"test_end\": \"2018-10-10\"},\n",
    "    '2': {\n",
    "        \"26M_train\":  {\"train_start\": \"2017-04-01\", \"train_end\": \"2019-06-01\"},\n",
    "        \"24M_train\":  {\"train_start\": \"2017-04-01\", \"train_end\": \"2019-04-01\"},\n",
    "        \"22M_train\":  {\"train_start\": \"2017-04-01\", \"train_end\": \"2019-02-01\"},\n",
    "        \"20M_train\":  {\"train_start\": \"2017-04-01\", \"train_end\": \"2018-12-01\"},\n",
    "        \"18M_train\":  {\"train_start\": \"2017-04-01\", \"train_end\": \"2018-10-01\"},\n",
    "        \"12M_train\":  {\"train_start\": \"2017-04-01\", \"train_end\": \"2018-04-01\"},\n",
    "        # \"10M_train\":  {\"train_start\": \"2017-04-01\", \"train_end\": \"2018-01-25\"},\n",
    "        # \"8M_train\":   {\"train_start\": \"2017-04-01\", \"train_end\": \"2017-10-25\"},\n",
    "        },\n",
    "}\n",
    "scenarios_sensors['5'] = scenarios_sensors['2'].copy()\n",
    "# scenarios_sensors['6'] = scenarios_sensors['2'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MLForecastPipeline import *\n",
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso, SGDRegressor\n",
    "def split_data(df, scenario, date_col=\"ds\"):\n",
    "    \"\"\"Extracts train and test data based on train end date.\"\"\"\n",
    "    train_data = df[df[date_col] <= scenario['train_end']]\n",
    "    test_start = pd.to_datetime(scenario['train_end']) + pd.Timedelta(days=1)\n",
    "    test_data = df[df[date_col] >= test_start]\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import time\n",
    "from neuralforecast.auto import AutoLSTM\n",
    "from neuralforecast.auto import AutoMLP\n",
    "from neuralforecast.tsdataset import TimeSeriesDataset\n",
    "\n",
    "def make_optuna_config_lstm(max_input_size, debug_mode=False):\n",
    "    max_steps_range = [500, 1000]\n",
    "    lr_range = [1e-4, 1e-1]\n",
    "    if debug_mode:\n",
    "        max_steps_range = [9,10]\n",
    "        lr_range = [0.5,0.6]\n",
    "    def optuna_config_lstm(trial):\n",
    "        return {\n",
    "            \"input_size\": trial.suggest_int(\"input_size\", 7, max_input_size),\n",
    "            # \"h\": trial.suggest_int(\"input_size\", 7, max_input_size),     # will be set externally too\n",
    "            \"encoder_hidden_size\": trial.suggest_categorical(\"encoder_hidden_size\", [16, 32, 64, 128]),\n",
    "            \"encoder_n_layers\": trial.suggest_int(\"encoder_n_layers\", 1, 3),\n",
    "            \"decoder_hidden_size\": trial.suggest_categorical(\"decoder_hidden_size\", [16, 32, 64, 128]),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", lr_range[0], lr_range[1], log=True),\n",
    "            \"max_steps\": trial.suggest_categorical(\"max_steps\", max_steps_range),\n",
    "            \"batch_size\": trial.suggest_categorical(\"batch_size\", [16, 32, 64]),\n",
    "            \"random_seed\": trial.suggest_int(\"random_seed\", 1, 19),\n",
    "            \"start_padding_enabled\": True\n",
    "        }\n",
    "    return optuna_config_lstm\n",
    "\n",
    "def make_optuna_config_mlp(max_input_size, debug_mode=False):\n",
    "    max_steps_range = [500, 1000]\n",
    "    if debug_mode:\n",
    "        max_steps_range = [9,10]\n",
    "    def optuna_config_mlp(trial):\n",
    "        return {\n",
    "            \"input_size\": trial.suggest_int(\"input_size\", 7, max_input_size),\n",
    "            \"step_size\": trial.suggest_int(\"step_size\", 1, max_input_size),\n",
    "            \"hidden_size\": trial.suggest_categorical(\"hidden_size\", [256, 512, 1024]),\n",
    "            \"num_layers\": trial.suggest_int(\"num_layers\", 2, 5),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 1e-1, log=True),\n",
    "            \"scaler_type\": trial.suggest_categorical(\"scaler_type\", [None, \"robust\", \"standard\"]),\n",
    "            \"max_steps\": trial.suggest_categorical(\"max_steps\", max_steps_range),\n",
    "            \"batch_size\": trial.suggest_categorical(\"batch_size\", [32, 64, 128, 256]),\n",
    "            \"windows_batch_size\": trial.suggest_categorical(\"windows_batch_size\", [128, 256, 512, 1024]),\n",
    "            \"random_seed\": trial.suggest_int(\"random_seed\", 1, 19),\n",
    "            \"start_padding_enabled\": True\n",
    "        }\n",
    "    return optuna_config_mlp\n",
    "\n",
    "\n",
    "def count_metrics(model_name, params, test_df, y_hat, test_lengths):\n",
    "    test_df_copy = test_df.copy()\n",
    "    test_df_copy['forecast'] = y_hat\n",
    "\n",
    "    error_dict = {}\n",
    "    for test_length in test_lengths:\n",
    "        eval_subset = test_df_copy.iloc[:test_length]  # Take subset for evaluation\n",
    "        error_dict[f\"test_{test_length}_days\"] = mape_met(eval_subset['y'].values,  eval_subset['forecast'].values)\n",
    "\n",
    "    monthly_error_dict = defaultdict(dict)\n",
    "    test_df_copy['year'] = test_df_copy['ds'].dt.year\n",
    "    test_df_copy['month'] = test_df_copy['ds'].dt.month\n",
    "\n",
    "    # Group by year and month and calculate MAPE for each group\n",
    "    grouped = test_df_copy.groupby(['year', 'month'])\n",
    "    for (year, month), group in grouped:\n",
    "        if not group.empty:\n",
    "            monthly_error_dict[year][month] = mape_met(group['y'].values, group['forecast'].values)\n",
    "\n",
    "    return {\n",
    "        \"Model\": model_name,\n",
    "        **error_dict,  # Expand error dictionary into separate columns\n",
    "        **monthly_error_dict,\n",
    "        \"preds\": test_df_copy['forecast'].values,\n",
    "        \"params\": params,\n",
    "    }\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from neuralforecast.tsdataset import TimeSeriesDataset\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "log_dir = r\"C:\\Users\\PC314\\Documents\\tair\\pm25\\code\\lightning_logs\"\n",
    "\n",
    "# Delete the folder if it already exists\n",
    "if os.path.exists(log_dir):\n",
    "    shutil.rmtree(log_dir)\n",
    "\n",
    "def recursive_forecast(model, train_df, sensor_name, steps, step_size, freq='D'):\n",
    "    \"\"\"\n",
    "    Given a trained AutoLSTM/AutoMLP with h=1, produce `steps` forecasts\n",
    "    by feeding back each 1-step prediction as input for the next.\n",
    "    \"\"\"\n",
    "    history = train_df.copy()\n",
    "    preds = []\n",
    "    remaining = steps\n",
    "    \n",
    "    while remaining > 0:\n",
    "        this_chunk = min(step_size, remaining)\n",
    "\n",
    "        # build dataset on current history\n",
    "        dataset_hist, _, _, _ = TimeSeriesDataset.from_df(df=history, id_col='unique_id', time_col='ds', target_col='y')\n",
    "        # predict this_chunk steps ahead\n",
    "        yhat = model.predict(dataset=dataset_hist, step_size=this_chunk)\n",
    "        # yhat includes the last this_chunk points\n",
    "        next_preds = yhat[-this_chunk:].tolist()\n",
    "        next_preds = [p[0] for p in next_preds]\n",
    "        preds.extend(next_preds)\n",
    "\n",
    "        # append those predictions to history with correct dates\n",
    "        last_date = history['ds'].max()\n",
    "        for i, p in enumerate(next_preds, start=1):\n",
    "            next_date = last_date + pd.Timedelta(days=i)\n",
    "            history = pd.concat([\n",
    "                history,\n",
    "                pd.DataFrame({\n",
    "                    'ds': [next_date],\n",
    "                    'unique_id': sensor_name,\n",
    "                    'y': [float(p)]\n",
    "                })\n",
    "            ], ignore_index=True)\n",
    "        remaining -= this_chunk\n",
    "        \n",
    "    return np.array(preds)\n",
    "\n",
    "def process_scenario(sensor_name, scenario_name, scenario, selected_sensors_df, models, lag_transforms_options, ratios=[0.33, 0.66, 1], num_samples=20, debug_mode=False):\n",
    "    from collections import defaultdict\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    print(f'{sensor_name}_{scenario_name}')\n",
    "    results = []\n",
    "\n",
    "    formatted_df = format_df_to_mlforecast(selected_sensors_df[['full_date', sensor_name]], 'full_date', sensor_name, unique_id=sensor_name)\n",
    "    formatted_df = formatted_df[['ds', 'y', 'unique_id']]\n",
    "    train_df, test_df = split_data(formatted_df, scenario)\n",
    "    train_df['ds'] = pd.to_datetime(train_df['ds'])\n",
    "    test_df['ds'] = pd.to_datetime(test_df['ds'])\n",
    "\n",
    "    horizon_values = [1, 7, 30, 90, 180]\n",
    "    forecast_horizon = len(test_df)\n",
    "\n",
    "    for h in horizon_values:\n",
    "        if os.path.exists(log_dir):\n",
    "            shutil.rmtree(log_dir)\n",
    "        # Validate that input_size can support h\n",
    "        min_train_len = train_df.groupby(\"unique_id\").size().min()\n",
    "        min_required_buffer = 20\n",
    "        safe_max_input_size = max(min_train_len - h - min_required_buffer, 7)\n",
    "        if safe_max_input_size < 7:\n",
    "            print(f\"Skipping h={h} due to insufficient training length.\")\n",
    "            continue\n",
    "\n",
    "        # Create extended df for forecasting future\n",
    "        future_dates = pd.date_range(start=train_df['ds'].max() + pd.Timedelta(days=1), periods=forecast_horizon, freq='D')\n",
    "        future_df = pd.DataFrame({'ds': future_dates, 'unique_id': sensor_name, 'y': np.nan})\n",
    "        df_for_forecasting = pd.concat([train_df, future_df], ignore_index=True)\n",
    "\n",
    "        dataset, indices, dates, ds_arr  = TimeSeriesDataset.from_df(df=train_df, id_col=\"unique_id\", time_col=\"ds\", target_col=\"y\")\n",
    "        dataset_future, indices, dates, ds_arr  = TimeSeriesDataset.from_df(df=df_for_forecasting, id_col=\"unique_id\", time_col=\"ds\", target_col=\"y\")\n",
    "\n",
    "        test_lengths = list(range(30, 181, 30)) + [240, 300, 360, 480, 600, 720, forecast_horizon]\n",
    "\n",
    "        assert future_df['ds'].min() > train_df['ds'].max()\n",
    "        assert pd.api.types.is_datetime64_any_dtype(df_for_forecasting['ds'])\n",
    "\n",
    "        # LSTM\n",
    "        optuna_config = make_optuna_config_lstm(safe_max_input_size, debug_mode)\n",
    "        model_lstm = AutoLSTM(h=h, num_samples=num_samples, backend='optuna', config=optuna_config)\n",
    "        model_lstm.fit(dataset=dataset, distributed_config=None)\n",
    "        # y_hat = model_lstm.model.predict(dataset=dataset_future)\n",
    "        y_hat = recursive_forecast( model_lstm, train_df, sensor_name, steps=forecast_horizon, step_size=h, freq='D')\n",
    "\n",
    "        # print('future_df:', future_df.shape)\n",
    "        # print('yhat: ', len(y_hat))\n",
    "        y_hat = y_hat[-forecast_horizon:]\n",
    "        # print('yhat: ', len(y_hat), y_hat[-10:])\n",
    "        results.append(count_metrics(f'lstm_h={h}', model_lstm.model.hparams, test_df, y_hat, test_lengths))\n",
    "    \n",
    "        # MLP\n",
    "        optuna_config = make_optuna_config_mlp(safe_max_input_size, debug_mode)\n",
    "        model_mlp = AutoMLP(h=h, num_samples=num_samples, backend='optuna', config=optuna_config)\n",
    "        model_mlp.fit(dataset=dataset, distributed_config=None)\n",
    "        y_hat = recursive_forecast( model_mlp, train_df, sensor_name, steps=forecast_horizon, step_size=h, freq='D')\n",
    "        y_hat = y_hat[-forecast_horizon:]\n",
    "        results.append(count_metrics(f'mlp_h={h}', model_mlp.model.hparams, test_df, y_hat, test_lengths))\n",
    "\n",
    "    save_results(results, f\"results/run_20/{sensor_name}_{scenario_name}.csv\")\n",
    "    return results\n",
    "\n",
    "def run_all_scenarios_parallel(scenarios_sensors, selected_sensors_df, models, lag_transforms_options, ratios=[0.33, 0.66, 1], num_samples=20):\n",
    "    # don't use all cpus (instead all but one)\n",
    "    results = Parallel(n_jobs=-1, verbose=30)( \n",
    "        delayed(process_scenario)(sensor_name, scenario_name, scenario, selected_sensors_df, models, lag_transforms_options, ratios=ratios, num_samples=num_samples)\n",
    "        for sensor_name, scenarios in scenarios_sensors.items()\n",
    "        for scenario_name, scenario in scenarios.items()\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sensor_name, scenarios in scenarios_sensors.items():\n",
    "#     for scenario_name, scenario in scenarios.items():\n",
    "#         res = process_scenario(sensor_name, scenario_name, scenario, selected_sensors_df, models=None, lag_transforms_options=None, num_samples=1, debug_mode=True)\n",
    "#         raise KeyError('stop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] Невозможно создать файл, так как он уже существует: 'c:\\\\Users\\\\PC314\\\\Documents\\\\tair\\\\pm25\\\\code\\\\lightning_logs\\\\version_429'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"c:\\Users\\PC314\\anaconda3\\Lib\\site-packages\\tensorboardX\\record_writer.py\", line 47, in directory_check\n    factory = REGISTERED_FACTORIES[prefix]\n              ~~~~~~~~~~~~~~~~~~~~^^^^^^^^\nKeyError: 'c'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"c:\\Users\\PC314\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 463, in _process_worker\n    r = call_item()\n        ^^^^^^^^^^^\n  File \"c:\\Users\\PC314\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\PC314\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py\", line 598, in __call__\n    return [func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\PC314\\AppData\\Local\\Temp\\ipykernel_18968\\1431596794.py\", line 161, in process_scenario\n  File \"C:\\Users\\PC314\\AppData\\Local\\Temp\\ipykernel_18968\\1431596794.py\", line 95, in recursive_forecast\n  File \"c:\\Users\\PC314\\anaconda3\\Lib\\site-packages\\neuralforecast\\common\\_base_auto.py\", line 462, in predict\n    return self.model.predict(dataset=dataset, step_size=step_size, **data_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\PC314\\anaconda3\\Lib\\site-packages\\neuralforecast\\common\\_base_model.py\", line 1526, in predict\n    fcsts = trainer.predict(self, datamodule=datamodule)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\PC314\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 859, in predict\n    return call._call_and_handle_interrupt(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\PC314\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py\", line 47, in _call_and_handle_interrupt\n    return trainer_fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\PC314\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 898, in _predict_impl\n    results = self._run(model, ckpt_path=ckpt_path)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\PC314\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 944, in _run\n    call._call_setup_hook(self)  # allow user to set up LightningModule in accelerator environment\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\PC314\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py\", line 96, in _call_setup_hook\n    if hasattr(logger, \"experiment\"):\n       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\PC314\\anaconda3\\Lib\\site-packages\\lightning_fabric\\loggers\\logger.py\", line 118, in experiment\n    return fn(self)\n           ^^^^^^^^\n  File \"c:\\Users\\PC314\\anaconda3\\Lib\\site-packages\\lightning_fabric\\loggers\\tensorboard.py\", line 195, in experiment\n    self._experiment = SummaryWriter(log_dir=self.log_dir, **self._kwargs)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\PC314\\anaconda3\\Lib\\site-packages\\tensorboardX\\writer.py\", line 300, in __init__\n    self._get_file_writer()\n  File \"c:\\Users\\PC314\\anaconda3\\Lib\\site-packages\\tensorboardX\\writer.py\", line 348, in _get_file_writer\n    self.file_writer = FileWriter(logdir=self.logdir,\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\PC314\\anaconda3\\Lib\\site-packages\\tensorboardX\\writer.py\", line 104, in __init__\n    self.event_writer = EventFileWriter(\n                        ^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\PC314\\anaconda3\\Lib\\site-packages\\tensorboardX\\event_file_writer.py\", line 104, in __init__\n    directory_check(self._logdir)\n  File \"c:\\Users\\PC314\\anaconda3\\Lib\\site-packages\\tensorboardX\\record_writer.py\", line 51, in directory_check\n    os.makedirs(path)\n  File \"<frozen os>\", line 225, in makedirs\nFileExistsError: [WinError 183] Невозможно создать файл, так как он уже существует: 'c:\\\\Users\\\\PC314\\\\Documents\\\\tair\\\\pm25\\\\code\\\\lightning_logs\\\\version_429'\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m run_all_scenarios_parallel(scenarios_sensors, selected_sensors_df, models\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, lag_transforms_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 182\u001b[0m, in \u001b[0;36mrun_all_scenarios_parallel\u001b[1;34m(scenarios_sensors, selected_sensors_df, models, lag_transforms_options, ratios, num_samples)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_all_scenarios_parallel\u001b[39m(scenarios_sensors, selected_sensors_df, models, lag_transforms_options, ratios\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.33\u001b[39m, \u001b[38;5;241m0.66\u001b[39m, \u001b[38;5;241m1\u001b[39m], num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m):\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;66;03m# don't use all cpus (instead all but one)\u001b[39;00m\n\u001b[1;32m--> 182\u001b[0m     results \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m)( \n\u001b[0;32m    183\u001b[0m         delayed(process_scenario)(sensor_name, scenario_name, scenario, selected_sensors_df, models, lag_transforms_options, ratios\u001b[38;5;241m=\u001b[39mratios, num_samples\u001b[38;5;241m=\u001b[39mnum_samples)\n\u001b[0;32m    184\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m sensor_name, scenarios \u001b[38;5;129;01min\u001b[39;00m scenarios_sensors\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    185\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m scenario_name, scenario \u001b[38;5;129;01min\u001b[39;00m scenarios\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    186\u001b[0m     )\n\u001b[0;32m    188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[1;32mc:\\Users\\PC314\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[1;32mc:\\Users\\PC314\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\PC314\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1754\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_retrieval():\n\u001b[0;32m   1748\u001b[0m \n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[0;32m   1752\u001b[0m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborting:\n\u001b[1;32m-> 1754\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_error_fast()\n\u001b[0;32m   1755\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1757\u001b[0m     \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m     \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\PC314\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1789\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1785\u001b[0m \u001b[38;5;66;03m# If this error job exists, immediately raise the error by\u001b[39;00m\n\u001b[0;32m   1786\u001b[0m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[0;32m   1788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1789\u001b[0m     error_job\u001b[38;5;241m.\u001b[39mget_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout)\n",
      "File \u001b[1;32mc:\\Users\\PC314\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:745\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    739\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39m_backend\n\u001b[0;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39msupports_retrieve_callback:\n\u001b[0;32m    742\u001b[0m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[0;32m    743\u001b[0m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[0;32m    744\u001b[0m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[1;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return_or_raise()\n\u001b[0;32m    747\u001b[0m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\PC314\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:763\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    762\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m TASK_ERROR:\n\u001b[1;32m--> 763\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    765\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mFileExistsError\u001b[0m: [Errno 17] Невозможно создать файл, так как он уже существует: 'c:\\\\Users\\\\PC314\\\\Documents\\\\tair\\\\pm25\\\\code\\\\lightning_logs\\\\version_429'"
     ]
    }
   ],
   "source": [
    "results = run_all_scenarios_parallel(scenarios_sensors, selected_sensors_df, models=None, lag_transforms_options=None, num_samples=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "cpu_count = multiprocessing.cpu_count()\n",
    "cpu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through scenarios and evaluate models\n",
    "results = []\n",
    "\n",
    "for sensor_name, scenarios in scenarios_sensors.items():\n",
    "    formatted_df = format_df_to_mlforecast(selected_sensors_df[['full_date', sensor_name]], 'full_date', sensor_name, unique_id=sensor_name)\n",
    "    formatted_df = formatted_df[['ds', 'y', 'unique_id']]\n",
    "\n",
    "    for scenario_name, scenario in scenarios.items():\n",
    "\n",
    "        train_df, test_df = split_data(formatted_df, scenario)\n",
    "\n",
    "        optimal_lags_list = get_optimal_lags(train_df, 'y', \n",
    "                                            # ratios=[1]\n",
    "                                            ratios=[0.33, 0.66, 1]\n",
    "                                            #  ratios=[0.25, 0.5, 0.75, 1]\n",
    "        )\n",
    "        target_transforms = get_dynamic_transforms(train_df)\n",
    "        results = evaluate_models(train_df, test_df, models, target_transforms, lag_transforms_options, optimal_lags_list, winter_weights=True)\n",
    "\n",
    "        save_results(results, f\"results/run_13/{sensor_name}_{scenario_name}.csv\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
