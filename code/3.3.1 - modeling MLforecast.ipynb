{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../data/total_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={\"full_date\": \"date_time\",\n",
    "                  \"pm\": \"value\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import holt_winters_imputation_and_expand, plot_imputation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sktime\n",
    "from matplotlib import pyplot\n",
    "import matplotlib as plt\n",
    "import seaborn\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import statsmodels\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index(\"date_time\", inplace=True)\n",
    "\n",
    "df.index = pd.to_datetime(df.index)\n",
    "\n",
    "# Generate the complete range of dates\n",
    "full_range = pd.date_range(start=df.index.min(), end=df.index.max(), freq='D')\n",
    "\n",
    "# Reindex the DataFrame to include all dates\n",
    "df = df.reindex(full_range)\n",
    "\n",
    "# Set the index name back (optional)\n",
    "df.index.name = 'date_time'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['value'] = holt_winters_imputation_and_expand(\n",
    "#     df['value'],\n",
    "#     seasonal_periods=365,\n",
    "# )\n",
    "# df.to_csv(\"../data/imputed_mean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/imputed_mean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index(\"date_time\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_date = \"2020-01-01\"\n",
    "df_train = df[df.index < test_date].copy()\n",
    "df_test = df[df.index >= test_date].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / (y_true + 1e-9))) * 100\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def evaluate_on_test(y_hat, y_true):\n",
    "    return mape(y_true, y_hat), np.sqrt(mean_squared_error(y_hat, y_true))\n",
    "\n",
    "def plot_preds(y_hat, y_true):\n",
    "    predictions = pd.DataFrame({\n",
    "        \"Preds\": y_hat,\n",
    "        \"Actual\": y_true,\n",
    "        # \"train\": t['value']\n",
    "    })\n",
    "    _, ax = pyplot.subplots()\n",
    "    ax = predictions.plot(ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsforecast.models import SeasonalNaive\n",
    "from statsforecast.core import StatsForecast\n",
    "import pandas as pd\n",
    "\n",
    "# Prepare your data\n",
    "data = pd.DataFrame({'ds': df_train.index,\n",
    "                     'y': df_train['value'].values})\n",
    "data['unique_id'] = \"mean\"\n",
    "\n",
    "data['ds'] = pd.to_datetime(data['ds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare your data\n",
    "data_test = pd.DataFrame({'ds': df_test.index,\n",
    "                     'y': df_test['value'].values})\n",
    "data_test['unique_id'] = \"mean\"\n",
    "\n",
    "data_test['ds'] = pd.to_datetime(data_test['ds'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML models basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(series, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Splits the series into train and test sets.\n",
    "\n",
    "    Parameters:\n",
    "        series (array-like): The transformed series to split.\n",
    "        test_size (float): Proportion of the series to include in the test set (default: 0.2).\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (train_series, test_series)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        n = len(series)\n",
    "        test_count = int(n * test_size)\n",
    "        train_series = series[:-test_count]\n",
    "        test_series = series[-test_count:]\n",
    "        return train_series, test_series\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"Failed to split series into train and test sets: {e}\")\n",
    "        return series, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# data = preprocessor.remove_seasonality(data)\n",
    "# data = data[~np.isnan(data)]\n",
    "# result = seasonal_decompose(data, model='additive', period=300)\n",
    "# result.plot()\n",
    "# pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlforecast import MLForecast\n",
    "from mlforecast.target_transforms import Differences\n",
    "from sklearn.linear_model import LinearRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlforecast.lag_transforms import ExpandingMean, RollingMean\n",
    "from numba import njit\n",
    "from window_ops.rolling import rolling_mean\n",
    "\n",
    "@njit\n",
    "def rolling_mean_48(x):\n",
    "    return rolling_mean(x, window_size=48)\n",
    "\n",
    "\n",
    "# fcst = MLForecast(\n",
    "#     models=[],\n",
    "#     freq='D',\n",
    "#     target_transforms=[Differences([365])],    \n",
    "#     lag_transforms={\n",
    "#         1: [ExpandingMean()],\n",
    "#         24: [RollingMean(window_size=48), rolling_mean_48],\n",
    "#     },\n",
    "# )\n",
    "# prep = fcst.preprocess(data)\n",
    "# prep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlforecast.target_transforms import BaseTargetTransform\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "from mlforecast.target_transforms import GlobalSklearnTransformer\n",
    "\n",
    "class LocalMinMaxScaler(BaseTargetTransform):\n",
    "    \"\"\"Scales each serie to be in the [0, 1] interval.\"\"\"\n",
    "    def fit_transform(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        self.stats_ = df.groupby(self.id_col)[self.target_col].agg(['min', 'max'])\n",
    "        df = df.merge(self.stats_, on=self.id_col)\n",
    "        df[self.target_col] = (df[self.target_col] - df['min']) / (df['max'] - df['min'])\n",
    "        df = df.drop(columns=['min', 'max'])\n",
    "        return df\n",
    "\n",
    "    def inverse_transform(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = df.merge(self.stats_, on=self.id_col)\n",
    "        for col in df.columns.drop([self.id_col, self.time_col, 'min', 'max']):\n",
    "            df[col] = df[col] * (df['max'] - df['min']) + df['min']\n",
    "        df = df.drop(columns=['min', 'max'])\n",
    "        return df\n",
    "\n",
    "sk_log1p = FunctionTransformer(func=np.log1p, inverse_func=np.expm1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from mlforecast import MLForecast\n",
    "from mlforecast.target_transforms import (BaseTargetTransform, Differences, AutoDifferences, AutoSeasonalDifferences, AutoSeasonalityAndDifferences,\n",
    "           LocalStandardScaler, LocalMinMaxScaler, LocalRobustScaler, LocalBoxCox, GlobalSklearnTransformer)\n",
    "from window_ops.expanding import expanding_mean\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from catboost import CatBoostRegressor \n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "train_df = data.copy()\n",
    "test_df = data_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MLForecast with transformations\n",
    "fcst = MLForecast(\n",
    "    models=[CatBoostRegressor(verbose=False), RandomForestRegressor(), Ridge(), Lasso(), XGBRegressor()],\n",
    "    freq='D',\n",
    "    lags=[1, 3, 7, 14, 30, 45, 60, 80, 90, 120, 150, 180, 365],  # Lag features\n",
    "    target_transforms=[\n",
    "        Differences([365]),  # First-order differencing\n",
    "        LocalStandardScaler()  # Local Standard Scaler normalization\n",
    "    ],\n",
    "    date_features=['dayofweek', 'month'],  # Additional time-based features\n",
    "    num_threads=1,\n",
    "    lag_transforms={\n",
    "        # 1: [ExpandingMean()],\n",
    "        # 7: [RollingMean(window_size=14)],\n",
    "    },\n",
    ")\n",
    "\n",
    "# Fit the model on training data\n",
    "fcst.fit(train_df)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = fcst.predict(h=len(test_df['y']))\n",
    "\n",
    "# Merge predictions with the test set for evaluation\n",
    "test_df['forecast'] = predictions['CatBoostRegressor']\n",
    "mape(test_df['y'].values, test_df['forecast'].values), \\\n",
    "plot_preds(test_df['forecast'].values, test_df['y'].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Splitting data: last observed value as prediction\n",
    "last_observed_value = data['y'].iloc[-1]  # Last known value\n",
    "\n",
    "# Creating a naive prediction (same value for past observed points)\n",
    "data['yhat'] = last_observed_value  # Naïve prediction\n",
    "\n",
    "# Calculating MAPE\n",
    "data = data.dropna()  # Ensure no NaNs before calculation\n",
    "mape = np.mean(np.abs((data['y'] - data['yhat']) / data['y'])) * 100\n",
    "\n",
    "print(f\"MAPE for Naïve Predictor: {mape:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from mlforecast import MLForecast\n",
    "from mlforecast.target_transforms import Differences\n",
    "from statsforecast.models import Naive\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Ensure your DataFrame has the right format\n",
    "# Define MLForecast instance with Naïve model\n",
    "forecast = MLForecast(\n",
    "    models={\"Naïve\": Naive()},\n",
    "    freq=\"D\",  # Adjust frequency as needed\n",
    "    lags=[1],  # Naïve method only needs last observation (lag 1)\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "forecast.fit(train_df)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = forecast.predict(h=len(test_df['y']))\n",
    "\n",
    "# Merge predictions with the test set for evaluation\n",
    "test_df['forecast'] = predictions['CatBoostRegressor']\n",
    "mape(test_df['y'].values, test_df['forecast'].values), \\\n",
    "plot_preds(test_df['forecast'].values, test_df['y'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mlforecast import MLForecast\n",
    "from mlforecast.target_transforms import (\n",
    "    Differences, AutoDifferences, AutoSeasonalDifferences, AutoSeasonalityAndDifferences,\n",
    "    LocalStandardScaler, LocalMinMaxScaler, LocalRobustScaler, LocalBoxCox\n",
    ")\n",
    "from window_ops.expanding import expanding_mean\n",
    "from window_ops.rolling import rolling_mean\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from itertools import combinations, chain\n",
    "\n",
    "def mape(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def plot_preds(y_pred, y_true):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(y_true, label='Actual', marker='o', linestyle='-')\n",
    "    plt.plot(y_pred, label='Forecast', marker='x', linestyle='--')\n",
    "    plt.legend()\n",
    "    plt.title('Forecast vs Actual')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Value')\n",
    "    plt.show()\n",
    "\n",
    "def select_important_lags(train_df, target_col, max_lags, model=RandomForestRegressor(n_estimators=50), num_of_lags=10):\n",
    "    \"\"\" Selects the most important lags based on feature importance analysis. \"\"\"\n",
    "    lagged_features = pd.concat([\n",
    "        train_df[target_col].shift(lag).rename(f'lag_{lag}') for lag in range(1, max_lags + 1)\n",
    "    ], axis=1)\n",
    "    \n",
    "    lagged_features.dropna(inplace=True)\n",
    "    y = train_df[target_col][max_lags:]\n",
    "    model.fit(lagged_features, y)\n",
    "    feature_importances = model.feature_importances_\n",
    "    important_lags = [i + 1 for i in np.argsort(feature_importances)[-num_of_lags:]]  # Select top lags\n",
    "    \n",
    "    return [int(x) for x in sorted(important_lags)]\n",
    "\n",
    "def filter_conflicting_transforms(transform_combination):\n",
    "    conflicting_transforms = {Differences, AutoDifferences, AutoSeasonalDifferences, AutoSeasonalityAndDifferences}\n",
    "    scaler_transforms = {LocalStandardScaler, LocalMinMaxScaler, LocalRobustScaler, LocalBoxCox}\n",
    "    \n",
    "    if sum(1 for t in transform_combination if type(t) in conflicting_transforms) > 1:\n",
    "        return False\n",
    "    if sum(1 for t in transform_combination if type(t) in scaler_transforms) > 1:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def evaluate_models(train_df, test_df, models, target_transforms, lag_transforms_options, optimal_lags_list, date_features=['dayofweek', 'month']):\n",
    "    best_model = None\n",
    "    best_error = float('inf')\n",
    "    best_transforms = None\n",
    "    best_lags = None\n",
    "    best_lag_transforms = None\n",
    "    results = {}\n",
    "    \n",
    "    valid_transform_combinations = list(chain(combinations(target_transforms, 1), combinations(target_transforms, 2)))\n",
    "    valid_transform_combinations = [tc for tc in valid_transform_combinations if filter_conflicting_transforms(tc)]\n",
    "\n",
    "    total_fits = len(models) * len(valid_transform_combinations) * len(optimal_lags_list) * len(lag_transforms_options)\n",
    "    print(f\"Total model fits to run: {total_fits}\")\n",
    "\n",
    "    fit_num = 0\n",
    "    for optimal_lags in optimal_lags_list:\n",
    "        for transform_combination in valid_transform_combinations:\n",
    "            for lag_transforms in lag_transforms_options:\n",
    "                for model_name, model in models.items():\n",
    "                    print(f\"{fit_num}/{total_fits} Training {model_name} with transforms {transform_combination}, lags {optimal_lags}, and lag_transforms {lag_transforms}...\")\n",
    "                    \n",
    "                    try:\n",
    "                        fcst = MLForecast(\n",
    "                            models=[model],\n",
    "                            freq='D',\n",
    "                            lags=optimal_lags,\n",
    "                            target_transforms=list(transform_combination),\n",
    "                            date_features=date_features,\n",
    "                            num_threads=1,\n",
    "                            lag_transforms=lag_transforms,\n",
    "                        )\n",
    "                        \n",
    "                        # Fit the model\n",
    "                        fcst.fit(train_df)\n",
    "                        \n",
    "                        # Predict\n",
    "                        predictions = fcst.predict(h=len(test_df['y']))\n",
    "                        \n",
    "                        # Store results\n",
    "                        test_df['forecast'] = predictions[model_name]\n",
    "                        error = mape(test_df['y'].values, test_df['forecast'].values)\n",
    "                        \n",
    "                        results[(model_name, transform_combination, tuple(optimal_lags), frozenset((k, tuple(v)) for k, v in lag_transforms.items()))] = error\n",
    "                        print(f\"{model_name} MAPE: {error:.2f}% with transforms {transform_combination}, lags {optimal_lags}, and lag_transforms {lag_transforms}\")\n",
    "                        \n",
    "                        if error < best_error:\n",
    "                            best_error = error\n",
    "                            best_model = model_name\n",
    "                            best_transforms = transform_combination\n",
    "                            best_lags = optimal_lags\n",
    "                            best_lag_transforms = lag_transforms\n",
    "                        \n",
    "                        # plot_preds(test_df['forecast'].values, test_df['y'].values)\n",
    "                    \n",
    "                    except Exception as e:\n",
    "                        print(f\"Skipping combination due to error: {e}\")\n",
    "                    fit_num += 1\n",
    "    \n",
    "    print(f\"Best Model: {best_model} with MAPE {best_error:.2f}% using transforms {best_transforms}, lags {best_lags}, and lag_transforms {best_lag_transforms}\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimal_lags = select_important_lags(train_df, 'y', max_lags=400, model=RandomForestRegressor(), num_of_lags=20)\n",
    "# optimal_lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_lags_list = [\n",
    "    [1, 7, 15, 18, 173, 200, 335, 368, 369, 379],\n",
    "    [1, 6, 11, 15, 173, 200, 335, 378, 379, 384],\n",
    "    [1, 6, 10, 11, 15, 182, 183, 185, 187, 193],\n",
    "    [1, 4, 7, 10, 14, 22, 173, 179, 182, 183, 184, 185, 187, 188, 193],\n",
    "    [1, 5, 6, 7, 11, 21, 116, 173, 180, 184, 187, 188, 334, 335, 368, 369, 373, 378, 379, 384],\n",
    "    \n",
    "    [1, 2, 5, 6, 9, 11, 12, 14, 131, 144, 146, 151, 183, 196, 210],\n",
    "    [1, 10, 11, 15, 16, 150, 172, 188, 198, 199, 222, 273, 336, 368, 384],\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"CatBoostRegressor\": CatBoostRegressor(verbose=False),\n",
    "    \"RandomForestRegressor\": RandomForestRegressor(n_estimators=100),\n",
    "    \"XGBRegressor\": XGBRegressor(),\n",
    "    # \"LGBMRegressor\": LGBMRegressor(),\n",
    "    # \"GradientBoostingRegressor\": GradientBoostingRegressor(),\n",
    "    \"Ridge\": Ridge(),\n",
    "    \"Lasso\": Lasso()\n",
    "}\n",
    "\n",
    "target_transforms = [\n",
    "    Differences([365]),  # some of them shoudln't be together\n",
    "    AutoDifferences(380), \n",
    "    AutoSeasonalDifferences(season_length=365, max_diffs=380), \n",
    "    AutoSeasonalityAndDifferences(max_season_length=380, max_diffs=380),\n",
    "    LocalStandardScaler(), \n",
    "    LocalMinMaxScaler(), \n",
    "    LocalRobustScaler('mad'), \n",
    "    LocalRobustScaler('iqr'), \n",
    "    LocalBoxCox()\n",
    "]\n",
    "\n",
    "@njit\n",
    "def rolling_mean_14(x):\n",
    "    return rolling_mean(x, window_size=14)\n",
    "@njit\n",
    "def rolling_mean_30(x):\n",
    "    return rolling_mean(x, window_size=30)\n",
    "\n",
    "lag_transforms_options = [\n",
    "    {1: [expanding_mean], 7: [expanding_mean, rolling_mean_14], 30: [rolling_mean_30]},\n",
    "    {1: [expanding_mean], 7: [rolling_mean_14], 30: [expanding_mean]},\n",
    "    {1: [rolling_mean_14], 7: [rolling_mean_30], 30: [expanding_mean]},\n",
    "    {1: [rolling_mean_14], 30: [expanding_mean]},\n",
    "    {1: [rolling_mean_14]},\n",
    "    {},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate_models(train_df, test_df, models, target_transforms, lag_transforms_options, \n",
    "                          optimal_lags_list\n",
    ")\n",
    "print(\"Final Results:\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def save_results(results, filename=\"forecast_results.json\"):\n",
    "    serializable_results = {\n",
    "        str(k): v for k, v in results.items()\n",
    "    }  # Convert keys to strings for JSON compatibility\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(serializable_results, f, indent=4)\n",
    "    print(f\"Results saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(results, \"results_mlforecast.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results from JSON\n",
    "filename = \"results_mlforecast.json\"\n",
    "with open(filename, \"r\") as f:\n",
    "    results_json = json.load(f)\n",
    "\n",
    "# Convert results into a DataFrame\n",
    "data = []\n",
    "# for key, mape in results_json.items():\n",
    "for key, mape in results.items():\n",
    "    # print(eval(key))\n",
    "    # model, transforms, lags, lag_transforms = eval(key)\n",
    "    model, transforms, lags, lag_transforms = key\n",
    "    transforms = tuple(t.__class__.__name__ for t in transforms)\n",
    "    data.append([model, transforms, lags, lag_transforms, mape])\n",
    "\n",
    "df_results = pd.DataFrame(data, columns=[\"Model\", \"Transforms\", \"Lags\", \"Lag Transforms\", \"MAPE\"])\n",
    "\n",
    "# Top 10 Best Configurations\n",
    "df_top_configs = df_results.nsmallest(50, \"MAPE\")\n",
    "\n",
    "# Model Performance Summary\n",
    "df_model_performance = df_results.groupby(\"Model\")[\"MAPE\"].agg([\"mean\", \"min\", \"max\", \"count\"]).sort_values(\"mean\")\n",
    "\n",
    "# Transformation Effectiveness\n",
    "df_transform_performance = df_results.explode(\"Transforms\").groupby(\"Transforms\")[\"MAPE\"].mean().sort_values()\n",
    "\n",
    "# Lags Effectiveness\n",
    "df_lag_performance = df_results.explode(\"Lags\").groupby(\"Lags\")[\"MAPE\"].mean().sort_values()\n",
    "\n",
    "# Visualization: Model Performance\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.boxplot(data=df_results, x=\"Model\", y=\"MAPE\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Model Performance Distribution\")\n",
    "plt.show()\n",
    "\n",
    "# Visualization: Transformation Effectiveness\n",
    "plt.figure(figsize=(10, 5))\n",
    "df_transform_performance.plot(kind=\"bar\", title=\"Average MAPE by Transformation\")\n",
    "plt.ylabel(\"Average MAPE\")\n",
    "plt.show()\n",
    "\n",
    "# Visualization: Lag Effectiveness\n",
    "plt.figure(figsize=(10, 5))\n",
    "df_lag_performance.plot(kind=\"bar\", title=\"Average MAPE by Lags\")\n",
    "plt.ylabel(\"Average MAPE\")\n",
    "plt.show()\n",
    "\n",
    "# Display top configurations and summaries\n",
    "# import ace_tools as tools\n",
    "\"Top 10 Best Configurations\", df_top_configs\n",
    "\"Model Performance Summary\", df_model_performance\n",
    "\"Transformation Effectiveness\", df_transform_performance\n",
    "\"Lag Effectiveness\", df_lag_performance\n",
    "/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_frozenset(frozenset_str):\n",
    "    frozenset_str = str(frozenset_str)\n",
    "    # Extract key-value pairs using regex\n",
    "    matches = re.findall(r\"\\((\\d+), \\(CPUDispatcher\\(<function (\\w+) at .*?>\\)\\)\\)\", frozenset_str)\n",
    "    # Convert to dictionary format\n",
    "    cleaned_dict = {int(k): v for k, v in matches}\n",
    "    return frozenset_str\n",
    "    return cleaned_dict\n",
    "\n",
    "clean_frozenset(df_top_configs['Lag Transforms'][49])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def stringify_transform(transform):\n",
    "    return str(transform)\n",
    "\n",
    "def clean_lag_transforms(lag_transforms):\n",
    "    \"\"\" Converts lag transforms into a string representation \"\"\"\n",
    "    return str(lag_transforms)\n",
    "\n",
    "def save_results(results, filename=\"forecast_results.json\"):\n",
    "    serializable_results = {\n",
    "        str((model, stringify_transform(transforms), lags, clean_lag_transforms(lag_transforms))): mape\n",
    "        for (model, transforms, lags, lag_transforms), mape in results.items()\n",
    "    }\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(serializable_results, f, indent=4)\n",
    "    print(f\"Results saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(results, \"results_mlforecast_strfied.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load results\n",
    "with open(\"results_mlforecast_strfied.json\", \"r\") as f:\n",
    "    loaded_results = json.load(f)\n",
    "\n",
    "# Unpack results into structured format\n",
    "unpacked_results = []\n",
    "for key, mape_metr in loaded_results.items():\n",
    "    model, transforms, lags, lag_transforms = eval(key)  # Convert back to tuple safely\n",
    "    \n",
    "    unpacked_results.append([model, transforms, lags, lag_transforms, mape_metr])\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_results = pd.DataFrame(unpacked_results, columns=[\"Model\", \"Transforms\", \"Lags\", \"Lag Transforms\", \"MAPE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_transforms(transform_str):\n",
    "    # Extract class names using regex\n",
    "    cleaned = re.findall(r\"<mlforecast\\.target_transforms\\.(\\w+) object\", transform_str)\n",
    "    return \", \".join(cleaned) if cleaned else transform_str\n",
    "\n",
    "def clean_lag_transforms(transform_str):\n",
    "    # Extract class names using regex\n",
    "    pattern = r\"\\((\\d+), \\(CPUDispatcher\\(<function ([a-zA-Z0-9_]+)\"\n",
    "    cleaned = re.findall(pattern, transform_str)\n",
    "\n",
    "    return {int(k): v for k, v in cleaned}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results['Transforms'] = df_results['Transforms'].apply(clean_transforms)\n",
    "df_results['Lag Transforms'] = df_results['Lag Transforms'].apply(clean_lag_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "df = df_results.copy()\n",
    "lag_transforms_map = {\n",
    "    \"expanding_mean_rolling_14_rolling_30\": {1: 'expanding_mean', 7: 'expanding_mean', 30: 'rolling_mean_30'},\n",
    "    \"expanding_mean_rolling_14\": {1: 'expanding_mean', 7: 'rolling_mean_14', 30: 'expanding_mean'},\n",
    "    \"rolling_14_rolling_30_expanding\": {1: 'rolling_mean_14', 7: 'rolling_mean_30', 30: 'expanding_mean'},\n",
    "    \"rolling_14_expanding\": {1: 'rolling_mean_14', 30: 'expanding_mean'},\n",
    "    \"rolling_14_only\": {1: 'rolling_mean_14'},\n",
    "    \"no_transform\": {},\n",
    "}\n",
    "\n",
    "def map_lag_transforms(lag_transform_dict):\n",
    "    for name, transform in lag_transforms_map.items():\n",
    "        if lag_transform_dict == transform:\n",
    "            return name\n",
    "    return \"unknown\"\n",
    "\n",
    "df['Lag Transform Name'] = df['Lag Transforms'].apply(map_lag_transforms)\n",
    "optimal_lags_map = {\n",
    "    \"rndforest_set_10_year_1\": [1, 7, 15, 18, 173, 200, 335, 368, 369, 379],\n",
    "    \"rndforest_set_10_year_2\": [1, 6, 11, 15, 173, 200, 335, 378, 379, 384],\n",
    "    \"rndforest_set_10_200\": [1, 6, 10, 11, 15, 182, 183, 185, 187, 193],\n",
    "    \"rndforest_set_15_200\": [1, 4, 7, 10, 14, 22, 173, 179, 182, 183, 184, 185, 187, 188, 193],\n",
    "    \"rndforest_set_20\": [1, 5, 6, 7, 11, 21, 116, 173, 180, 184, 187, 188, 334, 335, 368, 369, 373, 378, 379, 384],\n",
    "    \"catboost_set_15_210\": [1, 2, 5, 6, 9, 11, 12, 14, 131, 144, 146, 151, 183, 196, 210],\n",
    "    \"catboost_set_15_year\": [1, 10, 11, 15, 16, 150, 172, 188, 198, 199, 222, 273, 336, 368, 384],\n",
    "}\n",
    "\n",
    "def map_lag_sets(lag_list):\n",
    "    for name, lags in optimal_lags_map.items():\n",
    "        if lag_list == tuple(lags):\n",
    "            return name\n",
    "    return \"unknown\"\n",
    "\n",
    "df['Lag_Set_Name'] = df['Lags'].apply(map_lag_sets)\n",
    "\n",
    "top_df = df[df[\"MAPE\"]<40].copy() # 1st analyss\n",
    "# top_df = df[(df[\"MAPE\"]<40) & (df['Model']=='XGBRegressor')].copy() # 2nd analyss\n",
    "# top_df = df[(df['Model']=='XGBRegressor') \n",
    "#             & \n",
    "#             (df[\"MAPE\"]<50)\n",
    "#             # (df['Lag Transform Name']=='expanding_mean_rolling_14')\n",
    "#             ].copy() # 3rd analyss\n",
    "\n",
    "top_models = top_df.groupby(\"Model\")[\"MAPE\"].mean().sort_values().reset_index()\n",
    "\n",
    "# Extracting top transforms by lowest average MAPE\n",
    "top_transforms = top_df.groupby(\"Transforms\")[\"MAPE\"].mean().sort_values().reset_index()\n",
    "\n",
    "# Extracting top lag transforms by lowest MAPE\n",
    "top_lag_transforms = top_df.groupby(\"Lag Transform Name\")[\"MAPE\"].mean().sort_values().reset_index()\n",
    "\n",
    "# Extracting best-performing lag sets\n",
    "top_lags = top_df.groupby(\"Lag_Set_Name\")[\"MAPE\"].mean().sort_values().reset_index()\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(12, 5))\n",
    "sns.barplot(x=top_models[\"Model\"], y=top_models[\"MAPE\"], palette=\"viridis\", hue=top_models[\"Model\"])\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Average MAPE per Model\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "sns.barplot(x=top_transforms[\"Transforms\"], y=top_transforms[\"MAPE\"], palette=\"coolwarm\", hue=top_transforms[\"Transforms\"])\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Average MAPE per Transform\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "sns.barplot(x=top_lag_transforms[\"Lag Transform Name\"], y=top_lag_transforms[\"MAPE\"], palette=\"Blues\", hue=top_lag_transforms[\"Lag Transform Name\"])\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Average MAPE per Lag Transform\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "sns.barplot(x=top_lags[\"Lag_Set_Name\"], y=top_lags[\"MAPE\"], palette=\"Blues\", hue=top_lags[\"Lag_Set_Name\"])\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"MAPE vs Number of Lags\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2nd run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_time_features(df):\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        df = df.set_index(pd.to_datetime(df[\"ds\"]))  # Convert to datetime and set index\n",
    "\n",
    "    df['day_of_week'] = df.index.dayofweek\n",
    "    df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "    df['month'] = df.index.month\n",
    "    df['season'] = ((df['month'] % 12 + 3) // 3)  # 1:Winter, 2:Spring, 3:Summer, 4:Fall\n",
    "    return df\n",
    "\n",
    "\n",
    "exp_train_df = add_time_features(train_df)\n",
    "exp_test_df = add_time_features(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_w_params(train_df, test_df, model, model_name, lags, target_transforms, date_features, lag_transforms):    \n",
    "    fcst = MLForecast(\n",
    "        models=model,\n",
    "        freq='D',\n",
    "        lags=lags,\n",
    "        target_transforms=list(target_transforms),\n",
    "        date_features=date_features,\n",
    "        num_threads=1,\n",
    "        lag_transforms=lag_transforms,\n",
    "    )\n",
    "    \n",
    "    # Fit the model\n",
    "    fcst.fit(train_df, time_col = \"ds\",)\n",
    "    \n",
    "    # Predict\n",
    "    predictions = fcst.predict(h=len(test_df['y']))\n",
    "    \n",
    "    # Store results\n",
    "    test_df['forecast'] = predictions[model_name]\n",
    "    error = mape(test_df['y'].values, test_df['forecast'].values)\n",
    "    \n",
    "    print(f\"{model_name} MAPE: {error:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_weekend(dates):\n",
    "    return dates.isin([5, 6]).astype(int)\n",
    "def get_season(dates):\n",
    "    return ((dates.month % 12 + 3) // 3)  # 1:Winter, 2:Spring, 3:Summer, 4:Fall\n",
    "\n",
    "evaluate_model_w_params(exp_train_df, exp_test_df, Lasso(), \"Lasso\", \n",
    "                        optimal_lags_map['rndforest_set_10_year_1'], \n",
    "                        [AutoDifferences(380)], \n",
    "                        # ['dayofweek', 'month', is_weekend, get_season], \n",
    "                        [], \n",
    "                        lag_transforms_options[3], \n",
    "                        static_features=['day_of_week', 'month', 'is_weekend', 'season'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model_w_params(train_df, test_df, XGBRegressor(), \"XGBRegressor\", \n",
    "                        optimal_lags_map['rndforest_set_20'], \n",
    "                        [AutoDifferences(380), LocalMinMaxScaler()], \n",
    "                        # ['dayofweek', 'month'], \n",
    "                        ['dayofweek', 'month', is_weekend, get_season], \n",
    "                        lag_transforms_options[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lasso run w/ smaller window and fewer lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_results(name):\n",
    "    with open(name, \"r\") as f:\n",
    "        loaded_results = json.load(f)\n",
    "\n",
    "    # Unpack results into structured format\n",
    "    unpacked_results = []\n",
    "    for key, mape_metr in loaded_results.items():\n",
    "        model, transforms, lags, lag_transforms = eval(key)  # Convert back to tuple safely\n",
    "        \n",
    "        unpacked_results.append([model, transforms, lags, lag_transforms, mape_metr])\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df_results = pd.DataFrame(unpacked_results, columns=[\"Model\", \"Transforms\", \"Lags\", \"Lag Transforms\", \"MAPE\"])\n",
    "\n",
    "    df_results['Transforms'] = df_results['Transforms'].apply(clean_transforms)\n",
    "    df_results['Lag Transforms'] = df_results['Lag Transforms'].apply(clean_lag_transforms)\n",
    "\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_lags = select_important_lags(train_df, 'y', max_lags=100, model=RandomForestRegressor(), num_of_lags=15)\n",
    "optimal_lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\"Lasso\": Lasso()}\n",
    "target_transforms = [AutoDifferences(380)]\n",
    "lag_transforms_options = [{1: [rolling_mean_14], 30: [expanding_mean]}]\n",
    "lags_lasso = [\n",
    "                              [1, 6, 7, 11, 191, 199, 335], \n",
    "                              [1, 4, 14, 23, 136, 165, 177, 187, 188, 198],\n",
    "                              [1, 3, 4, 5, 6, 7, 21, 23, 135, 136],\n",
    "                              [1, 4, 5, 6, 7, 13, 14, 23, 68, 129], \n",
    "                              [1, 3, 4, 5, 6, 7, 14, 16, 21, 23],\n",
    "                              [1, 3, 4, 5, 6, 7, 10, 11, 14, 15, 20, 21, 23, 25, 41],\n",
    "                              select_important_lags(train_df, 'y', max_lags=40, model=RandomForestRegressor(), num_of_lags=10),\n",
    "                              select_important_lags(train_df, 'y', max_lags=40, model=RandomForestRegressor(), num_of_lags=15),\n",
    "                              select_important_lags(train_df, 'y', max_lags=40, model=RandomForestRegressor(), num_of_lags=20),\n",
    "                              select_important_lags(train_df, 'y', max_lags=40, model=RandomForestRegressor(), num_of_lags=25),\n",
    "                              select_important_lags(train_df, 'y', max_lags=40, model=RandomForestRegressor(), num_of_lags=30),\n",
    "                              select_important_lags(train_df, 'y', max_lags=40, model=RandomForestRegressor(), num_of_lags=35),\n",
    "                              select_important_lags(train_df, 'y', max_lags=40, model=RandomForestRegressor(), num_of_lags=40),\n",
    "                              select_important_lags(train_df, 'y', max_lags=70, model=RandomForestRegressor(), num_of_lags=10),\n",
    "                              select_important_lags(train_df, 'y', max_lags=70, model=RandomForestRegressor(), num_of_lags=15),\n",
    "                              select_important_lags(train_df, 'y', max_lags=70, model=RandomForestRegressor(), num_of_lags=20),\n",
    "                            #   select_important_lags(train_df, 'y', max_lags=100, model=RandomForestRegressor(), num_of_lags=20),\n",
    "                            #   select_important_lags(train_df, 'y', max_lags=100, model=RandomForestRegressor(), num_of_lags=25),\n",
    "                              select_important_lags(train_df, 'y', max_lags=100, model=RandomForestRegressor(), num_of_lags=30),\n",
    "                              select_important_lags(train_df, 'y', max_lags=100, model=RandomForestRegressor(), num_of_lags=40),\n",
    "                              select_important_lags(train_df, 'y', max_lags=100, model=RandomForestRegressor(), num_of_lags=50),\n",
    "                              select_important_lags(train_df, 'y', max_lags=100, model=RandomForestRegressor(), num_of_lags=60),\n",
    "                              select_important_lags(train_df, 'y', max_lags=100, model=RandomForestRegressor(), num_of_lags=70),\n",
    "                              select_important_lags(train_df, 'y', max_lags=150, model=RandomForestRegressor(), num_of_lags=10),\n",
    "                              select_important_lags(train_df, 'y', max_lags=150, model=RandomForestRegressor(), num_of_lags=20),\n",
    "                              select_important_lags(train_df, 'y', max_lags=150, model=RandomForestRegressor(), num_of_lags=30),\n",
    "                              select_important_lags(train_df, 'y', max_lags=150, model=RandomForestRegressor(), num_of_lags=40),\n",
    "                              select_important_lags(train_df, 'y', max_lags=150, model=RandomForestRegressor(), num_of_lags=50),\n",
    "                              select_important_lags(train_df, 'y', max_lags=150, model=RandomForestRegressor(), num_of_lags=60),\n",
    "                              select_important_lags(train_df, 'y', max_lags=150, model=RandomForestRegressor(), num_of_lags=70),\n",
    "                              select_important_lags(train_df, 'y', max_lags=200, model=RandomForestRegressor(), num_of_lags=20),\n",
    "                              select_important_lags(train_df, 'y', max_lags=200, model=RandomForestRegressor(), num_of_lags=30),\n",
    "                              select_important_lags(train_df, 'y', max_lags=200, model=RandomForestRegressor(), num_of_lags=40),\n",
    "                              select_important_lags(train_df, 'y', max_lags=250, model=RandomForestRegressor(), num_of_lags=30),\n",
    "                              select_important_lags(train_df, 'y', max_lags=250, model=RandomForestRegressor(), num_of_lags=90),\n",
    "                              select_important_lags(train_df, 'y', max_lags=250, model=RandomForestRegressor(), num_of_lags=60),\n",
    "                          ]\n",
    "\n",
    "results = evaluate_models(train_df, test_df, models, target_transforms, lag_transforms_options, \n",
    "                          optimal_lags_list = lags_lasso\n",
    ")\n",
    "save_results(results, \"results_mlforecast_lasso.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_lasso = load_results(\"results_mlforecast_lasso.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAG_TRANSFORM_MAPS = {\n",
    "    \"expanding_mean_rolling_14_rolling_30\": {1: 'expanding_mean', 7: 'expanding_mean', 30: 'rolling_mean_30'},\n",
    "    \"expanding_mean_rolling_14\": {1: 'expanding_mean', 7: 'rolling_mean_14', 30: 'expanding_mean'},\n",
    "    \"rolling_14_rolling_30_expanding\": {1: 'rolling_mean_14', 7: 'rolling_mean_30', 30: 'expanding_mean'},\n",
    "    \"rolling_14_expanding\": {1: 'rolling_mean_14', 30: 'expanding_mean'},\n",
    "    \"rolling_14_only\": {1: 'rolling_mean_14'},\n",
    "    \"no_transform\": {},\n",
    "}\n",
    "\n",
    "def map_lag_transforms(lag_transform_dict, lag_transforms_map):\n",
    "    for name, transform in lag_transforms_map.items():\n",
    "        if lag_transform_dict == transform:\n",
    "            return name\n",
    "    return \"unknown\"\n",
    "\n",
    "def map_lag_sets(lag_list, optimal_lags_map):\n",
    "    for name, lags in optimal_lags_map.items():\n",
    "        if lag_list == tuple(lags):\n",
    "            return name\n",
    "    return \"unknown\"\n",
    "\n",
    "def show_results(top_df):\n",
    "    top_models = top_df.groupby(\"Model\")[\"MAPE\"].mean().sort_values().reset_index()\n",
    "\n",
    "    # Extracting top transforms by lowest average MAPE\n",
    "    top_transforms = top_df.groupby(\"Transforms\")[\"MAPE\"].mean().sort_values().reset_index()\n",
    "\n",
    "    # Extracting top lag transforms by lowest MAPE\n",
    "    top_lag_transforms = top_df.groupby(\"Lag Transform Name\")[\"MAPE\"].mean().sort_values().reset_index()\n",
    "\n",
    "    # Extracting best-performing lag sets\n",
    "    top_lags = top_df.groupby(\"Lag_Set_Name\")[\"MAPE\"].mean().sort_values().reset_index()\n",
    "\n",
    "    # Visualization\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    sns.barplot(x=top_models[\"Model\"], y=top_models[\"MAPE\"], palette=\"viridis\", hue=top_models[\"Model\"])\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title(\"Average MAPE per Model\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    sns.barplot(x=top_transforms[\"Transforms\"], y=top_transforms[\"MAPE\"], palette=\"coolwarm\", hue=top_transforms[\"Transforms\"])\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(\"Average MAPE per Transform\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    sns.barplot(x=top_lag_transforms[\"Lag Transform Name\"], y=top_lag_transforms[\"MAPE\"], palette=\"Blues\", hue=top_lag_transforms[\"Lag Transform Name\"])\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(\"Average MAPE per Lag Transform\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    sns.barplot(x=top_lags[\"Lag_Set_Name\"], y=top_lags[\"MAPE\"], palette=\"Blues\", hue=top_lags[\"Lag_Set_Name\"])\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(\"MAPE vs Number of Lags\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_lags_names = ['wndw_400_len_7', 'wndw_200_len_10','wndw_150_len_10_1','wndw_150_len_10', 'wndw_100_len_10',\n",
    "                      'wndw_100_len_15','wndw_40_len_10)','wndw_40_len_15)','wndw_40_len_20)', 'wndw_40_len_25)',\n",
    "                    'wndw_40_len_30)', 'wndw_40_len_35)', 'wndw_40_len_40)', 'wndw_70_len_10)', 'wndw_70_len_15)',\n",
    "                    'wndw_70_len_20)','wndw_100_len_30)','wndw_100_len_40)','wndw_100_len_50)','wndw_100_len_60)','wndw_100_len_70)',\n",
    "                    'wndw_150_len_10)', 'wndw_150_len_20)', 'wndw_150_len_30)', 'wndw_150_len_40)', 'wndw_150_len_50)',\n",
    "                    'wndw_150_len_60)', 'wndw_150_len_70)', 'wndw_200_len_20)', 'wndw_200_len_30)', 'wndw_200_len_40)',\n",
    "                    'wndw_250_len_30)', 'wndw_250_len_90)', 'wndw_250_len_60)',\n",
    "]\n",
    "optimal_lags_map = dict(zip(optimal_lags_names, df_results_lasso['Lags'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_results_lasso.copy()\n",
    "\n",
    "df['Lag Transform Name'] = df['Lag Transforms'].apply(lambda x: map_lag_transforms(x, LAG_TRANSFORM_MAPS))\n",
    "\n",
    "\n",
    "df['Lag_Set_Name'] = df['Lags'].apply(lambda x: map_lag_sets(x, optimal_lags_map))\n",
    "\n",
    "top_df = df[df[\"MAPE\"]<40].copy() # 1st analyss\n",
    "# top_df = df[(df[\"MAPE\"]<40) & (df['Model']=='XGBRegressor')].copy() # 2nd analyss\n",
    "# top_df = df[(df['Model']=='XGBRegressor') \n",
    "#             & \n",
    "#             (df[\"MAPE\"]<50)\n",
    "#             # (df['Lag Transform Name']=='expanding_mean_rolling_14')\n",
    "#             ].copy() # 3rd analyss\n",
    "\n",
    "show_results(top_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xgboost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_important_lags_extended(train_df, target_col, max_lags, model=RandomForestRegressor(n_estimators=50), num_of_lags_list=[5, 10, 15]):\n",
    "    \"\"\" Selects the most important lags based on feature importance analysis for multiple numbers of lags.\"\"\"\n",
    "    lagged_features = pd.concat([\n",
    "        train_df[target_col].shift(lag).rename(f'lag_{lag}') for lag in range(1, max_lags + 1)\n",
    "    ], axis=1)\n",
    "    \n",
    "    lagged_features.dropna(inplace=True)\n",
    "    y = train_df[target_col][max_lags:]\n",
    "    model.fit(lagged_features, y)\n",
    "    feature_importances = model.feature_importances_\n",
    "    \n",
    "    important_lags_lists = []\n",
    "    for num_of_lags in num_of_lags_list:\n",
    "        important_lags = [i + 1 for i in np.argsort(feature_importances)[-num_of_lags:]]  # Select top lags\n",
    "        important_lags_lists.append([int(x) for x in sorted(important_lags)])\n",
    "    \n",
    "    return important_lags_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_forest_lags_list_100 = select_important_lags_extended(train_df, 'y', 100, num_of_lags_list=[33, 66, 100])\n",
    "rnd_forest_lags_list_200 = select_important_lags_extended(train_df, 'y', 200, num_of_lags_list=[33, 66, 100, 150])\n",
    "rnd_forest_lags_list_300 = select_important_lags_extended(train_df, 'y', 300, num_of_lags_list=[50, 100, 150, 200, 250])\n",
    "rnd_forest_lags_list_400 = select_important_lags_extended(train_df, 'y', 400, num_of_lags_list=[50, 100, 150, 200, 250, 300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_forest_lags_list = rnd_forest_lags_list_100 + rnd_forest_lags_list_200 + rnd_forest_lags_list_300 + rnd_forest_lags_list_400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = {\"XGBRegressor\": XGBRegressor()}\n",
    "# target_transforms = [AutoDifferences(380), LocalMinMaxScaler()]\n",
    "# lag_transforms_options = [{1: [rolling_mean_14], 7: [rolling_mean_30], 30: [expanding_mean]}]\n",
    "\n",
    "# results = evaluate_models(train_df, test_df, models, target_transforms, lag_transforms_options, \n",
    "#                           optimal_lags_list = rnd_forest_lags_list\n",
    "# )\n",
    "# save_results(results, \"results_mlforecast_xgboost.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "фыва"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "думаю, нет смысла хгб тестить дальше. с разными наборами признаков он не стал сильно лучше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_xgboost = load_results(\"results_mlforecast_xgboost.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### more linear models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_lags_list = [\n",
    "    [1, 7, 15, 18, 173, 200, 335, 368, 369, 379],\n",
    "    [1, 6, 11, 15, 173, 200, 335, 378, 379, 384],\n",
    "    [1, 6, 10, 11, 15, 182, 183, 185, 187, 193],\n",
    "    [1, 4, 7, 10, 14, 22, 173, 179, 182, 183, 184, 185, 187, 188, 193],\n",
    "    [1, 5, 6, 7, 11, 21, 116, 173, 180, 184, 187, 188, 334, 335, 368, 369, 373, 378, 379, 384],\n",
    "    \n",
    "    [1, 2, 5, 6, 9, 11, 12, 14, 131, 144, 146, 151, 183, 196, 210],\n",
    "    [1, 10, 11, 15, 16, 150, 172, 188, 198, 199, 222, 273, 336, 368, 384],\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso, LinearRegression, ElasticNet, BayesianRidge, HuberRegressor, SGDRegressor\n",
    "from sklearn.svm import SVR \n",
    "\n",
    "models = {\n",
    "    \"LinearRegression\": LinearRegression(),\n",
    "    \"ElasticNet\": ElasticNet(),\n",
    "    \"BayesianRidge\": BayesianRidge(),\n",
    "    \"HuberRegressor\": HuberRegressor(),\n",
    "    \"SGDRegressor\": SGDRegressor(),\n",
    "    # \"SVR\": SVR(\"poly\"),\n",
    "    \"SVR\": SVR(),\n",
    "    # \"SVR\": SVR(\"sigmoid\"),\n",
    "}\n",
    "\n",
    "target_transforms = [\n",
    "    AutoDifferences(380), \n",
    "    AutoSeasonalDifferences(season_length=365, max_diffs=380), \n",
    "    AutoSeasonalityAndDifferences(max_season_length=380, max_diffs=380),\n",
    "    LocalStandardScaler(), \n",
    "    LocalMinMaxScaler(), \n",
    "    LocalBoxCox()\n",
    "]\n",
    "\n",
    "@njit\n",
    "def rolling_mean_14(x):\n",
    "    return rolling_mean(x, window_size=14)\n",
    "@njit\n",
    "def rolling_mean_30(x):\n",
    "    return rolling_mean(x, window_size=30)\n",
    "\n",
    "lag_transforms_options = [\n",
    "    # {1: [expanding_mean], 7: [expanding_mean, rolling_mean_14], 30: [rolling_mean_30]},\n",
    "    {1: [expanding_mean], 7: [rolling_mean_14], 30: [expanding_mean]},\n",
    "    {1: [rolling_mean_14], 7: [rolling_mean_30], 30: [expanding_mean]},\n",
    "    {1: [rolling_mean_14], 30: [expanding_mean]},\n",
    "    {1: [rolling_mean_14]},\n",
    "    {},\n",
    "]\n",
    "\n",
    "results = evaluate_models(train_df, test_df, models, target_transforms, lag_transforms_options, \n",
    "                          optimal_lags_list\n",
    ")\n",
    "df_results_simple = process_results(results, \"results_mlforecast_simple.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_simple = load_results(\"results_mlforecast_simple.json\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_lags_map = {\n",
    "    \"rndforest_set_10_year_1\": [1, 7, 15, 18, 173, 200, 335, 368, 369, 379],\n",
    "    \"rndforest_set_10_year_2\": [1, 6, 11, 15, 173, 200, 335, 378, 379, 384],\n",
    "    \"rndforest_set_10_200\": [1, 6, 10, 11, 15, 182, 183, 185, 187, 193],\n",
    "    \"rndforest_set_15_200\": [1, 4, 7, 10, 14, 22, 173, 179, 182, 183, 184, 185, 187, 188, 193],\n",
    "    \"rndforest_set_20\": [1, 5, 6, 7, 11, 21, 116, 173, 180, 184, 187, 188, 334, 335, 368, 369, 373, 378, 379, 384],\n",
    "    \"catboost_set_15_210\": [1, 2, 5, 6, 9, 11, 12, 14, 131, 144, 146, 151, 183, 196, 210],\n",
    "    \"catboost_set_15_year\": [1, 10, 11, 15, 16, 150, 172, 188, 198, 199, 222, 273, 336, 368, 384],\n",
    "}\n",
    "\n",
    "df = df_results_simple.copy()\n",
    "\n",
    "df['Lag Transform Name'] = df['Lag Transforms'].apply(lambda x: map_lag_transforms(x, LAG_TRANSFORM_MAPS))\n",
    "\n",
    "\n",
    "df['Lag_Set_Name'] = df['Lags'].apply(lambda x: map_lag_sets(x, optimal_lags_map))\n",
    "\n",
    "top_df = df[df[\"MAPE\"]<40].copy() # 1st analyss\n",
    "top_df = df[(df[\"MAPE\"]<40) & (df['Model']=='SGDRegressor')].copy() # 2nd analyss\n",
    "# top_df = df[(df['Model']=='XGBRegressor') \n",
    "#             & \n",
    "#             (df[\"MAPE\"]<50)\n",
    "#             # (df['Lag Transform Name']=='expanding_mean_rolling_14')\n",
    "#             ].copy() # 3rd analyss\n",
    "\n",
    "show_results(top_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
