{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import njit\n",
    "from window_ops.expanding import expanding_mean\n",
    "from window_ops.rolling import rolling_mean\n",
    "\n",
    "@njit\n",
    "def rolling_mean_14(x):\n",
    "    return rolling_mean(x, window_size=14)\n",
    "@njit\n",
    "def rolling_mean_30(x):\n",
    "    return rolling_mean(x, window_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_df_to_mlforecast(df, date_col, target_col, unique_id='mean'):\n",
    "    df_ = df.rename({\n",
    "        date_col: \"ds\",\n",
    "        # target_col: 'y',\n",
    "    }, axis=1)\n",
    "\n",
    "    df_['ds'] = pd.to_datetime(df_['ds'])\n",
    "\n",
    "    df_['y'] = df_[target_col].copy()\n",
    "    # df_.drop(columns=target_col)\n",
    "\n",
    "    df_['unique_id'] = unique_id\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_sensors_df = pd.read_csv(\"../data/selected_sensors2_cleaned.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios_sensors = {\n",
    "    # 0: 1, 4372603\n",
    "    # \"0_12M_train_7M_test\": {\"train_start\": \"2017-03-25\", \"train_end\": \"2018-03-25\", \"test_start\": \"2018-03-26\", \"test_end\": \"2018-10-10\"},\n",
    "    '2': {\n",
    "        \"24M_train\":  {\"train_start\": \"2017-04-01\", \"train_end\": \"2019-04-01\"},\n",
    "        \"22M_train\":  {\"train_start\": \"2017-04-01\", \"train_end\": \"2019-02-01\"},\n",
    "        \"20M_train\":  {\"train_start\": \"2017-04-01\", \"train_end\": \"2018-12-01\"},\n",
    "        \"18M_train\":  {\"train_start\": \"2017-04-01\", \"train_end\": \"2018-10-01\"},\n",
    "        \"12M_train\":  {\"train_start\": \"2018-06-01\", \"train_end\": \"2019-06-01\"},\n",
    "        # \"8M_train\":   {\"train_start\": \"2017-04-01\", \"train_end\": \"2017-10-25\"},\n",
    "        },\n",
    "}\n",
    "scenarios_sensors['5'] = scenarios_sensors['2'].copy()\n",
    "# scenarios_sensors['6'] = scenarios_sensors['2'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-01 17:06:28,885\tINFO util.py:154 -- Outdated packages:\n",
      "  ipywidgets==7.8.1 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2025-04-01 17:06:29,147\tINFO util.py:154 -- Outdated packages:\n",
      "  ipywidgets==7.8.1 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from MLForecastPipeline import *\n",
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df, scenario, date_col=\"ds\"):\n",
    "    \"\"\"Extracts train and test data based on train end date.\"\"\"\n",
    "    train_data = df[df[date_col] <= scenario['train_end']]\n",
    "    test_start = pd.to_datetime(scenario['train_end']) + pd.Timedelta(days=1)\n",
    "    test_data = df[df[date_col] >= test_start]\n",
    "    return train_data, test_data\n",
    "\n",
    "models = {\n",
    "    \"SGD_Ridge\": SGDRegressor( penalty='l2', alpha=1, random_state=42 ),\n",
    "    \"SGDRegressor\": SGDRegressor(random_state=42),\n",
    "    \"SGD_ElasticNet\": SGDRegressor( penalty='elasticnet', l1_ratio=0.5, alpha=0.001, random_state=42 ),\n",
    "}\n",
    "\n",
    "# Define lag transformations\n",
    "from mlforecast.lag_transforms import *\n",
    "lag_transforms_options = [\n",
    "    {},\n",
    "    {7: [RollingMean(window_size=7)], 30: [RollingMean(window_size=30)], 60: [RollingMean(window_size=60)], },\n",
    "    {7: [RollingMean(7), RollingStd(7)], 30: [RollingMean(30)], 60: [ExpandingMean()], 14: [ExponentiallyWeightedMean(alpha=0.3)],},\n",
    "    {7: [RollingMean(7), RollingStd(7), ExpandingStd()], 14: [RollingMean(14), ExpandingStd(), ExponentiallyWeightedMean(alpha=0.3)], 30: [RollingMean(30)], 60: [ExpandingMean()],},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import time\n",
    "\n",
    "def process_scenario(sensor_name, scenario_name, scenario, selected_sensors_df, models, lag_transforms_options, ratios=[0.33, 0.66, 1]):\n",
    "    \"\"\" Process each scenario independently and save results. \"\"\"\n",
    "    print(f'{sensor_name}_{scenario_name}')\n",
    "    formatted_df = format_df_to_mlforecast(selected_sensors_df[['full_date', sensor_name]], 'full_date', sensor_name, unique_id=sensor_name)\n",
    "    formatted_df = formatted_df[['ds', 'y', 'unique_id']]\n",
    "    \n",
    "    train_df, test_df = split_data(formatted_df, scenario)\n",
    "\n",
    "    train_df['y'] = train_df['y'].rolling(window=7, min_periods=1).mean()\n",
    "    test_df['y'] = test_df['y'].rolling(window=7, min_periods=1).mean()\n",
    "\n",
    "    optimal_lags_list = get_optimal_lags(train_df, 'y', ratios=ratios)\n",
    "    target_transforms = get_dynamic_transforms(train_df)\n",
    "\n",
    "    results = evaluate_models(train_df, test_df, models, target_transforms, lag_transforms_options, optimal_lags_list, test_by_months=True)\n",
    "\n",
    "    save_results(results, f\"results/run_14/{sensor_name}_{scenario_name}.csv\")\n",
    "\n",
    "    return results\n",
    "\n",
    "def run_all_scenarios_parallel(scenarios_sensors, selected_sensors_df, models, lag_transforms_options, ratios=[0.33, 0.66, 1]):\n",
    "    results = Parallel(n_jobs=14, verbose=30)( \n",
    "        delayed(process_scenario)(sensor_name, scenario_name, scenario, selected_sensors_df, models, lag_transforms_options, ratios=ratios)\n",
    "        for sensor_name, scenarios in scenarios_sensors.items()\n",
    "        for scenario_name, scenario in scenarios.items()\n",
    "    )\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=14)]: Using backend LokyBackend with 14 concurrent workers.\n",
      "[Parallel(n_jobs=14)]: Done   1 tasks      | elapsed: 69.5min\n",
      "[Parallel(n_jobs=14)]: Done   2 out of  10 | elapsed: 71.0min remaining: 284.1min\n",
      "[Parallel(n_jobs=14)]: Done   3 out of  10 | elapsed: 71.7min remaining: 167.2min\n",
      "[Parallel(n_jobs=14)]: Done   4 out of  10 | elapsed: 90.5min remaining: 135.8min\n",
      "[Parallel(n_jobs=14)]: Done   5 out of  10 | elapsed: 94.9min remaining: 94.9min\n",
      "[Parallel(n_jobs=14)]: Done   6 out of  10 | elapsed: 95.2min remaining: 63.5min\n",
      "[Parallel(n_jobs=14)]: Done   7 out of  10 | elapsed: 99.6min remaining: 42.7min\n",
      "[Parallel(n_jobs=14)]: Done   8 out of  10 | elapsed: 100.0min remaining: 25.0min\n",
      "[Parallel(n_jobs=14)]: Done  10 out of  10 | elapsed: 104.1min finished\n"
     ]
    }
   ],
   "source": [
    "results = run_all_scenarios_parallel(scenarios_sensors, selected_sensors_df, models, lag_transforms_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "cpu_count = multiprocessing.cpu_count()\n",
    "cpu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through scenarios and evaluate models\n",
    "results = []\n",
    "\n",
    "for sensor_name, scenarios in scenarios_sensors.items():\n",
    "    formatted_df = format_df_to_mlforecast(selected_sensors_df[['full_date', sensor_name]], 'full_date', sensor_name, unique_id=sensor_name)\n",
    "    formatted_df = formatted_df[['ds', 'y', 'unique_id']]\n",
    "\n",
    "    for scenario_name, scenario in scenarios.items():\n",
    "\n",
    "        train_df, test_df = split_data(formatted_df, scenario)\n",
    "\n",
    "        train_df['y'] = train_df['y'].rolling(window=7, min_periods=1).mean()\n",
    "        test_df['y'] = test_df['y'].rolling(window=7, min_periods=1).mean()\n",
    "\n",
    "        optimal_lags_list = get_optimal_lags(train_df, 'y', \n",
    "                                            ratios=[1]\n",
    "                                            # ratios=[0.33, 0.66, 1]\n",
    "                                            #  ratios=[0.25, 0.5, 0.75, 1]\n",
    "        )\n",
    "        target_transforms = get_dynamic_transforms(train_df)\n",
    "        results = evaluate_models(train_df, test_df, models, target_transforms, lag_transforms_options, optimal_lags_list, test_by_months=True)\n",
    "\n",
    "        save_results(results, f\"results/run_14/{sensor_name}_{scenario_name}.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
