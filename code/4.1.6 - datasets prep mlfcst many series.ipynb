{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import njit\n",
    "from window_ops.expanding import expanding_mean\n",
    "from window_ops.rolling import rolling_mean\n",
    "\n",
    "@njit\n",
    "def rolling_mean_14(x):\n",
    "    return rolling_mean(x, window_size=14)\n",
    "@njit\n",
    "def rolling_mean_30(x):\n",
    "    return rolling_mean(x, window_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_df_to_mlforecast(df, date_col, target_col, unique_id='mean'):\n",
    "    df_ = df.rename({\n",
    "        date_col: \"ds\",\n",
    "        # target_col: 'y',\n",
    "    }, axis=1)\n",
    "\n",
    "    df_['ds'] = pd.to_datetime(df_['ds'])\n",
    "\n",
    "    df_['y'] = df_[target_col].copy()\n",
    "    # df_.drop(columns=target_col)\n",
    "\n",
    "    df_['unique_id'] = unique_id\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_sensors_df = pd.read_csv(\"../data/selected_sensors2_cleaned.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios_sensors = {\n",
    "    # 0: 1, 4372603\n",
    "    # \"0_12M_train_7M_test\": {\"train_start\": \"2017-03-25\", \"train_end\": \"2018-03-25\", \"test_start\": \"2018-03-26\", \"test_end\": \"2018-10-10\"},\n",
    "    '2': {\n",
    "        \"26M_train\":  {\"train_start\": \"2017-04-01\", \"train_end\": \"2019-06-01\"},\n",
    "        \"24M_train\":  {\"train_start\": \"2017-04-01\", \"train_end\": \"2019-04-01\"},\n",
    "        \"22M_train\":  {\"train_start\": \"2017-04-01\", \"train_end\": \"2019-02-01\"},\n",
    "        \"20M_train\":  {\"train_start\": \"2017-04-01\", \"train_end\": \"2018-12-01\"},\n",
    "        \"18M_train\":  {\"train_start\": \"2017-04-01\", \"train_end\": \"2018-10-01\"},\n",
    "        \"12M_train\":  {\"train_start\": \"2017-04-01\", \"train_end\": \"2018-04-01\"},\n",
    "        \"10M_train\":  {\"train_start\": \"2017-04-01\", \"train_end\": \"2018-01-25\"},\n",
    "        \"8M_train\":   {\"train_start\": \"2017-04-01\", \"train_end\": \"2017-10-25\"},\n",
    "        \n",
    "        # Non-Heating Periods\n",
    "        \"NH_3M_train\":  {\"train_start\": \"2017-04-15\", \"train_end\": \"2017-07-15\"},\n",
    "        \"NH_4M_train\":  {\"train_start\": \"2017-04-15\", \"train_end\": \"2017-08-15\"},\n",
    "        \"NH_2M_train\":  {\"train_start\": \"2017-04-15\", \"train_end\": \"2017-06-15\"},\n",
    "        \"NH_1M_train\":  {\"train_start\": \"2017-04-15\", \"train_end\": \"2017-05-15\"},\n",
    "        \"NH_15D_train\": {\"train_start\": \"2017-04-15\", \"train_end\": \"2017-04-30\"},\n",
    "        \"NH_feb_2M_train\": {\"train_start\": \"2017-02-15\", \"train_end\": \"2017-04-15\"},\n",
    "        \"NH_feb_1M_train\": {\"train_start\": \"2017-02-15\", \"train_end\": \"2017-04-15\"},\n",
    "        \"NH_mar_2M_train\": {\"train_start\": \"2017-03-15\", \"train_end\": \"2017-05-15\"},\n",
    "        \"NH_mar_1M_train\": {\"train_start\": \"2017-03-15\", \"train_end\": \"2017-04-15\"},\n",
    "\n",
    "        # Heating Periods\n",
    "        \"H_5M_train\":     {\"train_start\": \"2017-06-01\", \"train_end\": \"2017-11-01\"},\n",
    "        \"H_3M_jul_train\": {\"train_start\": \"2017-07-01\", \"train_end\": \"2017-10-10\"},\n",
    "        \"H_3M_sep_train\": {\"train_start\": \"2017-09-01\", \"train_end\": \"2017-12-10\"},\n",
    "        \"H_3M_nov_train\": {\"train_start\": \"2017-11-01\", \"train_end\": \"2018-02-10\"},\n",
    "        },\n",
    "}\n",
    "scenarios_sensors['5'] = scenarios_sensors['2'].copy()\n",
    "scenarios_sensors['6'] = scenarios_sensors['2'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios = {\n",
    "        \"26M_train\":  {\"train_start\": \"2017-04-01\", \"train_end\": \"2019-06-01\"},\n",
    "        \"24M_train\":  {\"train_start\": \"2017-04-01\", \"train_end\": \"2019-04-01\"},\n",
    "        \"22M_train\":  {\"train_start\": \"2017-04-01\", \"train_end\": \"2019-02-01\"},\n",
    "        \"20M_train\":  {\"train_start\": \"2017-04-01\", \"train_end\": \"2018-12-01\"},\n",
    "        \"18M_train\":  {\"train_start\": \"2017-04-01\", \"train_end\": \"2018-10-01\"},\n",
    "        \"12M_train\":  {\"train_start\": \"2017-04-01\", \"train_end\": \"2018-04-01\"},\n",
    "        \"10M_train\":  {\"train_start\": \"2017-04-01\", \"train_end\": \"2018-01-25\"},\n",
    "        \"8M_train\":   {\"train_start\": \"2017-04-01\", \"train_end\": \"2017-10-25\"},\n",
    "        \n",
    "        # Non-Heating Periods\n",
    "        \"NH_3M_train\":  {\"train_start\": \"2017-04-15\", \"train_end\": \"2017-07-15\"},\n",
    "        \"NH_4M_train\":  {\"train_start\": \"2017-04-15\", \"train_end\": \"2017-08-15\"},\n",
    "        \"NH_2M_train\":  {\"train_start\": \"2017-04-15\", \"train_end\": \"2017-06-15\"},\n",
    "        \"NH_1M_train\":  {\"train_start\": \"2017-04-15\", \"train_end\": \"2017-05-15\"},\n",
    "        \"NH_15D_train\": {\"train_start\": \"2017-04-15\", \"train_end\": \"2017-04-30\"},\n",
    "        \"NH_feb_2M_train\": {\"train_start\": \"2017-02-15\", \"train_end\": \"2017-04-15\"},\n",
    "        \"NH_feb_1M_train\": {\"train_start\": \"2017-02-15\", \"train_end\": \"2017-04-15\"},\n",
    "        \"NH_mar_2M_train\": {\"train_start\": \"2017-03-15\", \"train_end\": \"2017-05-15\"},\n",
    "        \"NH_mar_1M_train\": {\"train_start\": \"2017-03-15\", \"train_end\": \"2017-04-15\"},\n",
    "\n",
    "        # Heating Periods\n",
    "        \"H_5M_train\":     {\"train_start\": \"2017-06-01\", \"train_end\": \"2017-11-01\"},\n",
    "        \"H_3M_jul_train\": {\"train_start\": \"2017-07-01\", \"train_end\": \"2017-10-10\"},\n",
    "        \"H_3M_sep_train\": {\"train_start\": \"2017-09-01\", \"train_end\": \"2017-12-10\"},\n",
    "        \"H_3M_nov_train\": {\"train_start\": \"2017-11-01\", \"train_end\": \"2018-02-10\"},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MLForecastPipeline import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df, scenario, date_col=\"ds\"):\n",
    "    \"\"\"Extracts train and test data based on train end date.\"\"\"\n",
    "    train_data = df[df[date_col] <= scenario['train_end']]\n",
    "    test_start = pd.to_datetime(scenario['train_end']) + pd.Timedelta(days=1)\n",
    "    test_data = df[df[date_col] >= test_start]\n",
    "    return train_data, test_data\n",
    "\n",
    "models = {\n",
    "    \"XGBRegressor\": XGBRegressor(),\n",
    "    \"SGDRegressor\": SGDRegressor(random_state=42),\n",
    "    # \"SGDRegressor_1\": SGDRegressor(random_state=1),\n",
    "    \"Ridge\": Ridge(),\n",
    "    \"Lasso\": Lasso()\n",
    "}\n",
    "\n",
    "# Define lag transformations\n",
    "\n",
    "lag_transforms_options = [\n",
    "    {1: [expanding_mean], 7: [rolling_mean_14], 30: [expanding_mean]},\n",
    "    {1: [rolling_mean_14], 7: [rolling_mean_30], 30: [expanding_mean]},\n",
    "    # {1: [rolling_mean_14], 30: [expanding_mean]},\n",
    "    # {1: [rolling_mean_14]},\n",
    "    # {},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm.notebook import trange, tqdm\n",
    "# from time import sleep\n",
    "\n",
    "# for i in trange(3, desc='1st loop'):\n",
    "#     for j in tqdm(range(100), desc='2nd loop'):\n",
    "#         sleep(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping to MLForecast format\n",
    "def format_multi_df_to_mlforecast(df):\n",
    "    df_melted = df.melt(id_vars=['full_date'], var_name='unique_id', value_name='y')\n",
    "    return df_melted.rename(columns={'full_date': 'ds'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_df = format_multi_df_to_mlforecast(selected_sensors_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import time\n",
    "\n",
    "def process_scenario(scenario_name, scenario, selected_sensors_df, models, lag_transforms_options, ratios=[0.33, 0.66, 1], sensor_names=['2','5','6']):\n",
    "    \"\"\" Process each scenario independently and save results. \"\"\"\n",
    "    print(f'{scenario_name}')\n",
    "    formatted_df = format_multi_df_to_mlforecast(selected_sensors_df)\n",
    "    formatted_df = formatted_df[['ds', 'y', 'unique_id']]\n",
    "    \n",
    "    train_df, test_df = split_data(formatted_df, scenario)\n",
    "    for sensor_name in sensor_names:\n",
    "        sensor_train_df = train_df[train_df['unique_id']==sensor_name]\n",
    "        optimal_lags_list = get_optimal_lags(sensor_train_df, 'y', ratios=ratios)\n",
    "        target_transforms = get_dynamic_transforms(sensor_train_df)\n",
    "\n",
    "        results = evaluate_models_multi(train_df, test_df, models, target_transforms, lag_transforms_options, optimal_lags_list)\n",
    "\n",
    "        # Save results\n",
    "        save_results(results, f\"results/run_7/{sensor_name}_{scenario_name}.csv\")\n",
    "\n",
    "    return results\n",
    "\n",
    "def run_all_scenarios_parallel(scenarios, selected_sensors_df, models, lag_transforms_options, ratios=[0.33, 0.66, 1], sensor_names=['2','5','6']):\n",
    "    # don't use all cpus (instead all but one)\n",
    "    results = Parallel(n_jobs=8)( \n",
    "        delayed(process_scenario)(scenario_name, scenario, selected_sensors_df, models, lag_transforms_options, ratios=ratios, sensor_names=sensor_names)\n",
    "        for scenario_name, scenario in scenarios.items()\n",
    "    )\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = {\n",
    "#     \"Lasso\": Lasso()\n",
    "# }\n",
    "\n",
    "# # Define lag transformations\n",
    "\n",
    "# lag_transforms_options = [\n",
    "#     {1: [expanding_mean], 7: [rolling_mean_14], 30: [expanding_mean]},\n",
    "# ]\n",
    "# results = run_all_scenarios_parallel(scenarios_sensors, selected_sensors_df, models, lag_transforms_options, ratios=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_sensors_df['full_date'] = pd.to_datetime(selected_sensors_df['full_date']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m run_all_scenarios_parallel(scenarios, selected_sensors_df, models, lag_transforms_options)\n",
      "Cell \u001b[1;32mIn[27], line 25\u001b[0m, in \u001b[0;36mrun_all_scenarios_parallel\u001b[1;34m(scenarios, selected_sensors_df, models, lag_transforms_options, ratios, sensor_names)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_all_scenarios_parallel\u001b[39m(scenarios, selected_sensors_df, models, lag_transforms_options, ratios\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.33\u001b[39m, \u001b[38;5;241m0.66\u001b[39m, \u001b[38;5;241m1\u001b[39m], sensor_names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m5\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m6\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m# don't use all cpus (instead all but one)\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m     results \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)( \n\u001b[0;32m     26\u001b[0m         delayed(process_scenario)(scenario_name, scenario, selected_sensors_df, models, lag_transforms_options, ratios\u001b[38;5;241m=\u001b[39mratios, sensor_names\u001b[38;5;241m=\u001b[39msensor_names)\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m scenario_name, scenario \u001b[38;5;129;01min\u001b[39;00m scenarios\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m     28\u001b[0m     )\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[1;32mc:\\Users\\PC314\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[1;32mc:\\Users\\PC314\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\PC314\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = run_all_scenarios_parallel(scenarios, selected_sensors_df, models, lag_transforms_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "cpu_count = multiprocessing.cpu_count()\n",
    "cpu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through scenarios and evaluate models\n",
    "results = []\n",
    "\n",
    "for sensor_name, scenarios in scenarios_sensors.items():\n",
    "    formatted_df = format_df_to_mlforecast(selected_sensors_df[['full_date', sensor_name]], 'full_date', sensor_name, unique_id=sensor_name)\n",
    "    formatted_df = formatted_df[['ds', 'y', 'unique_id']]\n",
    "\n",
    "    for scenario_name, scenario in scenarios.items():\n",
    "\n",
    "        train_df, test_df = split_data(formatted_df, scenario)\n",
    "\n",
    "        optimal_lags_list = get_optimal_lags(train_df, 'y', \n",
    "                                            # ratios=[1]\n",
    "                                            ratios=[0.33, 0.66, 1]\n",
    "                                            #  ratios=[0.25, 0.5, 0.75, 1]\n",
    "        )\n",
    "        target_transforms = get_dynamic_transforms(train_df)\n",
    "        results = evaluate_models(train_df, test_df, models, target_transforms, lag_transforms_options, optimal_lags_list)\n",
    "\n",
    "        save_results(results, f\"results/run_6/{sensor_name}_{scenario_name}.csv\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
