{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MLForecastPipeline import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_sensors_df = pd.read_csv(\"../data/selected_sensors2_cleaned.csv\", index_col=0)\n",
    "\n",
    "scenarios_sensors = {\n",
    "    # 0: 1, 4372603\n",
    "    # \"0_12M_train_7M_test\": {\"train_start\": \"2017-03-25\", \"train_end\": \"2018-03-25\", \"test_start\": \"2018-03-26\", \"test_end\": \"2018-10-10\"},\n",
    "    '2': {\n",
    "        \"26M_train\":  {\"train_start\": \"2017-04-01\", \"train_end\": \"2019-06-01\"},\n",
    "        \"24M_train\":  {\"train_start\": \"2017-04-01\", \"train_end\": \"2019-04-01\"},\n",
    "        \"22M_train\":  {\"train_start\": \"2017-04-01\", \"train_end\": \"2019-02-01\"},\n",
    "        \"20M_train\":  {\"train_start\": \"2017-04-01\", \"train_end\": \"2018-12-01\"},\n",
    "        \"18M_train\":  {\"train_start\": \"2017-04-01\", \"train_end\": \"2018-10-01\"},\n",
    "        \"12M_train\":  {\"train_start\": \"2017-04-01\", \"train_end\": \"2018-04-01\"},\n",
    "        \"10M_train\":  {\"train_start\": \"2017-04-01\", \"train_end\": \"2018-01-25\"},\n",
    "        \"8M_train\":   {\"train_start\": \"2017-04-01\", \"train_end\": \"2017-10-25\"},\n",
    "        \n",
    "        # Non-Heating Periods\n",
    "        \"NH_3M_train\":  {\"train_start\": \"2017-04-15\", \"train_end\": \"2017-07-15\"},\n",
    "        \"NH_4M_train\":  {\"train_start\": \"2017-04-15\", \"train_end\": \"2017-08-15\"},\n",
    "        \"NH_2M_train\":  {\"train_start\": \"2017-04-15\", \"train_end\": \"2017-06-15\"},\n",
    "        \"NH_1M_train\":  {\"train_start\": \"2017-04-15\", \"train_end\": \"2017-05-15\"},\n",
    "        \"NH_15D_train\": {\"train_start\": \"2017-04-15\", \"train_end\": \"2017-04-30\"},\n",
    "        \"NH_feb_2M_train\": {\"train_start\": \"2017-02-15\", \"train_end\": \"2017-04-15\"},\n",
    "        \"NH_feb_1M_train\": {\"train_start\": \"2017-02-15\", \"train_end\": \"2017-04-15\"},\n",
    "        \"NH_mar_2M_train\": {\"train_start\": \"2017-03-15\", \"train_end\": \"2017-05-15\"},\n",
    "        \"NH_mar_1M_train\": {\"train_start\": \"2017-03-15\", \"train_end\": \"2017-04-15\"},\n",
    "\n",
    "        # Heating Periods\n",
    "        \"H_5M_train\":     {\"train_start\": \"2017-06-01\", \"train_end\": \"2017-11-01\"},\n",
    "        \"H_3M_jul_train\": {\"train_start\": \"2017-07-01\", \"train_end\": \"2017-10-10\"},\n",
    "        \"H_3M_sep_train\": {\"train_start\": \"2017-09-01\", \"train_end\": \"2017-12-10\"},\n",
    "        \"H_3M_nov_train\": {\"train_start\": \"2017-11-01\", \"train_end\": \"2018-02-10\"},\n",
    "        },\n",
    "}\n",
    "scenarios_sensors['5'] = scenarios_sensors['2'].copy()\n",
    "scenarios_sensors['6'] = scenarios_sensors['2'].copy()\n",
    "\n",
    "def split_data(df, scenario, date_col=\"ds\"):\n",
    "    \"\"\"Extracts train and test data based on train end date.\"\"\"\n",
    "    train_data = df[df[date_col] <= scenario['train_end']]\n",
    "    test_start = pd.to_datetime(scenario['train_end']) + pd.Timedelta(days=1)\n",
    "    test_data = df[df[date_col] >= test_start]\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "LAG_TRANSFORMS_MAP = {\n",
    "    \"expanding_mean_rolling_14_rolling_30\": {1: 'expanding_mean', 7: 'expanding_mean', 30: 'rolling_mean_30'},\n",
    "    \"expanding_mean_rolling_14\": {1: 'expanding_mean', 7: 'rolling_mean_14', 30: 'expanding_mean'},\n",
    "    \"rolling_14_rolling_30_expanding\": {1: 'rolling_mean_14', 7: 'rolling_mean_30', 30: 'expanding_mean'},\n",
    "    \"rolling_14_expanding\": {1: 'rolling_mean_14', 30: 'expanding_mean'},\n",
    "    \"rolling_14_only\": {1: 'rolling_mean_14'},\n",
    "    \"no_transform\": {},\n",
    "}\n",
    "\n",
    "def map_lag_transforms(lag_transform_dict, lag_transforms_map=LAG_TRANSFORMS_MAP):\n",
    "    for name, transform in lag_transforms_map.items():\n",
    "        if lag_transform_dict == transform:\n",
    "            return name\n",
    "    return \"unknown\"\n",
    "\n",
    "def analyze_results(df, lag_transforms_map=LAG_TRANSFORMS_MAP, mape_threshold=40, model_filter=None):\n",
    "    df = df.copy()\n",
    "    df['Lag Transform Name'] = df['Lag Transforms'].apply(lambda x: map_lag_transforms(x, lag_transforms_map))\n",
    "    df['Lag_Set_Name'] = df['Lag Name']\n",
    "    # Identify MAPE columns dynamically\n",
    "    mape_columns = [col for col in df.columns if col.startswith(\"test_\") and col.endswith(\"_days\")]\n",
    "    \n",
    "    # Compute mean MAPE across all test periods\n",
    "    df['MAPE'] = df[mape_columns].mean(axis=1)\n",
    "    \n",
    "    # Apply filtering\n",
    "    top_df = df[df[\"MAPE\"] < mape_threshold].copy()\n",
    "    if model_filter:\n",
    "        top_df = top_df[top_df['Model'] == model_filter].copy()\n",
    "    \n",
    "    # Compute groupings\n",
    "    top_models = top_df.groupby(\"Model\")[\"MAPE\"].mean().sort_values().reset_index()\n",
    "    top_transforms = top_df.groupby(\"Transforms\")[\"MAPE\"].mean().sort_values().reset_index()\n",
    "    top_lag_transforms = top_df.groupby(\"Lag Transform Name\")[\"MAPE\"].mean().sort_values().reset_index()\n",
    "    top_lags = top_df.groupby(\"Lag_Set_Name\")[\"MAPE\"].mean().sort_values().reset_index()\n",
    "    \n",
    "    # Compute MAPE trends over different forecasting horizons\n",
    "    mape_trends = top_df.groupby(\"Model\")[mape_columns].mean().reset_index()\n",
    "    \n",
    "    return top_models, top_transforms, top_lag_transforms, top_lags, mape_trends\n",
    "\n",
    "def plot_results(top_models, top_transforms, top_lag_transforms, top_lags, mape_trends):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    sns.barplot(x=top_models[\"Model\"], y=top_models[\"MAPE\"], palette=\"viridis\", hue=top_models[\"Model\"])\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title(\"Average MAPE per Model\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    sns.barplot(x=top_transforms[\"Transforms\"], y=top_transforms[\"MAPE\"], palette=\"coolwarm\", hue=top_transforms[\"Transforms\"])\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(\"Average MAPE per Transform\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    sns.barplot(x=top_lag_transforms[\"Lag Transform Name\"], y=top_lag_transforms[\"MAPE\"], palette=\"Blues\", hue=top_lag_transforms[\"Lag Transform Name\"])\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(\"Average MAPE per Lag Transform\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    sns.barplot(x=top_lags[\"Lag_Set_Name\"], y=top_lags[\"MAPE\"], palette=\"Blues\", hue=top_lags[\"Lag_Set_Name\"])\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(\"MAPE vs Number of Lags\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot MAPE trends across different forecasting horizons\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    for model in mape_trends[\"Model\"]:\n",
    "        plt.plot(mape_trends.columns[1:], mape_trends[mape_trends[\"Model\"] == model].values[0][1:], label=model)\n",
    "    plt.xlabel(\"Forecasting Horizon (Days)\")\n",
    "    plt.ylabel(\"MAPE\")\n",
    "    plt.title(\"MAPE Trends Across Forecast Horizons\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# top_models, top_transforms, top_lag_transforms, top_lags, mape_trends = analyze_results(df, lag_transforms_map, optimal_lags_map)\n",
    "# plot_results(top_models, top_transforms, top_lag_transforms, top_lags, mape_trends)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "results = {}\n",
    "for file in glob.glob(\"results/run_7/*.csv\"):\n",
    "    dataset_name = file.split(\"\\\\\")[-1].replace(\".csv\", \"\")\n",
    "    results[dataset_name] = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_train_info(dataset_name):\n",
    "    \"\"\"\n",
    "    Extracts sensor ID, training length in months, and a standardized train label.\n",
    "    \n",
    "    Example Inputs:\n",
    "        - \"6_NH_15D_train\"  → (6, 0.5, \"NH_15D_train\")\n",
    "        - \"6_H_5M_train\"    → (6, 5, \"H_5M_train\")\n",
    "        - \"2_18M_train\"     → (2, 18, \"18M_train\")\n",
    "    \n",
    "    Returns:\n",
    "        - sensor (int): Sensor ID\n",
    "        - train_months (float): Training length in months\n",
    "        - train_label (str): Everything after the sensor ID (used for finding comparable datasets)\n",
    "    \"\"\"\n",
    "    name_parts = dataset_name.split('_')\n",
    "\n",
    "    # Extract sensor ID\n",
    "    sensor = int(name_parts[0])  # First part is always the sensor number\n",
    "\n",
    "    # Reconstruct the label for easy dataset comparison\n",
    "    train_label = '_'.join(name_parts[1:])\n",
    "\n",
    "    # Extract training length (2nd to last part contains number + unit)\n",
    "    train_info = name_parts[-2]  # Example: \"15D\" or \"5M\"\n",
    "    match = re.match(r\"(\\d+)([MD])\", train_info)  # Extract number and unit\n",
    "\n",
    "    if match:\n",
    "        train_length = int(match.group(1))\n",
    "        unit = match.group(2)\n",
    "\n",
    "        # Convert days to months (approximate)\n",
    "        train_months = train_length / 30 if unit == \"D\" else train_length\n",
    "    else:\n",
    "        return None, None, None  # Invalid format, return None values\n",
    "\n",
    "    return sensor, train_months, train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numba import njit\n",
    "from window_ops.expanding import expanding_mean\n",
    "from window_ops.rolling import rolling_mean\n",
    "\n",
    "@njit\n",
    "def rolling_mean_14(x):\n",
    "    return rolling_mean(x, window_size=14)\n",
    "\n",
    "@njit\n",
    "def rolling_mean_30(x):\n",
    "    return rolling_mean(x, window_size=30)\n",
    "\n",
    "# Define function mapping\n",
    "lag_transforms_mapping = {\n",
    "    'expanding_mean': expanding_mean,\n",
    "    'rolling_mean_14': rolling_mean_14,\n",
    "    'rolling_mean_30': rolling_mean_30,\n",
    "}\n",
    "\n",
    "# Function to convert lag_transforms dictionary to a string\n",
    "def stringify_lag_transforms(lag_transforms):\n",
    "    \"\"\"Converts lag_transforms dictionary to a clean string format for storage.\"\"\"\n",
    "    return str({key: [func.__name__ for func in funcs] for key, funcs in lag_transforms.items()})\n",
    "\n",
    "# Function to parse lag_transforms safely from an invalid dictionary-like string\n",
    "def parse_lag_transforms(lag_transforms_str):\n",
    "    \"\"\"Parses a raw function dictionary string and converts it back to a proper dictionary with function references.\"\"\"\n",
    "    try:\n",
    "        # Extract function names using regex pattern: `<function function_name at 0x...>`\n",
    "        cleaned_str = re.sub(r'<function (\\w+) at 0x[0-9A-Fa-f]+>', r'\"\\1\"', lag_transforms_str)\n",
    "\n",
    "        # Convert cleaned string into a valid Python dictionary\n",
    "        temp_dict = eval(cleaned_str)  # Evaluates after function names are fixed\n",
    "\n",
    "        # Map function names back to actual function references\n",
    "        return {key: [lag_transforms_mapping[func_name] for func_name in funcs] for key, funcs in temp_dict.items()}\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing lag_transforms {lag_transforms_str}: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from numba import njit\n",
    "from window_ops.expanding import expanding_mean\n",
    "from window_ops.rolling import rolling_mean\n",
    "from mlforecast import MLForecast\n",
    "# from my_custom_utils import format_df_to_mlforecast, split_data  # Assuming these are in a module\n",
    "import ast\n",
    "\n",
    "# Define parameters\n",
    "HORIZONS = [30, 60, 90, 120, 150, 180, 240, 300, 360, 480, 600, 720, 737]\n",
    "MAPE_COLUMNS = [f\"test_{h}_days\" for h in HORIZONS]\n",
    "\n",
    "# Thresholds for different filtering criteria\n",
    "EARLY_HORIZON_THRESHOLD = 30  # MAPE threshold for early horizons\n",
    "ONE_THIRD_THRESHOLD = 30       # Threshold for MAPE at 1/3 of training length\n",
    "ONE_THIRD_THRESHOLD_GENERAL = 35       # Threshold for MAPE at 1/3 of training length\n",
    "\n",
    "\n",
    "EARLY_HORIZONS = [30, 60, 90, 120]  # Horizons we check for early filtering\n",
    "SENSORS_TO_COMPARE = [2, 5, 6]  # Sensors that share training length criteria\n",
    "\n",
    "# Define available lag transforms\n",
    "lag_transforms_options = [\n",
    "    {1: [expanding_mean], 7: [rolling_mean], 30: [expanding_mean]},\n",
    "    {1: [rolling_mean], 7: [rolling_mean], 30: [expanding_mean]},\n",
    "]\n",
    "\n",
    "model_mapping = {\n",
    "    \"XGBRegressor\": XGBRegressor(),\n",
    "    \"SGDRegressor_42\": SGDRegressor(random_state=42),\n",
    "    \"Ridge\": Ridge(),\n",
    "    \"Lasso\": Lasso()\n",
    "}\n",
    "\n",
    "transform_mapping = {\n",
    "    \"AutoDifferences\" : AutoDifferences, \n",
    "    \"AutoSeasonalDifferences\" : AutoSeasonalDifferences, \n",
    "    \"AutoSeasonalityAndDifferences\" : AutoSeasonalityAndDifferences,\n",
    "    \"LocalStandardScaler\" : LocalStandardScaler, \n",
    "    \"LocalMinMaxScaler\" : LocalMinMaxScaler, \n",
    "    \"LocalBoxCox\" : LocalBoxCox\n",
    "}\n",
    "\n",
    "@njit\n",
    "def rolling_mean_14(x):\n",
    "    return rolling_mean(x, window_size=14)\n",
    "\n",
    "@njit\n",
    "def rolling_mean_30(x):\n",
    "    return rolling_mean(x, window_size=30)\n",
    "\n",
    "# Function to extract sensor ID and train length\n",
    "def extract_train_info(dataset_name):\n",
    "    name_parts = dataset_name.split('_')\n",
    "    sensor = int(name_parts[0])  \n",
    "    train_label = '_'.join(name_parts[1:])\n",
    "\n",
    "    train_info = name_parts[-2]  \n",
    "    match = re.match(r\"(\\d+)([MD])\", train_info)  \n",
    "\n",
    "    if match:\n",
    "        train_length = int(match.group(1))\n",
    "        unit = match.group(2)\n",
    "        train_months = train_length / 30 if unit == \"D\" else train_length\n",
    "    else:\n",
    "        return None, None, None  \n",
    "\n",
    "    return sensor, train_months, train_label\n",
    "\n",
    "# Function to reverse `stringify_transform`\n",
    "def parse_transform_string(transform_str):\n",
    "    transform_str = transform_str.strip()\n",
    "    if \"(\" in transform_str:\n",
    "        class_name, params = transform_str.split(\"(\", 1)\n",
    "        params = params.rstrip(\")\")\n",
    "        param_dict = {}\n",
    "        if params != \"NoParams\" and class_name != \"LocalBoxCox\":\n",
    "            for param in params.split(\", \"):\n",
    "                key, value = param.split(\"=\")\n",
    "                if key in (\"max_diffs\", \"season_length\", \"max_season_length\"):\n",
    "                    param_dict[key] = int(value) if value.replace(\".\", \"\").isdigit() else value\n",
    "        return class_name, param_dict\n",
    "    return transform_str, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define patterns to match and their correct format\n",
    "rename_patterns = {\n",
    "    \"H_3M_jul_train\": \"H_jul_3M_train\",\n",
    "    \"H_3M_sep_train\": \"H_sep_3M_train\",\n",
    "    \"H_3M_nov_train\": \"H_nov_3M_train\",\n",
    "}\n",
    "\n",
    "# Create a new dictionary with updated keys\n",
    "updated_results = {}\n",
    "\n",
    "for dataset_name, df in results.items():\n",
    "    new_name = dataset_name  # Default: keep the same name\n",
    "\n",
    "    for old_pattern, new_pattern in rename_patterns.items():\n",
    "        if old_pattern in dataset_name:\n",
    "            new_name = dataset_name.replace(old_pattern, new_pattern)\n",
    "            break  # Stop checking once renamed\n",
    "\n",
    "    updated_results[new_name] = df  # Preserve the dataset content\n",
    "\n",
    "results = updated_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import json\n",
    "\n",
    "def generate_model_id(model_name, transforms, lags, lag_transforms_str, dataset_name):\n",
    "    \"\"\"Generate a unique hash ID for a model based on its core attributes.\"\"\"\n",
    "    model_info = {\n",
    "        \"model\": model_name,\n",
    "        \"transforms\": transforms,\n",
    "        \"lags\": lags,\n",
    "        \"lag_transforms\": lag_transforms_str,\n",
    "        \"dataset\": dataset_name  # Include dataset name\n",
    "    }\n",
    "    model_str = json.dumps(model_info, sort_keys=True)\n",
    "    return hashlib.sha256(model_str.encode()).hexdigest()[:10]  # Shorten to 10 chars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATASET: 2_10M_train\n",
      "================================================================================\n",
      "\n",
      "Processing Sensor 2...\n",
      "\n",
      "Processing Sensor 5...\n",
      "\n",
      "Processing Sensor 6...\n",
      "================================================================================\n",
      "DATASET: 2_12M_train\n",
      "================================================================================\n",
      "\n",
      "Processing Sensor 2...\n",
      "\n",
      "Processing Sensor 5...\n",
      "\n",
      "Processing Sensor 6...\n",
      "================================================================================\n",
      "DATASET: 2_18M_train\n",
      "================================================================================\n",
      "\n",
      "Processing Sensor 2...\n",
      "\n",
      "Processing Sensor 5...\n",
      "\n",
      "Processing Sensor 6...\n",
      "================================================================================\n",
      "DATASET: 2_20M_train\n",
      "================================================================================\n",
      "\n",
      "Processing Sensor 2...\n",
      "\n",
      "Processing Sensor 5...\n",
      "\n",
      "Processing Sensor 6...\n",
      "================================================================================\n",
      "DATASET: 2_22M_train\n",
      "================================================================================\n",
      "\n",
      "Processing Sensor 2...\n",
      "\n",
      "Processing Sensor 5...\n",
      "\n",
      "Processing Sensor 6...\n",
      "================================================================================\n",
      "DATASET: 2_24M_train\n",
      "================================================================================\n",
      "\n",
      "Processing Sensor 2...\n",
      "\n",
      "Processing Sensor 5...\n",
      "\n",
      "Processing Sensor 6...\n",
      "================================================================================\n",
      "DATASET: 2_26M_train\n",
      "================================================================================\n",
      "\n",
      "Processing Sensor 2...\n",
      "\n",
      "Processing Sensor 5...\n",
      "\n",
      "Processing Sensor 6...\n",
      "================================================================================\n",
      "DATASET: 2_8M_train\n",
      "================================================================================\n",
      "\n",
      "Processing Sensor 2...\n",
      "\n",
      "Processing Sensor 5...\n",
      "\n",
      "Processing Sensor 6...\n",
      "================================================================================\n",
      "DATASET: 5_26M_train\n",
      "================================================================================\n",
      "\n",
      "Processing Sensor 2...\n",
      "\n",
      "Processing Sensor 5...\n",
      "\n",
      "Processing Sensor 6...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import hashlib\n",
    "import json\n",
    "\n",
    "# Define parameters\n",
    "HORIZONS = [30, 60, 90, 120, 150, 180, 240, 300, 360, 480, 600, 720, 737]\n",
    "EARLY_HORIZON_THRESHOLD = 25\n",
    "REFERENCE_THRESHOLD = 30\n",
    "ONE_THIRD_THRESHOLD = 30\n",
    "\n",
    "EARLY_HORIZONS = [30, 60, 90, 120]\n",
    "SENSORS_TO_COMPARE = [2, 5, 6]\n",
    "\n",
    "stored_models = {}  # Store all models globally\n",
    "\n",
    "def generate_model_id(model_name, transforms, lags, lag_transforms, dataset_name):\n",
    "    \"\"\"Generate a unique hash ID based on model attributes and dataset.\"\"\"\n",
    "    model_info = {\n",
    "        \"model\": model_name,\n",
    "        \"transforms\": transforms,\n",
    "        \"lags\": lags,\n",
    "        \"lag_transforms\": lag_transforms,\n",
    "        \"dataset\": dataset_name  # Ensures uniqueness across datasets\n",
    "    }\n",
    "    model_str = json.dumps(model_info, sort_keys=True)\n",
    "    return hashlib.sha256(model_str.encode()).hexdigest()[:10]  # Short ID\n",
    "\n",
    "for dataset_name, df in results.items():\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"DATASET: {dataset_name}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    sensor_id_main, train_months, train_label = extract_train_info(dataset_name)\n",
    "    if sensor_id_main is None:\n",
    "        print(\"Skipping dataset: Cannot determine sensor ID or train length.\")\n",
    "        continue\n",
    "\n",
    "    # Extract unique sensors from column names\n",
    "    sensor_pattern = re.compile(r\"(\\d+)_test_(\\d+)_days\")\n",
    "    sensor_horizons = {}\n",
    "\n",
    "    for col in df.columns:\n",
    "        match = sensor_pattern.match(col)\n",
    "        if match:\n",
    "            sensor_id, horizon = match.groups()\n",
    "            horizon = int(horizon)\n",
    "            sensor_horizons.setdefault(sensor_id, []).append(horizon)\n",
    "\n",
    "    filtered_results = {}\n",
    "\n",
    "    for sensor_id, horizons in sensor_horizons.items():\n",
    "        print(f\"\\nProcessing Sensor {sensor_id}...\")\n",
    "\n",
    "        sensor_mape_cols = [f\"{sensor_id}_test_{h}_days\" for h in sorted(horizons)]\n",
    "        early_horizon_cols = [f\"{sensor_id}_test_{h}_days\" for h in EARLY_HORIZONS if f\"{sensor_id}_test_{h}_days\" in df.columns]\n",
    "\n",
    "        if not early_horizon_cols:\n",
    "            continue  # Skip if no early horizon data\n",
    "\n",
    "        # Apply Early Horizon Filter\n",
    "        mask_early_horizon = (df[early_horizon_cols] <= EARLY_HORIZON_THRESHOLD).all(axis=1)\n",
    "\n",
    "        # Reference Sensors Filtering\n",
    "        reference_mape_values = []\n",
    "        for ref_sensor in SENSORS_TO_COMPARE:\n",
    "            if str(ref_sensor) == sensor_id:\n",
    "                continue\n",
    "            ref_cols = [f\"{ref_sensor}_test_{h}_days\" for h in EARLY_HORIZONS if f\"{ref_sensor}_test_{h}_days\" in df.columns]\n",
    "            if ref_cols:\n",
    "                reference_mape_values.append(df[ref_cols].mean(axis=1))\n",
    "\n",
    "        mask_reference_sensors = True  # Default to True if no references found\n",
    "        if reference_mape_values:\n",
    "            reference_mape = np.mean(reference_mape_values, axis=0)\n",
    "            mask_reference_sensors = df[early_horizon_cols].mean(axis=1) <= reference_mape\n",
    "\n",
    "        # One-Third Training Length Filter\n",
    "        one_third_horizon = int(train_months * 30 // 3)\n",
    "        closest_horizon = min(horizons, key=lambda x: abs(x - one_third_horizon))\n",
    "        mask_one_third = df.get(f\"{sensor_id}_test_{closest_horizon}_days\", pd.Series(True)) <= ONE_THIRD_THRESHOLD\n",
    "\n",
    "        # Apply Filters\n",
    "        df_filtered = df[mask_early_horizon & mask_reference_sensors & mask_one_third].copy()\n",
    "        if df_filtered.empty:\n",
    "            continue\n",
    "\n",
    "        # Compute Stability Metrics\n",
    "        df_filtered[f\"{sensor_id}_std_mape\"] = df_filtered[sensor_mape_cols].std(axis=1)\n",
    "        df_filtered[f\"{sensor_id}_avg_mape\"] = df_filtered[sensor_mape_cols].mean(axis=1)\n",
    "\n",
    "        # Identify Best Model Per Horizon\n",
    "        best_models = {}\n",
    "        for col in sensor_mape_cols:\n",
    "            if not df_filtered.empty and df_filtered[col].notna().sum() > 0:\n",
    "                best_idx = df_filtered[col].idxmin()\n",
    "                best_models[col] = df_filtered.loc[best_idx]\n",
    "\n",
    "        # Identify Most Stable Model\n",
    "        if df_filtered.empty:\n",
    "            continue\n",
    "        best_stability_idx = df_filtered[f\"{sensor_id}_std_mape\"].idxmin()\n",
    "        best_stability_row = df_filtered.loc[best_stability_idx]\n",
    "\n",
    "        filtered_results[sensor_id] = {\n",
    "            \"df_filtered\": df_filtered,\n",
    "            \"best_models\": best_models,\n",
    "            \"best_stability_row\": best_stability_row,\n",
    "        }\n",
    "\n",
    "    # **Store Best Models**\n",
    "    for _, sensor_results in filtered_results.items():\n",
    "        for horizon, row in sensor_results[\"best_models\"].items():\n",
    "            model_name, transforms, lags, lag_transforms_str = row[\"Model\"], row[\"Transforms\"], row[\"Lags\"], row[\"Lag Transforms\"]\n",
    "\n",
    "            model_id = generate_model_id(model_name, transforms, lags, lag_transforms_str, dataset_name)\n",
    "\n",
    "            # Use dataset_name as top-level key (sensor_id not needed separately)\n",
    "            stored_models.setdefault(dataset_name, {}).setdefault(model_id, {\n",
    "                \"model\": model_name,\n",
    "                \"transforms\": transforms,\n",
    "                \"lags\": lags,\n",
    "                \"lag_transforms\": lag_transforms_str,\n",
    "                \"horizons\": {}\n",
    "            })[\"horizons\"][horizon] = row[horizon]  # Add horizon data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Best Models for Dataset 2_12M_train ===\n",
      "ID: 188b8a690f\n",
      "  Model: Lasso\n",
      "  Transforms: AutoDifferences(max_diffs=188)\n",
      "  Lags: [1, 2, 3, 4, 5, 6, 7, 8, 9, 14, 15, 16, 23, 29, 33]\n",
      "  MAPE per Horizon:\n",
      "    2_test_30_days days: MAPE=17.74\n",
      "    2_test_60_days days: MAPE=18.49\n",
      "\n",
      "ID: 6f6a88f17c\n",
      "  Model: Ridge\n",
      "  Transforms: AutoDifferences(max_diffs=188) | LocalStandardScaler(stats_=[[ 1.95070557e-02  2.04360268e+01]\n",
      " [-1.89855432e-02  3.18971791e+01]\n",
      " [-7.62623944e-03  2.42097752e+01]])\n",
      "  Lags: [1, 2, 3, 4, 5, 6, 7, 8, 9, 14, 15, 16, 23, 29, 33]\n",
      "  MAPE per Horizon:\n",
      "    2_test_90_days days: MAPE=18.48\n",
      "\n",
      "ID: d10bd54db3\n",
      "  Model: Ridge\n",
      "  Transforms: AutoDifferences(max_diffs=188) | LocalStandardScaler(stats_=[[ 1.95070557e-02  2.04360268e+01]\n",
      " [-1.89855432e-02  3.18971791e+01]\n",
      " [-7.62623944e-03  2.42097752e+01]])\n",
      "  Lags: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 23, 24, 25, 26, 28, 29, 32, 33, 37, 40, 41, 42, 43]\n",
      "  MAPE per Horizon:\n",
      "    2_test_120_days days: MAPE=18.13\n",
      "\n",
      "ID: 5308482a5d\n",
      "  Model: XGBRegressor\n",
      "  Transforms: LocalMinMaxScaler(stats_=[[  5.11211699 183.47202765]\n",
      " [  5.1691023  286.54204218]\n",
      " [  6.43602225 208.81675552]])\n",
      "  Lags: [1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 14, 15, 16, 21, 23, 24, 25, 26, 29, 33, 49, 57, 58, 67, 68, 69, 78, 79, 80, 83, 94]\n",
      "  MAPE per Horizon:\n",
      "    2_test_150_days days: MAPE=17.96\n",
      "    2_test_240_days days: MAPE=19.18\n",
      "    2_test_300_days days: MAPE=22.82\n",
      "\n",
      "ID: 1b6481d847\n",
      "  Model: Ridge\n",
      "  Transforms: AutoDifferences(max_diffs=188) | LocalStandardScaler(stats_=[[ 1.95070557e-02  2.04360268e+01]\n",
      " [-1.89855432e-02  3.18971791e+01]\n",
      " [-7.62623944e-03  2.42097752e+01]])\n",
      "  Lags: [1, 2, 3, 4, 5, 6, 7, 8, 9, 14, 15, 16, 23, 29, 33]\n",
      "  MAPE per Horizon:\n",
      "    2_test_180_days days: MAPE=18.19\n",
      "    2_test_360_days days: MAPE=24.01\n",
      "    2_test_480_days days: MAPE=22.07\n",
      "    2_test_600_days days: MAPE=23.07\n",
      "    2_test_720_days days: MAPE=25.10\n",
      "\n",
      "ID: 1a6b38364a\n",
      "  Model: Lasso\n",
      "  Transforms: AutoDifferences(max_diffs=188)\n",
      "  Lags: [1, 2, 3, 4, 5, 6, 7, 8, 9, 14, 15, 16, 23, 29, 33]\n",
      "  MAPE per Horizon:\n",
      "    2_test_2760_days days: MAPE=28.45\n",
      "\n",
      "\n",
      "=== Best Models for Dataset 2_22M_train ===\n",
      "ID: df4f1eafd5\n",
      "  Model: XGBRegressor\n",
      "  Transforms: nan\n",
      "  Lags: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 23, 26, 27, 28, 29, 32, 33, 34, 36, 37, 38, 39, 40, 41, 43, 44, 45, 47, 48, 61, 62, 63, 64, 65, 67, 69, 71, 72, 73, 74, 77, 78, 79, 80, 81, 83]\n",
      "  MAPE per Horizon:\n",
      "    2_test_120_days days: MAPE=21.49\n",
      "    2_test_150_days days: MAPE=21.62\n",
      "    2_test_180_days days: MAPE=24.24\n",
      "    2_test_1842_days days: MAPE=64.25\n",
      "    2_test_240_days days: MAPE=26.24\n",
      "    2_test_300_days days: MAPE=26.52\n",
      "    2_test_30_days days: MAPE=22.48\n",
      "    2_test_360_days days: MAPE=29.67\n",
      "    2_test_480_days days: MAPE=42.29\n",
      "    2_test_600_days days: MAPE=64.32\n",
      "    2_test_60_days days: MAPE=20.86\n",
      "    2_test_720_days days: MAPE=64.25\n",
      "    2_test_90_days days: MAPE=20.01\n",
      "\n",
      "\n",
      "=== Best Models for Dataset 2_24M_train ===\n",
      "ID: f8b580bd7d\n",
      "  Model: XGBRegressor\n",
      "  Transforms: LocalBoxCox(lower=0.0, method=loglik, season_length=None, stats_=[[4.10226185e-05 3.25928969e+02]\n",
      " [4.10226185e-05 2.86542042e+02]\n",
      " [4.10226185e-05 2.11616531e+02]], upper=2.0)\n",
      "  Lags: [1, 2, 3, 4, 5, 6, 7, 8, 11, 12, 13, 14, 15, 16, 17, 18, 21, 26, 27, 28, 30, 33, 36, 37, 47, 52, 62, 75, 87, 98, 101, 102, 107, 109, 114, 120, 126, 138, 144, 145, 146, 147, 148, 157, 159, 161, 162, 163, 164, 165, 168, 169, 176, 181, 184, 185, 186, 189, 190, 191, 192, 193, 194, 195, 199, 201, 202, 209, 210, 211, 213, 215, 216, 218, 219, 220, 225, 227, 228, 229, 244, 245, 248, 251, 253, 255, 272, 282, 288, 305, 307, 308, 311, 320, 321, 328, 335, 337, 338, 339, 340, 342, 347, 348, 350, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 367, 368, 370]\n",
      "  MAPE per Horizon:\n",
      "    2_test_30_days days: MAPE=17.94\n",
      "\n",
      "ID: 3b75702966\n",
      "  Model: Ridge\n",
      "  Transforms: LocalBoxCox(lower=0.0, method=loglik, season_length=None, stats_=[[4.10226185e-05 3.25928969e+02]\n",
      " [4.10226185e-05 2.86542042e+02]\n",
      " [4.10226185e-05 2.11616531e+02]], upper=2.0)\n",
      "  Lags: [1, 2, 3, 4, 5, 6, 7, 8, 9, 13, 14, 15, 16, 18, 27, 28, 34, 36, 37, 41, 57, 60, 72, 73, 74, 75, 76, 78, 82, 92]\n",
      "  MAPE per Horizon:\n",
      "    2_test_60_days days: MAPE=17.50\n",
      "\n",
      "ID: b90c8bd422\n",
      "  Model: Ridge\n",
      "  Transforms: LocalBoxCox(lower=0.0, method=loglik, season_length=None, stats_=[[4.10226185e-05 3.25928969e+02]\n",
      " [4.10226185e-05 2.86542042e+02]\n",
      " [4.10226185e-05 2.11616531e+02]], upper=2.0)\n",
      "  Lags: [1, 2, 3, 4, 5, 6, 7, 8, 9, 13, 14, 15, 16, 18, 27, 28, 34, 36, 37, 41, 57, 60, 72, 73, 74, 75, 76, 78, 82, 92]\n",
      "  MAPE per Horizon:\n",
      "    2_test_90_days days: MAPE=15.88\n",
      "\n",
      "ID: 8ab48ac0fa\n",
      "  Model: SGDRegressor\n",
      "  Transforms: LocalMinMaxScaler(stats_=[[  5.11211699 325.92896936]\n",
      " [  5.1691023  286.54204218]\n",
      " [  3.63624679 211.61653099]])\n",
      "  Lags: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370]\n",
      "  MAPE per Horizon:\n",
      "    2_test_120_days days: MAPE=17.64\n",
      "    2_test_150_days days: MAPE=19.45\n",
      "    2_test_180_days days: MAPE=19.00\n",
      "\n",
      "ID: fef5c0bf52\n",
      "  Model: XGBRegressor\n",
      "  Transforms: LocalStandardScaler(stats_=[[48.56416613 39.6199012 ]\n",
      " [50.43335601 49.88962405]\n",
      " [43.01435456 36.25880212]])\n",
      "  Lags: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92]\n",
      "  MAPE per Horizon:\n",
      "    2_test_240_days days: MAPE=21.29\n",
      "\n",
      "ID: 8fcc2016f4\n",
      "  Model: Ridge\n",
      "  Transforms: LocalBoxCox(lower=0.0, method=loglik, season_length=None, stats_=[[4.10226185e-05 3.25928969e+02]\n",
      " [4.10226185e-05 2.86542042e+02]\n",
      " [4.10226185e-05 2.11616531e+02]], upper=2.0)\n",
      "  Lags: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92]\n",
      "  MAPE per Horizon:\n",
      "    2_test_1665_days days: MAPE=25.86\n",
      "    2_test_300_days days: MAPE=24.84\n",
      "    2_test_360_days days: MAPE=25.80\n",
      "    2_test_480_days days: MAPE=26.28\n",
      "    2_test_600_days days: MAPE=25.86\n",
      "    2_test_720_days days: MAPE=25.86\n",
      "\n",
      "\n",
      "=== Best Models for Dataset 2_26M_train ===\n",
      "ID: edc4ca5634\n",
      "  Model: Ridge\n",
      "  Transforms: AutoDifferences(max_diffs=380) | LocalStandardScaler(stats_=[[-2.09155811e-02  2.20585897e+01]\n",
      " [-2.55305868e-02  3.10128893e+01]\n",
      " [-3.23691980e-02  2.27027448e+01]])\n",
      "  Lags: [1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 14, 15, 16, 17, 18, 27, 28, 34, 36, 37, 41, 48, 57, 63, 71, 73, 74, 75, 78, 79, 80, 97]\n",
      "  MAPE per Horizon:\n",
      "    2_test_30_days days: MAPE=10.37\n",
      "    2_test_90_days days: MAPE=19.31\n",
      "\n",
      "ID: 54fb16ea15\n",
      "  Model: XGBRegressor\n",
      "  Transforms: AutoDifferences(max_diffs=380) | LocalStandardScaler(stats_=[[-2.09155811e-02  2.20585897e+01]\n",
      " [-2.55305868e-02  3.10128893e+01]\n",
      " [-3.23691980e-02  2.27027448e+01]])\n",
      "  Lags: [1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 14, 15, 16, 17, 18, 27, 28, 34, 36, 37, 41, 48, 57, 63, 71, 73, 74, 75, 78, 79, 80, 97]\n",
      "  MAPE per Horizon:\n",
      "    2_test_60_days days: MAPE=16.46\n",
      "\n",
      "ID: c87ddb27ba\n",
      "  Model: XGBRegressor\n",
      "  Transforms: LocalStandardScaler(stats_=[[47.42372721 38.3630083 ]\n",
      " [48.27088754 48.59938052]\n",
      " [41.02913136 35.59567409]])\n",
      "  Lags: [1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 14, 15, 16, 17, 18, 21, 22, 23, 26, 27, 28, 33, 36, 37, 41, 43, 48, 62, 63, 64, 72, 73, 74, 78, 79, 85, 101, 107, 114, 118, 130, 131, 136, 140, 147, 149, 152, 157, 159, 160, 161, 169, 170, 175, 177, 185, 188, 189, 191, 192, 194, 195, 196, 198, 199]\n",
      "  MAPE per Horizon:\n",
      "    2_test_120_days days: MAPE=19.68\n",
      "\n",
      "ID: 88efb61823\n",
      "  Model: XGBRegressor\n",
      "  Transforms: AutoDifferences(max_diffs=380) | LocalStandardScaler(stats_=[[-2.09155811e-02  2.20585897e+01]\n",
      " [-2.55305868e-02  3.10128893e+01]\n",
      " [-3.23691980e-02  2.27027448e+01]])\n",
      "  Lags: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100]\n",
      "  MAPE per Horizon:\n",
      "    2_test_150_days days: MAPE=19.25\n",
      "    2_test_180_days days: MAPE=20.52\n",
      "\n",
      "ID: fd3b952d1c\n",
      "  Model: XGBRegressor\n",
      "  Transforms: LocalStandardScaler(stats_=[[47.42372721 38.3630083 ]\n",
      " [48.27088754 48.59938052]\n",
      " [41.02913136 35.59567409]])\n",
      "  Lags: [1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 14, 15, 16, 17, 18, 27, 28, 34, 36, 37, 41, 48, 57, 63, 71, 73, 74, 75, 78, 79, 80, 97]\n",
      "  MAPE per Horizon:\n",
      "    2_test_240_days days: MAPE=25.04\n",
      "\n",
      "ID: 3d886f2725\n",
      "  Model: SGDRegressor\n",
      "  Transforms: AutoDifferences(max_diffs=380) | LocalStandardScaler(stats_=[[-2.09155811e-02  2.20585897e+01]\n",
      " [-2.55305868e-02  3.10128893e+01]\n",
      " [-3.23691980e-02  2.27027448e+01]])\n",
      "  Lags: [1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 14, 15, 16, 17, 18, 27, 28, 34, 36, 37, 41, 48, 57, 63, 71, 73, 74, 75, 78, 79, 80, 97]\n",
      "  MAPE per Horizon:\n",
      "    2_test_300_days days: MAPE=26.50\n",
      "    2_test_360_days days: MAPE=27.19\n",
      "\n",
      "ID: 6e8e466681\n",
      "  Model: SGDRegressor\n",
      "  Transforms: AutoDifferences(max_diffs=380) | LocalStandardScaler(stats_=[[-2.09155811e-02  2.20585897e+01]\n",
      " [-2.55305868e-02  3.10128893e+01]\n",
      " [-3.23691980e-02  2.27027448e+01]])\n",
      "  Lags: [1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 14, 15, 16, 17, 18, 27, 28, 34, 36, 37, 41, 48, 57, 63, 71, 73, 74, 75, 78, 79, 80, 97]\n",
      "  MAPE per Horizon:\n",
      "    2_test_1482_days days: MAPE=29.25\n",
      "    2_test_480_days days: MAPE=29.64\n",
      "    2_test_600_days days: MAPE=29.25\n",
      "    2_test_720_days days: MAPE=29.25\n",
      "\n",
      "\n",
      "=== Best Models for Dataset 5_26M_train ===\n",
      "ID: 5e3c60795c\n",
      "  Model: XGBRegressor\n",
      "  Transforms: nan\n",
      "  Lags: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 31, 33, 34, 35, 38, 39, 40, 42, 44, 46, 47, 48, 50, 52, 55, 62, 65, 70, 72, 73, 75, 76, 78, 79, 80, 81, 83, 85, 86, 88, 90, 91, 93, 95, 96, 99, 100]\n",
      "  MAPE per Horizon:\n",
      "    2_test_30_days days: MAPE=9.30\n",
      "\n",
      "ID: b55a3ac1b0\n",
      "  Model: XGBRegressor\n",
      "  Transforms: LocalMinMaxScaler(stats_=[[  5.11211699 325.92896936]\n",
      " [  5.1691023  286.54204218]\n",
      " [  3.63624679 211.61653099]])\n",
      "  Lags: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 17, 18, 19, 21, 22, 23, 25, 28, 29, 34, 62, 76, 80, 81, 86, 91, 93, 95, 96]\n",
      "  MAPE per Horizon:\n",
      "    2_test_120_days days: MAPE=17.75\n",
      "    2_test_180_days days: MAPE=19.29\n",
      "    2_test_60_days days: MAPE=15.00\n",
      "    2_test_90_days days: MAPE=17.98\n",
      "\n",
      "ID: abfbe5ace5\n",
      "  Model: SGDRegressor\n",
      "  Transforms: LocalStandardScaler(stats_=[[47.42372721 38.3630083 ]\n",
      " [48.27088754 48.59938052]\n",
      " [41.02913136 35.59567409]])\n",
      "  Lags: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 17, 18, 19, 21, 22, 23, 25, 28, 29, 34, 62, 76, 80, 81, 86, 91, 93, 95, 96]\n",
      "  MAPE per Horizon:\n",
      "    2_test_150_days days: MAPE=18.14\n",
      "\n",
      "ID: 9e43db0f88\n",
      "  Model: XGBRegressor\n",
      "  Transforms: AutoDifferences(max_diffs=380) | LocalStandardScaler(stats_=[[-2.09155811e-02  2.20585897e+01]\n",
      " [-2.55305868e-02  3.10128893e+01]\n",
      " [-3.23691980e-02  2.27027448e+01]])\n",
      "  Lags: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 19, 20, 21, 22, 23, 24, 25, 27, 29, 30, 33, 34, 35, 36, 38, 39, 40, 41, 42, 44, 46, 55, 62, 75, 80, 86, 89, 90, 91, 93, 95, 98, 100, 101, 106, 107, 108, 109, 110, 115, 116, 117, 119, 120, 121, 123, 124, 126, 127, 129, 130, 131, 132, 134, 135, 136, 138, 139, 141, 142, 143, 144, 146, 147, 148, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200]\n",
      "  MAPE per Horizon:\n",
      "    2_test_240_days days: MAPE=24.96\n",
      "\n",
      "ID: ed1d102b45\n",
      "  Model: XGBRegressor\n",
      "  Transforms: LocalBoxCox(lower=0.0, method=loglik, season_length=None, stats_=[[4.10226185e-05 3.25928969e+02]\n",
      " [4.10226185e-05 2.86542042e+02]\n",
      " [4.10226185e-05 2.11616531e+02]], upper=2.0)\n",
      "  Lags: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 17, 18, 19, 21, 22, 23, 25, 28, 29, 34, 62, 76, 80, 81, 86, 91, 93, 95, 96]\n",
      "  MAPE per Horizon:\n",
      "    2_test_300_days days: MAPE=28.73\n",
      "    2_test_360_days days: MAPE=30.38\n",
      "\n",
      "ID: c54a4e855f\n",
      "  Model: XGBRegressor\n",
      "  Transforms: LocalBoxCox(lower=0.0, method=loglik, season_length=None, stats_=[[4.10226185e-05 3.25928969e+02]\n",
      " [4.10226185e-05 2.86542042e+02]\n",
      " [4.10226185e-05 2.11616531e+02]], upper=2.0)\n",
      "  Lags: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 17, 18, 19, 21, 22, 23, 25, 28, 29, 34, 62, 76, 80, 81, 86, 91, 93, 95, 96]\n",
      "  MAPE per Horizon:\n",
      "    2_test_1482_days days: MAPE=32.20\n",
      "    2_test_480_days days: MAPE=32.48\n",
      "    2_test_600_days days: MAPE=32.20\n",
      "    2_test_720_days days: MAPE=32.20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# **Print Stored Models**\n",
    "for dataset_name, models in stored_models.items():\n",
    "    print(f\"\\n=== Best Models for Dataset {dataset_name} ===\")\n",
    "    for model_id, model_info in models.items():\n",
    "        print(f\"ID: {model_id}\")\n",
    "        print(f\"  Model: {model_info['model']}\")\n",
    "        print(f\"  Transforms: {model_info['transforms']}\")\n",
    "        print(f\"  Lags: {model_info['lags']}\")\n",
    "        # print(f\"  Lag Transforms: {model_info['lag_transforms']}\")\n",
    "        print(\"  MAPE per Horizon:\")\n",
    "        for horizon, mape in sorted(model_info[\"horizons\"].items()):\n",
    "            print(f\"    {horizon} days: MAPE={mape:.2f}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_by_id(sensor_id, model_id):\n",
    "    \"\"\"Retrieve model details using sensor ID and model hash ID.\"\"\"\n",
    "    return stored_models.get(sensor_id, {}).get(model_id, None)\n",
    "\n",
    "selected_model_ids = ['5308482a5d', '1b6481d847', '1a6b38364a', \n",
    "             '8ab48ac0fa', '8fcc2016f4', \n",
    "             'c54a4e855f', 'ed1d102b45', \n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recreating models for dataset: 2_12M_train\n",
      "\n",
      "Processing Model ID: 5308482a5d\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of values (8280) does not match length of index (2760)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 81\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# Store forecast results\u001b[39;00m\n\u001b[0;32m     80\u001b[0m test_df_copy \u001b[38;5;241m=\u001b[39m test_df\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m---> 81\u001b[0m test_df_copy[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforecast\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m predictions[model_name]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m     83\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m     84\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(test_df_copy[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mds\u001b[39m\u001b[38;5;124m'\u001b[39m], test_df_copy[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mActual\u001b[39m\u001b[38;5;124m'\u001b[39m, marker\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mo\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\PC314\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4311\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4308\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[0;32m   4309\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4310\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[1;32m-> 4311\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_item(key, value)\n",
      "File \u001b[1;32mc:\\Users\\PC314\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4524\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4514\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4515\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4516\u001b[0m \u001b[38;5;124;03m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[0;32m   4517\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4522\u001b[0m \u001b[38;5;124;03m    ensure homogeneity.\u001b[39;00m\n\u001b[0;32m   4523\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4524\u001b[0m     value, refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sanitize_column(value)\n\u001b[0;32m   4526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   4527\u001b[0m         key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m   4528\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   4529\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value\u001b[38;5;241m.\u001b[39mdtype, ExtensionDtype)\n\u001b[0;32m   4530\u001b[0m     ):\n\u001b[0;32m   4531\u001b[0m         \u001b[38;5;66;03m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[0;32m   4532\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[1;32mc:\\Users\\PC314\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:5266\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m   5263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _reindex_for_setitem(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m   5265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_list_like(value):\n\u001b[1;32m-> 5266\u001b[0m     com\u001b[38;5;241m.\u001b[39mrequire_length_match(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m   5267\u001b[0m arr \u001b[38;5;241m=\u001b[39m sanitize_array(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   5268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   5269\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(value, Index)\n\u001b[0;32m   5270\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5273\u001b[0m     \u001b[38;5;66;03m# TODO: Remove kludge in sanitize_array for string mode when enforcing\u001b[39;00m\n\u001b[0;32m   5274\u001b[0m     \u001b[38;5;66;03m# this deprecation\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\PC314\\anaconda3\\Lib\\site-packages\\pandas\\core\\common.py:573\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[1;34m(data, index)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    570\u001b[0m \u001b[38;5;124;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[1;32m--> 573\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    575\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    576\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match length of index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    577\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    578\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Length of values (8280) does not match length of index (2760)"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "selected_sensors_df['full_date'] = pd.to_datetime(selected_sensors_df['full_date']) \n",
    "def format_multi_df_to_mlforecast(df):\n",
    "    df_melted = df.melt(id_vars=['full_date'], var_name='unique_id', value_name='y')\n",
    "    return df_melted.rename(columns={'full_date': 'ds'})\n",
    "\n",
    "for dataset_name, models in stored_models.items():\n",
    "    print(f\"\\nRecreating models for dataset: {dataset_name}\")\n",
    "    \n",
    "    # Extract sensor_id and train_label from dataset_name\n",
    "    sensor_id_main, train_months, train_label = extract_train_info(dataset_name)\n",
    "\n",
    "    if sensor_id_main is None:\n",
    "        print(f\"Skipping dataset {dataset_name}: Cannot determine sensor ID or train length.\")\n",
    "        continue\n",
    "\n",
    "    for model_id, model_info in models.items():\n",
    "        if model_id not in selected_model_ids:\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nProcessing Model ID: {model_id}\")\n",
    "\n",
    "        # Extract model components\n",
    "        model_name = model_info[\"model\"]\n",
    "        transforms_str = model_info[\"transforms\"]\n",
    "        lags = model_info[\"lags\"]\n",
    "        lag_transforms_str = model_info[\"lag_transforms\"]\n",
    "\n",
    "        # Reconstruct transforms\n",
    "        transform_objects = []\n",
    "        if isinstance(transforms_str, str):\n",
    "            for transform_str in transforms_str.split(\" | \"):\n",
    "                class_name, params = parse_transform_string(transform_str)\n",
    "                \n",
    "                if class_name in transform_mapping:\n",
    "                    transform_objects.append(transform_mapping[class_name](**params) if params else transform_mapping[class_name]())\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown transform: {class_name}\")\n",
    "\n",
    "        # Reverse lag transforms\n",
    "        lag_transforms = parse_lag_transforms(lag_transforms_str)\n",
    "\n",
    "        # Prepare dataset\n",
    "        sensor_id = str(sensor_id_main)  # Convert to string for consistency\n",
    "        if sensor_id not in scenarios_sensors:\n",
    "            print(f\"Skipping sensor {sensor_id}: No scenario configuration found.\")\n",
    "            continue\n",
    "        \n",
    "        scenario = scenarios_sensors[sensor_id][train_label]\n",
    "\n",
    "        # Format dataset for MLForecast\n",
    "        formatted_df = format_multi_df_to_mlforecast(selected_sensors_df)\n",
    "        formatted_df = formatted_df[['ds', 'y', 'unique_id']]\n",
    "        \n",
    "        # Split data into train and test\n",
    "        train_df, test_df = split_data(formatted_df, scenario)\n",
    "\n",
    "        # Initialize MLForecast model\n",
    "        model_instance = model_mapping.get(model_name)\n",
    "        if model_instance is None:\n",
    "            raise ValueError(f\"Model {model_name} not found in model_mapping\")\n",
    "\n",
    "        fcst = MLForecast(\n",
    "            models=[model_instance],\n",
    "            freq='D',\n",
    "            lags=ast.literal_eval(lags),  # Convert string representation of list to actual list\n",
    "            target_transforms=transform_objects,\n",
    "            date_features=['dayofweek', 'month'],\n",
    "            num_threads=1,\n",
    "            lag_transforms=lag_transforms,\n",
    "        )\n",
    "\n",
    "        # Fit the model\n",
    "        fcst.fit(train_df)\n",
    "\n",
    "        # Generate forecasts\n",
    "        predictions = fcst.predict(h=test_df.shape[0])\n",
    "        \n",
    "        # Store forecast results\n",
    "        test_df_copy = test_df.copy()\n",
    "        test_df_copy['forecast'] = predictions[model_name].values\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(test_df_copy['ds'], test_df_copy['y'], label='Actual', marker='o')\n",
    "        plt.plot(test_df_copy['ds'], test_df_copy['forecast'], label='Predicted', linestyle='dashed', marker='x')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Value')\n",
    "        plt.title(f\"Actual vs. Predicted - Sensor {sensor_id} ({model_name})\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"Model {model_name} (ID: {model_id}) trained and predicted successfully for {dataset_name}.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
