{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "path_to_data = \"..//data//\"\n",
    "\n",
    "df = pd.read_csv(path_to_data+\"pm_data.csv\")\n",
    "sensors_df = pd.read_csv(path_to_data+\"sensors.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_percentage = df.isnull().mean() * 100\n",
    "\n",
    "threshold = 95  \n",
    "filtered_df = df.loc[:, nan_percentage <= threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sensors = filtered_df.columns[2:-1]\n",
    "filtered_sensors_df = sensors_df.loc[sensors_df['id'].astype('str').isin(filtered_sensors)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(path_to_data+\"raw_train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clustering by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_df['data_completeness'] = None\n",
    "\n",
    "for sens in sensors_df['id'].values:\n",
    "    if str(sens) in train_df.columns:\n",
    "        sensors_df.loc[sensors_df['id']==sens, \"data_completeness\"] = train_df[str(sens)].count() / train_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_0 = {\n",
    "    1: [1, 4372603],\n",
    "    2: [2],\n",
    "    5: [5], \n",
    "    6: [6],\n",
    "    # 12: [12],\n",
    "}\n",
    "\n",
    "# clusters_0 = {8: [8],} # trash\n",
    "# clusters_0 = {14: [14],} # kamenka\n",
    "# clusters_0 = {16: [16],} # tec3\n",
    "clusters_0 = {104: [104],} #  malo dannyh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class SensorClusterTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, clusters, date_column=\"full_date\"):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        - clusters: dict, mapping cluster IDs to lists of sensor IDs.\n",
    "        - date_column: str, name of the column containing date information.\n",
    "        \"\"\"\n",
    "        self.clusters = clusters\n",
    "        self.date_column = date_column\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        No fitting required for this transformer.\n",
    "        \"\"\"\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transforms the input DataFrame to compute mean time series for each cluster.\n",
    "\n",
    "        Parameters:\n",
    "        - X: pd.DataFrame, the input time series data with a date column and sensor time series.\n",
    "\n",
    "        Returns:\n",
    "        - pd.DataFrame, mean time series for each cluster.\n",
    "        \"\"\"\n",
    "        # Check if the date column exists\n",
    "        if self.date_column not in X.columns:\n",
    "            raise ValueError(f\"The specified date column '{self.date_column}' does not exist in the DataFrame.\")\n",
    "\n",
    "        # Ensure the date column is set as index temporarily for easier time series operations\n",
    "        X = X.set_index(self.date_column)\n",
    "\n",
    "        # Dictionary to hold mean time series for each cluster\n",
    "        cluster_means = {}\n",
    "\n",
    "        # Compute mean time series for each cluster\n",
    "        for cluster_id, sensor_ids in self.clusters.items():\n",
    "            # Check if all sensor IDs exist in the DataFrame\n",
    "            sensor_ids = [str(s) for s in sensor_ids]\n",
    "            missing_sensors = [sensor for sensor in sensor_ids if sensor not in X.columns]\n",
    "            if missing_sensors:\n",
    "                raise ValueError(f\"The following sensor IDs are missing from the DataFrame: {missing_sensors}\")\n",
    "\n",
    "            # Calculate mean time series for the cluster\n",
    "            cluster_means[cluster_id] = X[sensor_ids].mean(axis=1)\n",
    "\n",
    "        # Create a DataFrame for the cluster means\n",
    "        cluster_means_df = pd.DataFrame(cluster_means)\n",
    "\n",
    "        # Reset the index to include the date column in the result\n",
    "        cluster_means_df.reset_index(inplace=True)\n",
    "\n",
    "        return cluster_means_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = SensorClusterTransformer(clusters=clusters_0)\n",
    "cluster_means_df = transformer.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filling nan values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы будем заполнять только промежутки в каждом сенсоре"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_list = list(sensors_df['id'].values)\n",
    "sensors_list = [str(s) for s in sensors_list if str(s) in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def rolling_zscore_outlier_removal(series, window=7, z_threshold=3.0):\n",
    "    \"\"\"\n",
    "    Removes (or flags) outliers based on a rolling mean/std approach.\n",
    "    - series: pd.Series (time indexed)\n",
    "    - window: rolling window size\n",
    "    - z_threshold: threshold for z-score\n",
    "    Returns: A new Series with outliers replaced by NaN (or some other logic).\n",
    "    \"\"\"\n",
    "    rolling_mean = series.rolling(window=window, center=True, min_periods=1).mean()\n",
    "    rolling_std = series.rolling(window=window, center=True, min_periods=1).std()\n",
    "    \n",
    "    z_scores = (series - rolling_mean) / rolling_std\n",
    "    outliers = z_scores.abs() > z_threshold\n",
    "    \n",
    "    # Option 1: Replace outliers with NaN\n",
    "    cleaned_series = series.mask(outliers, np.nan)\n",
    "    \n",
    "    return cleaned_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_series_into_chunks(series):\n",
    "    is_nan = series.isna()\n",
    "    chunks = []\n",
    "    start_idx = None\n",
    "\n",
    "    for i, val in enumerate(is_nan):\n",
    "        if not val and start_idx is None:\n",
    "            start_idx = i  # Start a new chunk\n",
    "        elif val and start_idx is not None:\n",
    "            chunks.append(series[start_idx:i])  # Append the chunk\n",
    "            start_idx = None\n",
    "\n",
    "    # Add the last chunk if the series ends without NaNs\n",
    "    if start_idx is not None:\n",
    "        chunks.append(series[start_idx:])\n",
    "\n",
    "    return chunks\n",
    "\n",
    "from sktime.forecasting.arima import AutoARIMA\n",
    "from sktime.forecasting.base import ForecastingHorizon\n",
    "from sktime.forecasting.exp_smoothing import ExponentialSmoothing\n",
    "from sktime.forecasting.ets import AutoETS  \n",
    "\n",
    "def fill_nans_with_arima(series, sp=12):\n",
    "    # Get the chunks of non-NaN data\n",
    "    chunks = divide_series_into_chunks(series)\n",
    "    filled_series = series.copy()\n",
    "\n",
    "    for i in range(len(chunks) - 1):\n",
    "        current_chunk = chunks[i]\n",
    "        next_chunk_start = chunks[i + 1].index[0]\n",
    "\n",
    "        # Ensure datetime types\n",
    "        last_index = current_chunk.index[-1]\n",
    "        next_start = next_chunk_start\n",
    "\n",
    "        # Define training and forecasting horizons\n",
    "        y_train = current_chunk\n",
    "        print(y_train)\n",
    "        \n",
    "        fh = np.arange(last_index+1, next_start)\n",
    "\n",
    "        # Fit ARIMA and predict the gap\n",
    "        forecaster = ExponentialSmoothing(trend=\"add\", seasonal=\"additive\", sp=6)\n",
    "        forecaster.fit(y_train)\n",
    "        y_pred = forecaster.predict(fh)\n",
    "\n",
    "        # Fill the gap in the series\n",
    "        filled_series.values[last_index+1:next_start] = y_pred\n",
    "\n",
    "    return filled_series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_with_same_day_values(df, date_col, value_col, direction=\"both\"):\n",
    "    \"\"\"\n",
    "    Fills missing values in a time series by using values from the same day in the next or previous year.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The dataframe containing the time series.\n",
    "        date_col (str): Column name representing the date.\n",
    "        value_col (str): Column name representing the values with potential missing data.\n",
    "        direction (str): Direction to search for values (\"next\", \"previous\", or \"both\"). Default is \"both\".\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: The dataframe with missing values filled.\n",
    "    \"\"\"\n",
    "    # Ensure the date column is in datetime format\n",
    "    df[date_col] = pd.to_datetime(df[date_col])\n",
    "\n",
    "    # Create an index for fast lookup\n",
    "    df.set_index(date_col, inplace=True)\n",
    "\n",
    "    # Fill missing values\n",
    "    for idx in df[df[value_col].isna()].index:\n",
    "        year = idx.year\n",
    "        day_of_year = idx.timetuple().tm_yday\n",
    "\n",
    "        # Look for the value in the next year\n",
    "        next_year_date = pd.Timestamp(year + 1, 1, 1) + pd.Timedelta(days=day_of_year - 1)\n",
    "        previous_year_date = pd.Timestamp(year - 1, 1, 1) + pd.Timedelta(days=day_of_year - 1)\n",
    "\n",
    "        value_to_fill = None\n",
    "        if direction in [\"next\", \"both\"] and next_year_date in df.index:\n",
    "            value_to_fill = df.at[next_year_date, value_col]\n",
    "        \n",
    "        if direction in [\"previous\", \"both\"] and previous_year_date in df.index and pd.isna(value_to_fill):\n",
    "            value_to_fill = df.at[previous_year_date, value_col]\n",
    "\n",
    "        # Assign the value if found\n",
    "        if not pd.isna(value_to_fill):\n",
    "            df.at[idx, value_col] = value_to_fill\n",
    "\n",
    "    # Reset the index\n",
    "    df.reset_index(inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# full pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Clean all sensors\n",
    "def clean_sensors(dataframe, sensors_list, window=15, z_threshold=3):\n",
    "    cleaned_data = dataframe.copy()\n",
    "    for sensor in sensors_list:\n",
    "        cleaned_data[sensor] = rolling_zscore_outlier_removal(dataframe[sensor], window=window, z_threshold=z_threshold)\n",
    "    \n",
    "    return cleaned_data\n",
    "\n",
    "# Step 2: Apply sensor cluster transformer\n",
    "def transform_clusters(cleaned_df, clusters):\n",
    "    transformer = SensorClusterTransformer(clusters=clusters)\n",
    "    return transformer.transform(cleaned_df)\n",
    "\n",
    "# Step 3: Fill missing values for all columns in cluster means\n",
    "def fill_missing_for_all_columns(cluster_means_df, date_col, direction=\"both\"):\n",
    "    filled_df = cluster_means_df.copy()\n",
    "    for column in cluster_means_df.columns:\n",
    "        if column != date_col:\n",
    "            filled_df = fill_missing_with_same_day_values(\n",
    "                filled_df, date_col=date_col, value_col=column, direction=direction\n",
    "            )\n",
    "    return filled_df\n",
    "\n",
    "def process_time_series(dataframe, sensors_list, clusters, date_col, window=15, z_threshold=3, direction=\"both\"):\n",
    "    # Step 1: Clean all sensors\n",
    "    cleaned_df = clean_sensors(dataframe, sensors_list, window=window, z_threshold=z_threshold)\n",
    "    \n",
    "    # Step 2: Calculate cluster means\n",
    "    cluster_means_df = transform_clusters(cleaned_df, clusters)\n",
    "    \n",
    "    # Step 3: Fill missing values\n",
    "    cluster_means_df_filled = fill_missing_for_all_columns(cluster_means_df, date_col=date_col, direction=direction)\n",
    "    \n",
    "    return cluster_means_df_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_missing_dates(df, date_col=\"date_time\"):\n",
    "    df = df.set_index(date_col)\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    full_range = pd.date_range(start=df.index.min(), end=df.index.max(), freq='D')\n",
    "    df = df.reindex(full_range)\n",
    "    df.index.name = date_col\n",
    "\n",
    "    return df \n",
    "\n",
    "df_dates_filled = input_missing_dates(df, \"full_date\").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df = process_time_series(\n",
    "    dataframe=df_dates_filled,\n",
    "    sensors_list=sensors_list,\n",
    "    clusters=clusters_0,\n",
    "    date_col=\"full_date\",\n",
    "    window=60,\n",
    "    z_threshold=2,\n",
    "    # direction=\"both\"\n",
    "    direction=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_date_to_distr(df, date_col='full_date', value_col=8):\n",
    "    filtered_df = df.dropna(subset=[value_col])\n",
    "\n",
    "    # Get min and max dates\n",
    "    min_date = filtered_df[date_col].min()\n",
    "    max_date = filtered_df[date_col].max()\n",
    "\n",
    "    return df.loc[(df[date_col]>=min_date) & (df[date_col]<=max_date)]\n",
    "\n",
    "def cut_slack_dates(df, start_date, end_date, date_col='full_date'):\n",
    "    return df.loc[(df[date_col]>=start_date) & (df[date_col]<=end_date)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df = cut_date_to_distr(processed_df, value_col=104)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 892 entries, 0 to 891\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype         \n",
      "---  ------     --------------  -----         \n",
      " 0   full_date  892 non-null    datetime64[ns]\n",
      " 1   104        892 non-null    float64       \n",
      "dtypes: datetime64[ns](1), float64(1)\n",
      "memory usage: 14.1 KB\n"
     ]
    }
   ],
   "source": [
    "processed_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_df = cut_slack_dates(processed_df, '2017-07-20', '2019-09-15')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import holt_winters_imputation_and_expand\n",
    "\n",
    "# processed_df[8] = holt_winters_imputation_and_expand(processed_df[2], 365)\n",
    "# processed_df[5] = holt_winters_imputation_and_expand(processed_df[5], 365)\n",
    "\n",
    "processed_df = fill_missing_for_all_columns(processed_df, 'full_date')\n",
    "processed_df[104] = holt_winters_imputation_and_expand(processed_df[104], 365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in processed_df.columns:\n",
    "    processed_df.rename(columns={col: str(col)}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df.to_csv(\"../data/decomp/kusok_5.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
