{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-01 17:24:49,190\tINFO util.py:154 -- Outdated packages:\n",
      "  ipywidgets==7.8.1 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2025-04-01 17:24:49,395\tINFO util.py:154 -- Outdated packages:\n",
      "  ipywidgets==7.8.1 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from MLForecastPipeline import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_NUM = 'run_12'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_sensors_df = pd.read_csv(\"../data/selected_sensors2_cleaned.csv\", index_col=0)\n",
    "\n",
    "scenarios_sensors = {\n",
    "    # 0: 1, 4372603\n",
    "    # \"0_12M_train_7M_test\": {\"train_start\": \"2017-03-25\", \"train_end\": \"2018-03-25\", \"test_start\": \"2018-03-26\", \"test_end\": \"2018-10-10\"},\n",
    "    '2': {\n",
    "        \"26M_train\":  {\"train_start\": \"2017-04-01\", \"train_end\": \"2019-06-01\"},\n",
    "        \"24M_train\":  {\"train_start\": \"2017-04-01\", \"train_end\": \"2019-04-01\"},\n",
    "        \"22M_train\":  {\"train_start\": \"2017-04-01\", \"train_end\": \"2019-02-01\"},\n",
    "        \"20M_train\":  {\"train_start\": \"2017-04-01\", \"train_end\": \"2018-12-01\"},\n",
    "        \"18M_train\":  {\"train_start\": \"2017-04-01\", \"train_end\": \"2018-10-01\"},\n",
    "        \"12M_train\":  {\"train_start\": \"2017-04-01\", \"train_end\": \"2018-04-01\"},\n",
    "        \"10M_train\":  {\"train_start\": \"2017-04-01\", \"train_end\": \"2018-01-25\"},\n",
    "        \"8M_train\":   {\"train_start\": \"2017-04-01\", \"train_end\": \"2017-10-25\"},\n",
    "        \n",
    "        # Non-Heating Periods\n",
    "        \"NH_3M_train\":  {\"train_start\": \"2017-04-15\", \"train_end\": \"2017-07-15\"},\n",
    "        \"NH_4M_train\":  {\"train_start\": \"2017-04-15\", \"train_end\": \"2017-08-15\"},\n",
    "        \"NH_2M_train\":  {\"train_start\": \"2017-04-15\", \"train_end\": \"2017-06-15\"},\n",
    "        \"NH_1M_train\":  {\"train_start\": \"2017-04-15\", \"train_end\": \"2017-05-15\"},\n",
    "        \"NH_15D_train\": {\"train_start\": \"2017-04-15\", \"train_end\": \"2017-04-30\"},\n",
    "        \"NH_feb_2M_train\": {\"train_start\": \"2017-02-15\", \"train_end\": \"2017-04-15\"},\n",
    "        \"NH_feb_1M_train\": {\"train_start\": \"2017-02-15\", \"train_end\": \"2017-04-15\"},\n",
    "        \"NH_mar_2M_train\": {\"train_start\": \"2017-03-15\", \"train_end\": \"2017-05-15\"},\n",
    "        \"NH_mar_1M_train\": {\"train_start\": \"2017-03-15\", \"train_end\": \"2017-04-15\"},\n",
    "\n",
    "        # Heating Periods\n",
    "        \"H_5M_train\":     {\"train_start\": \"2017-06-01\", \"train_end\": \"2017-11-01\"},\n",
    "        \"H_3M_jul_train\": {\"train_start\": \"2017-07-01\", \"train_end\": \"2017-10-10\"},\n",
    "        \"H_3M_sep_train\": {\"train_start\": \"2017-09-01\", \"train_end\": \"2017-12-10\"},\n",
    "        \"H_3M_nov_train\": {\"train_start\": \"2017-11-01\", \"train_end\": \"2018-02-10\"},\n",
    "        },\n",
    "}\n",
    "scenarios_sensors['5'] = scenarios_sensors['2'].copy()\n",
    "scenarios_sensors['6'] = scenarios_sensors['2'].copy()\n",
    "\n",
    "def split_data(df, scenario, date_col=\"ds\"):\n",
    "    \"\"\"Extracts train and test data based on train end date.\"\"\"\n",
    "    train_data = df[df[date_col] <= scenario['train_end']]\n",
    "    test_start = pd.to_datetime(scenario['train_end']) + pd.Timedelta(days=1)\n",
    "    test_data = df[df[date_col] >= test_start]\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "LAG_TRANSFORMS_MAP = {\n",
    "    \"expanding_mean_rolling_14_rolling_30\": {1: 'expanding_mean', 7: 'expanding_mean', 30: 'rolling_mean_30'},\n",
    "    \"expanding_mean_rolling_14\": {1: 'expanding_mean', 7: 'rolling_mean_14', 30: 'expanding_mean'},\n",
    "    \"rolling_14_rolling_30_expanding\": {1: 'rolling_mean_14', 7: 'rolling_mean_30', 30: 'expanding_mean'},\n",
    "    \"rolling_14_expanding\": {1: 'rolling_mean_14', 30: 'expanding_mean'},\n",
    "    \"rolling_14_only\": {1: 'rolling_mean_14'},\n",
    "    \"no_transform\": {},\n",
    "}\n",
    "\n",
    "def map_lag_transforms(lag_transform_dict, lag_transforms_map=LAG_TRANSFORMS_MAP):\n",
    "    for name, transform in lag_transforms_map.items():\n",
    "        if lag_transform_dict == transform:\n",
    "            return name\n",
    "    return \"unknown\"\n",
    "\n",
    "def analyze_results(df, lag_transforms_map=LAG_TRANSFORMS_MAP, mape_threshold=40, model_filter=None):\n",
    "    df = df.copy()\n",
    "    df['Lag Transform Name'] = df['Lag Transforms'].apply(lambda x: map_lag_transforms(x, lag_transforms_map))\n",
    "    df['Lag_Set_Name'] = df['Lag Name']\n",
    "    # Identify MAPE columns dynamically\n",
    "    mape_columns = [col for col in df.columns if col.startswith(\"test_\") and col.endswith(\"_days\")]\n",
    "    \n",
    "    # Compute mean MAPE across all test periods\n",
    "    df['MAPE'] = df[mape_columns].mean(axis=1)\n",
    "    \n",
    "    # Apply filtering\n",
    "    top_df = df[df[\"MAPE\"] < mape_threshold].copy()\n",
    "    if model_filter:\n",
    "        top_df = top_df[top_df['Model'] == model_filter].copy()\n",
    "    \n",
    "    # Compute groupings\n",
    "    top_models = top_df.groupby(\"Model\")[\"MAPE\"].mean().sort_values().reset_index()\n",
    "    top_transforms = top_df.groupby(\"Transforms\")[\"MAPE\"].mean().sort_values().reset_index()\n",
    "    top_lag_transforms = top_df.groupby(\"Lag Transform Name\")[\"MAPE\"].mean().sort_values().reset_index()\n",
    "    top_lags = top_df.groupby(\"Lag_Set_Name\")[\"MAPE\"].mean().sort_values().reset_index()\n",
    "    \n",
    "    # Compute MAPE trends over different forecasting horizons\n",
    "    mape_trends = top_df.groupby(\"Model\")[mape_columns].mean().reset_index()\n",
    "    \n",
    "    return top_models, top_transforms, top_lag_transforms, top_lags, mape_trends\n",
    "\n",
    "def plot_results(top_models, top_transforms, top_lag_transforms, top_lags, mape_trends):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    sns.barplot(x=top_models[\"Model\"], y=top_models[\"MAPE\"], palette=\"viridis\", hue=top_models[\"Model\"])\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title(\"Average MAPE per Model\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    sns.barplot(x=top_transforms[\"Transforms\"], y=top_transforms[\"MAPE\"], palette=\"coolwarm\", hue=top_transforms[\"Transforms\"])\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(\"Average MAPE per Transform\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    sns.barplot(x=top_lag_transforms[\"Lag Transform Name\"], y=top_lag_transforms[\"MAPE\"], palette=\"Blues\", hue=top_lag_transforms[\"Lag Transform Name\"])\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(\"Average MAPE per Lag Transform\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    sns.barplot(x=top_lags[\"Lag_Set_Name\"], y=top_lags[\"MAPE\"], palette=\"Blues\", hue=top_lags[\"Lag_Set_Name\"])\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(\"MAPE vs Number of Lags\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot MAPE trends across different forecasting horizons\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    for model in mape_trends[\"Model\"]:\n",
    "        plt.plot(mape_trends.columns[1:], mape_trends[mape_trends[\"Model\"] == model].values[0][1:], label=model)\n",
    "    plt.xlabel(\"Forecasting Horizon (Days)\")\n",
    "    plt.ylabel(\"MAPE\")\n",
    "    plt.title(\"MAPE Trends Across Forecast Horizons\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# top_models, top_transforms, top_lag_transforms, top_lags, mape_trends = analyze_results(df, lag_transforms_map, optimal_lags_map)\n",
    "# plot_results(top_models, top_transforms, top_lag_transforms, top_lags, mape_trends)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import glob\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# # Load all results\n",
    "# results = {}\n",
    "# for file in glob.glob(\"results/run_3/*.csv\"):\n",
    "#     dataset_name = file.split(\"/\")[-1].replace(\".csv\", \"\")\n",
    "#     results[dataset_name] = pd.read_csv(file)\n",
    "\n",
    "# # Combine all datasets into a single DataFrame\n",
    "# df = pd.concat([df.assign(Dataset=name) for name, df in results.items()], ignore_index=True)\n",
    "\n",
    "# # Define threshold for MAPE\n",
    "# mape_threshold = 35  \n",
    "\n",
    "# # Identify best models per test period\n",
    "# best_models_per_period = {}\n",
    "# for test_n in [30, 60, 90, 120, 150, 180, 240, 300, 360, 480, 600, 720, 737]:\n",
    "#     col = f\"test_{test_n}_days\"\n",
    "#     best_models_per_period[test_n] = df[df[col] < mape_threshold].nsmallest(1, col)\n",
    "\n",
    "# # Calculate variance across all test columns\n",
    "# test_cols = [col for col in df.columns if \"test_\" in col]\n",
    "# df[\"stability\"] = df[test_cols].std(axis=1)\n",
    "\n",
    "# # Select most stable model\n",
    "# most_stable_model = df.nsmallest(1, \"stability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "results = {}\n",
    "for file in glob.glob(f\"results/{RUN_NUM}/*.csv\"):\n",
    "    dataset_name = file.split(\"\\\\\")[-1].replace(\".csv\", \"\")\n",
    "    try:\n",
    "        results[dataset_name] = pd.read_csv(file)\n",
    "    except Exception as e:\n",
    "        print(e)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6_NH_15D_train → (6, 0.5, 'NH_15D_train')\n",
      "6_H_5M_train → (6, 5, 'H_5M_train')\n",
      "2_18M_train → (2, 18, '18M_train')\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_train_info(dataset_name):\n",
    "    \"\"\"\n",
    "    Extracts sensor ID, training length in months, and a standardized train label.\n",
    "    \n",
    "    Example Inputs:\n",
    "        - \"6_NH_15D_train\"  → (6, 0.5, \"NH_15D_train\")\n",
    "        - \"6_H_5M_train\"    → (6, 5, \"H_5M_train\")\n",
    "        - \"2_18M_train\"     → (2, 18, \"18M_train\")\n",
    "    \n",
    "    Returns:\n",
    "        - sensor (int): Sensor ID\n",
    "        - train_months (float): Training length in months\n",
    "        - train_label (str): Everything after the sensor ID (used for finding comparable datasets)\n",
    "    \"\"\"\n",
    "    name_parts = dataset_name.split('_')\n",
    "\n",
    "    # Extract sensor ID\n",
    "    sensor = int(name_parts[0])  # First part is always the sensor number\n",
    "\n",
    "    # Reconstruct the label for easy dataset comparison\n",
    "    train_label = '_'.join(name_parts[1:])\n",
    "\n",
    "    # Extract training length (2nd to last part contains number + unit)\n",
    "    train_info = name_parts[-2]  # Example: \"15D\" or \"5M\"\n",
    "    match = re.match(r\"(\\d+)([MD])\", train_info)  # Extract number and unit\n",
    "\n",
    "    if match:\n",
    "        train_length = int(match.group(1))\n",
    "        unit = match.group(2)\n",
    "\n",
    "        # Convert days to months (approximate)\n",
    "        train_months = train_length / 30 if unit == \"D\" else train_length\n",
    "    else:\n",
    "        return None, None, None  # Invalid format, return None values\n",
    "\n",
    "    return sensor, train_months, train_label\n",
    "\n",
    "# Example Usage\n",
    "datasets = [\"6_NH_15D_train\", \"6_H_5M_train\", \"2_18M_train\"]\n",
    "for ds in datasets:\n",
    "    print(f\"{ds} → {extract_train_info(ds)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String Input: {1: [<function expanding_mean at 0x0000021CAC0F2840>], 7: [<function rolling_mean_14 at 0x0000021CA47C3C40>], 30: [<function expanding_mean at 0x0000021CAC0F2840>]}\n",
      "Parsed Back: {1: [<function expanding_mean at 0x00000261B1DF80E0>], 7: [<function rolling_mean_14 at 0x00000261C2554A40>], 30: [<function expanding_mean at 0x00000261B1DF80E0>]}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numba import njit\n",
    "from window_ops.expanding import expanding_mean\n",
    "from window_ops.rolling import rolling_mean\n",
    "\n",
    "@njit\n",
    "def rolling_mean_14(x):\n",
    "    return rolling_mean(x, window_size=14)\n",
    "\n",
    "@njit\n",
    "def rolling_mean_30(x):\n",
    "    return rolling_mean(x, window_size=30)\n",
    "\n",
    "# Define function mapping\n",
    "lag_transforms_mapping = {\n",
    "    'expanding_mean': expanding_mean,\n",
    "    'rolling_mean_14': rolling_mean_14,\n",
    "    'rolling_mean_30': rolling_mean_30,\n",
    "}\n",
    "\n",
    "# Function to convert lag_transforms dictionary to a string\n",
    "def stringify_lag_transforms(lag_transforms):\n",
    "    \"\"\"Converts lag_transforms dictionary to a clean string format for storage.\"\"\"\n",
    "    return str({key: [func.__name__ for func in funcs] for key, funcs in lag_transforms.items()})\n",
    "\n",
    "# Function to parse lag_transforms safely from an invalid dictionary-like string\n",
    "def parse_lag_transforms(lag_transforms_str):\n",
    "    \"\"\"Parses a raw function dictionary string and converts it back to a proper dictionary with function references.\"\"\"\n",
    "    try:\n",
    "        # Extract function names using regex pattern: `<function function_name at 0x...>`\n",
    "        cleaned_str = re.sub(r'<function (\\w+) at 0x[0-9A-Fa-f]+>', r'\"\\1\"', lag_transforms_str)\n",
    "\n",
    "        # Convert cleaned string into a valid Python dictionary\n",
    "        temp_dict = eval(cleaned_str)  # Evaluates after function names are fixed\n",
    "\n",
    "        # Map function names back to actual function references\n",
    "        return {key: [lag_transforms_mapping[func_name] for func_name in funcs] for key, funcs in temp_dict.items()}\n",
    "    \n",
    "    except Exception as e:\n",
    "        # print(f\"Error parsing lag_transforms {lag_transforms_str}: {e}\")\n",
    "        return 1\n",
    "    \n",
    "from mlforecast.lag_transforms import *\n",
    "\n",
    "lag_transform_mapping = {\n",
    "    'RollingMean': RollingMean,\n",
    "    'ExpandingMean': ExpandingMean,\n",
    "    'ExponentiallyWeightedMean': ExponentiallyWeightedMean,\n",
    "}\n",
    "def create_lag_transform_object(transform_str):\n",
    "    \"\"\"Creates a lag transform object from a string representation.\"\"\"\n",
    "    class_name = transform_str.split('(')[0].strip()\n",
    "    params_str = transform_str[transform_str.find('(') + 1:transform_str.find(')')] if '(' in transform_str else None\n",
    "    params = {}\n",
    "    if params_str:\n",
    "        for param in params_str.split(','):\n",
    "            key, value = param.split('=')\n",
    "            params[key.strip()] = int(value.strip())  # Assuming numerical parameters\n",
    "\n",
    "    if class_name in lag_transform_mapping:\n",
    "        return lag_transform_mapping[class_name](**params) if params else lag_transform_mapping[class_name]()\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown lag transform: {class_name}\")\n",
    "\n",
    "from collections import defaultdict\n",
    "def parse_lag_transforms_mlfcst(initial_lag_transforms_str):\n",
    "    \"\"\"Parses the string representation of lag transforms into a dictionary.\"\"\"\n",
    "    lag_transforms = defaultdict(list)  # Use defaultdict for easier handling\n",
    "    if isinstance(initial_lag_transforms_str, str):\n",
    "        try:\n",
    "            # Split the string into individual lag and transform groups\n",
    "            transform_groups = initial_lag_transforms_str.strip(\"{}\").split(\", \")\n",
    "\n",
    "            for group in transform_groups:\n",
    "                lag_str, transform_strs_list = group.split(\": \")\n",
    "                lag = int(lag_str)\n",
    "\n",
    "                # Remove brackets and split the transform strings\n",
    "                transform_strs = transform_strs_list.strip(\"[]\").split(\", \")\n",
    "\n",
    "                for transform_str in transform_strs:\n",
    "                    # Clean the transform string (remove quotes and any extra spaces)\n",
    "                    clean_transform_str = transform_str.strip().strip(\"'\")\n",
    "\n",
    "                    # Create transform object and append to the list\n",
    "                    transform_object = create_lag_transform_object(clean_transform_str)\n",
    "                    lag_transforms[lag].append(transform_object)\n",
    "\n",
    "        except ValueError as e:\n",
    "            print(f\"Error parsing lag transforms string: {e}\")\n",
    "            return {}  # Or raise the exception\n",
    "\n",
    "    return dict(lag_transforms)\n",
    "\n",
    "def get_lag_transforms(lag_transform_str):\n",
    "    lag_transforms = parse_lag_transforms(lag_transform_str)\n",
    "    if lag_transforms == 1:\n",
    "        return parse_lag_transforms_mlfcst(lag_transform_str)\n",
    "    return lag_transforms\n",
    "\n",
    "# Example input with incorrect function references\n",
    "initial_lag_transforms_str = \"{1: [<function expanding_mean at 0x0000021CAC0F2840>], 7: [<function rolling_mean_14 at 0x0000021CA47C3C40>], 30: [<function expanding_mean at 0x0000021CAC0F2840>]}\"\n",
    "\n",
    "# Convert to proper lag_transforms dictionary\n",
    "parsed_lag_transforms = get_lag_transforms(initial_lag_transforms_str)\n",
    "\n",
    "print(\"String Input:\", initial_lag_transforms_str)\n",
    "print(\"Parsed Back:\", parsed_lag_transforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{7: [RollingMean(window_size=7)],\n",
       " 30: [RollingMean(window_size=30)],\n",
       " 60: [RollingMean(window_size=60)]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_lag_transforms_str = \"{7: [RollingMean(window_size=7)], 30: [RollingMean(window_size=30)], 60: [RollingMean(window_size=60)]}\"\n",
    "get_lag_transforms(initial_lag_transforms_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from numba import njit\n",
    "from window_ops.expanding import expanding_mean\n",
    "from window_ops.rolling import rolling_mean\n",
    "from mlforecast import MLForecast\n",
    "from lightgbm import LGBMRegressor\n",
    "# from my_custom_utils import format_df_to_mlforecast, split_data  # Assuming these are in a module\n",
    "import ast\n",
    "\n",
    "# Define parameters\n",
    "\n",
    "\n",
    "# Define available lag transforms\n",
    "lag_transforms_options = [\n",
    "    {1: [expanding_mean], 7: [rolling_mean], 30: [expanding_mean]},\n",
    "    {1: [rolling_mean], 7: [rolling_mean], 30: [expanding_mean]},\n",
    "]\n",
    "\n",
    "model_mapping = {\n",
    "    \"XGBRegressor\": XGBRegressor(),\n",
    "    \"SGDRegressor_42\": SGDRegressor(random_state=42),\n",
    "    \"Ridge\": Ridge(),\n",
    "    \"Lasso\": Lasso(),\n",
    "    \"SGD_EarlyStopping\": SGDRegressor( random_state=42, early_stopping=True, validation_fraction=0.1, n_iter_no_change=7, tol=1e-4 ),\n",
    "    \"SGD_ElasticNet\": SGDRegressor( penalty='elasticnet', l1_ratio=0.5, alpha=0.001, random_state=42 ),\n",
    "    \"SGD_Lasso\": SGDRegressor( penalty='l1', alpha=1, random_state=42 ),\n",
    "    \"SGD_Ridge\": SGDRegressor( penalty='l2', alpha=1, random_state=42 ),\n",
    "    \"SGDRegressor\": SGDRegressor(random_state=42),\n",
    "    \"XGBRegressor\": XGBRegressor(),\n",
    "    \"LGBMRegressor\": LGBMRegressor(),\n",
    "}\n",
    "\n",
    "transform_mapping = {\n",
    "    \"AutoDifferences\" : AutoDifferences, \n",
    "    \"AutoSeasonalDifferences\" : AutoSeasonalDifferences, \n",
    "    \"AutoSeasonalityAndDifferences\" : AutoSeasonalityAndDifferences,\n",
    "    \"LocalStandardScaler\" : LocalStandardScaler, \n",
    "    \"LocalMinMaxScaler\" : LocalMinMaxScaler, \n",
    "    \"LocalBoxCox\" : LocalBoxCox\n",
    "}\n",
    "\n",
    "@njit\n",
    "def rolling_mean_14(x):\n",
    "    return rolling_mean(x, window_size=14)\n",
    "\n",
    "@njit\n",
    "def rolling_mean_30(x):\n",
    "    return rolling_mean(x, window_size=30)\n",
    "\n",
    "# Function to extract sensor ID and train length\n",
    "def extract_train_info(dataset_name):\n",
    "    name_parts = dataset_name.split('_')\n",
    "    sensor = int(name_parts[0])  \n",
    "    train_label = '_'.join(name_parts[1:])\n",
    "\n",
    "    train_info = name_parts[-2]  \n",
    "    match = re.match(r\"(\\d+)([MD])\", train_info)  \n",
    "\n",
    "    if match:\n",
    "        train_length = int(match.group(1))\n",
    "        unit = match.group(2)\n",
    "        train_months = train_length / 30 if unit == \"D\" else train_length\n",
    "    else:\n",
    "        return None, None, None  \n",
    "\n",
    "    return sensor, train_months, train_label\n",
    "\n",
    "# Function to reverse `stringify_transform`\n",
    "def parse_transform_string(transform_str):\n",
    "    transform_str = transform_str.strip()\n",
    "    if \"(\" in transform_str:\n",
    "        class_name, params = transform_str.split(\"(\", 1)\n",
    "        params = params.rstrip(\")\")\n",
    "        param_dict = {}\n",
    "        if params != \"NoParams\" and class_name != \"LocalBoxCox\":\n",
    "            for param in params.split(\", \"):\n",
    "                key, value = param.split(\"=\")\n",
    "                if key in (\"max_diffs\", \"season_length\", \"max_season_length\"):\n",
    "                    param_dict[key] = int(value) if value.replace(\".\", \"\").isdigit() else value\n",
    "        return class_name, param_dict\n",
    "    return transform_str, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define patterns to match and their correct format\n",
    "rename_patterns = {\n",
    "    \"H_3M_jul_train\": \"H_jul_3M_train\",\n",
    "    \"H_3M_sep_train\": \"H_sep_3M_train\",\n",
    "    \"H_3M_nov_train\": \"H_nov_3M_train\",\n",
    "}\n",
    "\n",
    "# Create a new dictionary with updated keys\n",
    "updated_results = {}\n",
    "\n",
    "for dataset_name, df in results.items():\n",
    "    new_name = dataset_name  # Default: keep the same name\n",
    "\n",
    "    for old_pattern, new_pattern in rename_patterns.items():\n",
    "        if old_pattern in dataset_name:\n",
    "            new_name = dataset_name.replace(old_pattern, new_pattern)\n",
    "            break  # Stop checking once renamed\n",
    "\n",
    "    updated_results[new_name] = df  # Preserve the dataset content\n",
    "\n",
    "results = updated_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "def calculate_monthly_mape(predictions_df, date_col='ds', actual_col='y', forecast_col='forecast', id_col='unique_id'):\n",
    "    \"\"\"\n",
    "    Calculate MAPE (Mean Absolute Percentage Error) for each month-year in the predictions dataframe.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with keys as 'month_year' (e.g., 'Jan_2019') and values as MAPE for that month\n",
    "    dict\n",
    "        Dictionary with keys as 'month_year' and values as sample counts for that month\n",
    "    pandas.DataFrame\n",
    "        DataFrame with detailed MAPE information by month-year and entity\n",
    "    dict\n",
    "        Dictionary with keys as (year, month) tuples and values as MAPE for that year and month\n",
    "    \"\"\"\n",
    "    # Ensure the date column is datetime\n",
    "    df = predictions_df.copy()\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df[date_col]):\n",
    "        df[date_col] = pd.to_datetime(df[date_col])\n",
    "    \n",
    "    # Create month-year column for string representation\n",
    "    df['month_year'] = df[date_col].dt.strftime('%b_%Y')\n",
    "    \n",
    "    # Create separate year and month columns\n",
    "    df['year'] = df[date_col].dt.year\n",
    "    df['month'] = df[date_col].dt.month  # Numeric month (1-12)\n",
    "    df['month_name'] = df[date_col].dt.strftime('%b')  # Month abbreviation\n",
    "    \n",
    "    # Calculate absolute percentage error for each row\n",
    "    df['abs_pct_error'] = np.abs((df[actual_col] - df[forecast_col]) / df[actual_col])\n",
    "    \n",
    "    # Replace inf and NaN values (occurs when actual value is 0)\n",
    "    df['abs_pct_error'] = df['abs_pct_error'].replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    # Initialize results dictionaries\n",
    "    mape_by_month_year = {}  # String format: 'Jan_2019'\n",
    "    mape_by_year_month = {}  # Tuple format: (2019, 1)\n",
    "    sample_counts = {}\n",
    "    \n",
    "    # Calculate MAPE for each month-year\n",
    "    for month_year, group in df.groupby('month_year'):\n",
    "        mape = group['abs_pct_error'].mean() * 100  # Convert to percentage\n",
    "        mape_by_month_year[month_year] = mape if not pd.isna(mape) else None\n",
    "        sample_counts[month_year] = len(group)\n",
    "    \n",
    "    # Calculate MAPE for each year, month tuple\n",
    "    for (year, month), group in df.groupby(['year', 'month']):\n",
    "        mape = group['abs_pct_error'].mean() * 100  # Convert to percentage\n",
    "        mape_by_year_month[(year, month)] = mape if not pd.isna(mape) else None\n",
    "    \n",
    "    # Also calculate MAPE by month-year-entity for detailed analysis\n",
    "    detailed_results = []\n",
    "    for (year, month, entity_id), group in df.groupby(['year', 'month', id_col]):\n",
    "        month_name = group['month_name'].iloc[0]\n",
    "        month_year = group['month_year'].iloc[0]\n",
    "        mape = group['abs_pct_error'].mean() * 100  # Convert to percentage\n",
    "        detailed_results.append({\n",
    "            'year': year,\n",
    "            'month': month,\n",
    "            'month_name': month_name,\n",
    "            'month_year': month_year,\n",
    "            'entity_id': entity_id,\n",
    "            'mape': mape if not pd.isna(mape) else None,\n",
    "            'sample_count': len(group)\n",
    "        })\n",
    "    \n",
    "    detailed_df = pd.DataFrame(detailed_results)\n",
    "    \n",
    "    # Sort the dictionaries by date for easier interpretation\n",
    "    sorted_month_years = sorted(mape_by_month_year.keys(), \n",
    "                               key=lambda x: datetime.strptime(x, '%b_%Y'))\n",
    "    \n",
    "    sorted_mape = {month_year: mape_by_month_year[month_year] for month_year in sorted_month_years}\n",
    "    sorted_counts = {month_year: sample_counts[month_year] for month_year in sorted_month_years}\n",
    "    \n",
    "    return sorted_mape, sorted_counts, detailed_df, mape_by_year_month\n",
    "\n",
    "def get_monthly_aggregated_mape(predictions_df, date_col='ds', actual_col='y', forecast_col='forecast'):\n",
    "    \"\"\"\n",
    "    Calculate MAPE for each month (e.g., January, February) aggregated across all years.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    Same as calculate_monthly_mape\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with keys as month names (e.g., 'Jan', 'Feb') and values as MAPE for that month across all years\n",
    "    \"\"\"\n",
    "    # Ensure the date column is datetime\n",
    "    df = predictions_df.copy()\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df[date_col]):\n",
    "        df[date_col] = pd.to_datetime(df[date_col])\n",
    "    \n",
    "    # Create month column\n",
    "    df['month'] = df[date_col].dt.strftime('%b')\n",
    "    \n",
    "    # Calculate absolute percentage error for each row\n",
    "    df['abs_pct_error'] = np.abs((df[actual_col] - df[forecast_col]) / df[actual_col])\n",
    "    df['abs_pct_error'] = df['abs_pct_error'].replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    # Calculate MAPE for each month across all years\n",
    "    mape_by_month = {}\n",
    "    for month, group in df.groupby('month'):\n",
    "        mape = group['abs_pct_error'].mean() * 100\n",
    "        mape_by_month[month] = mape if not pd.isna(mape) else None\n",
    "    \n",
    "    # Sort months in calendar order\n",
    "    month_order = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "    sorted_mape = {month: mape_by_month.get(month) for month in month_order if month in mape_by_month}\n",
    "    \n",
    "    return sorted_mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import hashlib\n",
    "# import joblib\n",
    "# import json\n",
    "# import pandas as pd\n",
    "# import os\n",
    "\n",
    "\n",
    "# failed_models_log = f\"results_of_results/{RUN_NUM}_failed_models_log.txt\"\n",
    "\n",
    "# # Ensure directory exists\n",
    "# save_dir = f\"results_of_results/{RUN_NUM}\"\n",
    "# os.makedirs(save_dir, exist_ok=True)  # Creates the directory if it doesn't exist\n",
    "\n",
    "# # Function to create a unique hash based on model parameters\n",
    "# def generate_model_hash(model_name, transforms, lags, lag_transforms):\n",
    "#     \"\"\"Creates a short unique hash for the model based on its features & transforms.\"\"\"\n",
    "#     hash_input = f\"{model_name}_{transforms}_{lags}_{lag_transforms}\"\n",
    "#     return hashlib.md5(hash_input.encode()).hexdigest()[:8]  # Take only the first 8 characters\n",
    "\n",
    "# # Dictionary to store information about passed models\n",
    "# passed_models_info = {}\n",
    "# good_winter_models = []\n",
    "\n",
    "# # Process each dataset\n",
    "# for dataset_name, df in results.items():\n",
    "#     print(\"=\" * 80)\n",
    "#     print(f\"DATASET: {dataset_name}\")\n",
    "#     print(\"=\" * 80)\n",
    "\n",
    "#     sensor_id, train_months, train_label = extract_train_info(dataset_name)\n",
    "#     if sensor_id is None:\n",
    "#         print(\"Skipping dataset: Cannot determine sensor ID or train length.\")\n",
    "#         continue\n",
    "\n",
    "#     mape_columns_local = [c for c in MAPE_COLUMNS if c in df.columns]\n",
    "\n",
    "#     # --- (a) Filtering Criteria ---\n",
    "\n",
    "#     # 1. Remove models exceeding the early horizon threshold\n",
    "#     early_horizon_cols = [f\"test_{h}_days\" for h in EARLY_HORIZONS if f\"test_{h}_days\" in df.columns]\n",
    "#     mask_early_horizon = (df[early_horizon_cols] <= EARLY_HORIZON_THRESHOLD).all(axis=1)\n",
    "\n",
    "#     # 2. Ensure MAPE is good at 1/3 of train length\n",
    "#     one_third_horizon = int(train_months * 30 // 3)\n",
    "#     closest_horizon = min(HORIZONS, key=lambda x: abs(x - one_third_horizon))\n",
    "\n",
    "#     if f\"test_{closest_horizon}_days\" in df.columns:\n",
    "#         mask_one_third = df[f\"test_{closest_horizon}_days\"] <= ONE_THIRD_THRESHOLD\n",
    "#     else:\n",
    "#         mask_one_third = True\n",
    "\n",
    "#     # Apply filters\n",
    "#     df_filtered = df[mask_early_horizon & mask_one_third].copy()\n",
    "\n",
    "#     # --- (b) Extract best models and reconstruct `MLForecast` ---\n",
    "#     for _, row in df_filtered.iterrows():\n",
    "#         model_name = row[\"Model\"]\n",
    "#         transforms_str = row[\"Transforms\"]\n",
    "#         lags = row[\"Lags\"]\n",
    "#         lag_transforms_str = row[\"Lag Transforms\"]\n",
    "\n",
    "#         # Reverse transforms\n",
    "#         transform_objects = []\n",
    "#         if isinstance(transforms_str, str):\n",
    "#             for transform_str in transforms_str.split(\" | \"):\n",
    "#                 class_name, params = parse_transform_string(transform_str)\n",
    "\n",
    "#                 if class_name in transform_mapping:\n",
    "#                     transform_objects.append(transform_mapping[class_name](**params) if params else transform_mapping[class_name]())\n",
    "#                 else:\n",
    "#                     raise ValueError(f\"Unknown transform: {class_name}\")\n",
    "\n",
    "#         # Reverse lag transforms\n",
    "#         lag_transforms = get_lag_transforms(lag_transforms_str)\n",
    "\n",
    "#         # Prepare dataset\n",
    "#         sensor_id = str(sensor_id)\n",
    "#         scenario = scenarios_sensors[sensor_id][train_label]\n",
    "\n",
    "#         formatted_df = format_df_to_mlforecast(\n",
    "#             selected_sensors_df[['full_date', sensor_id]], 'full_date', sensor_id, unique_id=sensor_id\n",
    "#         )\n",
    "#         formatted_df = formatted_df[['ds', 'y', 'unique_id']]\n",
    "#         train_df, test_df = split_data(formatted_df, scenario)\n",
    "\n",
    "#         # Initialize MLForecast\n",
    "#         model_instance = model_mapping.get(model_name)\n",
    "#         if model_instance is None:\n",
    "#             raise ValueError(f\"Model {model_name} not found in model_mapping\")\n",
    "\n",
    "#         #Crucial: Convert lags to a list *before* passing it to MLForecast\n",
    "#         lags_list = ast.literal_eval(lags) if isinstance(lags, str) else lags\n",
    "\n",
    "#         fcst = MLForecast(\n",
    "#             models=[model_instance],\n",
    "#             freq='D',\n",
    "#             lags=lags_list,  # Use the converted list\n",
    "#             target_transforms=transform_objects,\n",
    "#             date_features=['dayofweek', 'month'],\n",
    "#             num_threads=1,\n",
    "#             lag_transforms=lag_transforms,\n",
    "#         )\n",
    "\n",
    "#         fcst.fit(train_df)\n",
    "#         predictions = fcst.predict(h=test_df.shape[0])\n",
    "#         test_df_copy = test_df.copy()\n",
    "#         test_df_copy['forecast'] = predictions[get_sgdreg_name(model_name)].values\n",
    "\n",
    "#         sorted_mape, sorted_counts, detailed_df, mape_by_year_month = calculate_monthly_mape(test_df_copy)\n",
    "\n",
    "#         covid_year = [2020]\n",
    "#         winter_months = [1, 2, 3, 10, 11, 12]  # Jan, Feb, Mar, Oct, Nov, Dec\n",
    "#         winter_mape_full = detailed_df[detailed_df['month'].isin(winter_months)]\n",
    "#         winter_mape = winter_mape_full[~winter_mape_full['year'].isin(covid_year)]\n",
    "#         mean_winter_mape = winter_mape['mape'].mean()\n",
    "#         year_winter_mean_mape = winter_mape_full.groupby('year')['mape'].mean().to_dict()\n",
    "\n",
    "#         if mean_winter_mape < 40:\n",
    "#             good_winter_models.append({\n",
    "#                 'dataset': dataset_name,\n",
    "#                 'model_name': model_name,\n",
    "#                 'transforms_str': transforms_str,\n",
    "#                 'lags': lags,\n",
    "#                 'lag_transforms_str': lag_transforms_str,\n",
    "#                 'mean_winter_mape': mean_winter_mape,\n",
    "#                 **year_winter_mean_mape\n",
    "#             })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_left = {}\n",
    "results_left['2_24M_train'] = results['2_24M_train'].copy()\n",
    "results_left['2_26M_train'] = results['2_26M_train'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_threshold_map = {\n",
    "    'run_12': [30, 30, 30], # no weights new models and lag transforms \n",
    "    'run_13': [30, 30, 30], # run_12 + weigths\n",
    "    # from here saving preds, months, cumulative\n",
    "    'run_14': [30, 30, 30], # roll 7\n",
    "    'run_15': [30, 30, 30], # resids from prophet_2_sensor\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATASET: 2_10M_train\n",
      "================================================================================\n",
      "(0, 18)\n",
      "================================================================================\n",
      "DATASET: 2_12M_train\n",
      "================================================================================\n",
      "(27, 18)\n",
      "================================================================================\n",
      "DATASET: 2_18M_train\n",
      "================================================================================\n",
      "(73, 18)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 164\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_filtered\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m    162\u001b[0m \u001b[38;5;66;03m# --- (b) Extract best models and reconstruct `MLForecast` ---\u001b[39;00m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;66;03m# Use joblib to parallelize the processing of rows\u001b[39;00m\n\u001b[1;32m--> 164\u001b[0m results_list \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)(  \u001b[38;5;66;03m# Use all available cores\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     delayed(process_row)(row, dataset_name, sensor_id, train_label, selected_sensors_df, scenarios_sensors, transform_mapping, model_mapping)\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m df_filtered\u001b[38;5;241m.\u001b[39miterrows()\n\u001b[0;32m    167\u001b[0m )\n\u001b[0;32m    169\u001b[0m \u001b[38;5;66;03m# Filter out None results (failed models or models not meeting criteria)\u001b[39;00m\n\u001b[0;32m    170\u001b[0m good_winter_models\u001b[38;5;241m.\u001b[39mextend(r \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results_list \u001b[38;5;28;01mif\u001b[39;00m r \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\PC314\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[1;32mc:\\Users\\PC314\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\PC314\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "import joblib\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import ast  # Import ast\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "failed_models_log = f\"results_of_results/{RUN_NUM}_failed_models_log.txt\"\n",
    "\n",
    "HORIZONS = [30, 60, 90, 120, 150, 180, 240, 300, 360, 480, 600, 720, 737]\n",
    "MAPE_COLUMNS = [f\"test_{h}_days\" for h in HORIZONS]\n",
    "EARLY_HORIZON_THRESHOLD = 30  # MAPE threshold for early horizons\n",
    "ONE_THIRD_THRESHOLD = 30       # Threshold for MAPE at 1/3 of training length\n",
    "ONE_THIRD_THRESHOLD_GENERAL = 30       # Threshold for MAPE at 1/3 of training length\n",
    "EARLY_HORIZONS = [30, 60, 90, 120]  # Horizons we check for early filtering\n",
    "SENSORS_TO_COMPARE = [2, 5, 6]  # Sensors that share training length criteria\n",
    "\n",
    "LONG_TRAIN_FILTER = False\n",
    "\n",
    "# Ensure directory exists\n",
    "save_dir = f\"results_of_results/{RUN_NUM}\"\n",
    "os.makedirs(save_dir, exist_ok=True)  # Creates the directory if it doesn't exist\n",
    "\n",
    "# Function to create a unique hash based on model parameters\n",
    "def generate_model_hash(model_name, transforms, lags, lag_transforms):\n",
    "    \"\"\"Creates a short unique hash for the model based on its features & transforms.\"\"\"\n",
    "    hash_input = f\"{model_name}_{transforms}_{lags}_{lag_transforms}\"\n",
    "    return hashlib.md5(hash_input.encode()).hexdigest()[:8]  # Take only the first 8 characters\n",
    "\n",
    "# Dictionary to store information about passed models\n",
    "passed_models_info = {}\n",
    "good_winter_models = []\n",
    "from mlforecast.lag_transforms import RollingMean, ExpandingMean, ExponentiallyWeightedMean\n",
    "def process_row(row, dataset_name, sensor_id, train_label, selected_sensors_df, scenarios_sensors, transform_mapping, model_mapping):\n",
    "    \"\"\"Processes a single row (model) from the filtered DataFrame.\"\"\"\n",
    "    model_name = row[\"Model\"]\n",
    "    transforms_str = row[\"Transforms\"]\n",
    "    lags = row[\"Lags\"]\n",
    "    lag_transforms_str = row[\"Lag Transforms\"]\n",
    "\n",
    "    # Reverse transforms\n",
    "    transform_objects = []\n",
    "    if isinstance(transforms_str, str):\n",
    "        for transform_str in transforms_str.split(\" | \"):\n",
    "            class_name, params = parse_transform_string(transform_str)\n",
    "\n",
    "            if class_name in transform_mapping:\n",
    "                transform_objects.append(transform_mapping[class_name](**params) if params else transform_mapping[class_name]())\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown transform: {class_name}\")\n",
    "\n",
    "    # Reverse lag transforms\n",
    "    lag_transforms = get_lag_transforms(lag_transforms_str)\n",
    "\n",
    "    # Prepare dataset\n",
    "    sensor_id = str(sensor_id)\n",
    "    scenario = scenarios_sensors[sensor_id][train_label]\n",
    "\n",
    "    if sensor_id not in scenarios_sensors:\n",
    "        return None  # Skip this model\n",
    "\n",
    "    formatted_df = format_df_to_mlforecast(\n",
    "        selected_sensors_df[['full_date', sensor_id]], 'full_date', sensor_id, unique_id=sensor_id\n",
    "    )\n",
    "    formatted_df = formatted_df[['ds', 'y', 'unique_id']]\n",
    "    train_df, test_df = split_data(formatted_df, scenario)\n",
    "\n",
    "    # Initialize MLForecast\n",
    "    model_instance = model_mapping.get(model_name)\n",
    "    if model_instance is None:\n",
    "        raise ValueError(f\"Model {model_name} not found in model_mapping\")\n",
    "\n",
    "    #Crucial: Convert lags to a list *before* passing it to MLForecast\n",
    "    lags_list = ast.literal_eval(lags) if isinstance(lags, str) else lags\n",
    "\n",
    "    fcst = MLForecast(\n",
    "        models=[model_instance],\n",
    "        freq='D',\n",
    "        lags=lags_list,  # Use the converted list\n",
    "        target_transforms=transform_objects,\n",
    "        date_features=['dayofweek', 'month'],\n",
    "        num_threads=1,\n",
    "        lag_transforms=lag_transforms,\n",
    "    )\n",
    "\n",
    "    fcst.fit(train_df)\n",
    "    predictions = fcst.predict(h=test_df.shape[0])\n",
    "    test_df_copy = test_df.copy()\n",
    "    test_df_copy['forecast'] = predictions[get_sgdreg_name(model_name)].values\n",
    "\n",
    "    sorted_mape, sorted_counts, detailed_df, mape_by_year_month = calculate_monthly_mape(test_df_copy)\n",
    "\n",
    "    covid_year = [2020]\n",
    "    winter_months = [1, 2, 3, 10, 11, 12]  # Jan, Feb, Mar, Oct, Nov, Dec\n",
    "    winter_mape_full = detailed_df[detailed_df['month'].isin(winter_months)]\n",
    "    winter_mape = winter_mape_full[~winter_mape_full['year'].isin(covid_year)]\n",
    "    mean_winter_mape = winter_mape['mape'].mean()\n",
    "    year_winter_mean_mape = winter_mape_full.groupby('year')['mape'].mean().to_dict()\n",
    "\n",
    "    if mean_winter_mape < 35 and year_winter_mean_mape[2020] < 30:\n",
    "        plot_data = {\n",
    "            'test_df_copy': test_df_copy,\n",
    "            'model_name': model_name,\n",
    "            'dataset_name': dataset_name\n",
    "        }\n",
    "        return {\n",
    "            'dataset': dataset_name,\n",
    "            'model_name': model_name,\n",
    "            'transforms_str': transforms_str,\n",
    "            'lags': lags,\n",
    "            'lag_transforms_str': lag_transforms_str,\n",
    "            'mean_winter_mape': mean_winter_mape,\n",
    "            **year_winter_mean_mape,\n",
    "            'sorted_mape': sorted_mape,\n",
    "            'plot_data': plot_data\n",
    "        }\n",
    "    else:\n",
    "        return None  # Model didn't meet criteria\n",
    "\n",
    "# Process each dataset\n",
    "for dataset_name, df in results.items():\n",
    "# for dataset_name, df in results_left.items():\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"DATASET: {dataset_name}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    sensor_id, train_months, train_label = extract_train_info(dataset_name)\n",
    "    if sensor_id is None:\n",
    "        print(\"Skipping dataset: Cannot determine sensor ID or train length.\")\n",
    "        continue\n",
    "    if train_months < 10:\n",
    "        continue\n",
    "    mape_columns_local = [c for c in MAPE_COLUMNS if c in df.columns]\n",
    "\n",
    "    # --- (a) Filtering Criteria ---\n",
    "\n",
    "    # 1. Remove models exceeding the early horizon threshold\n",
    "    local_early_horizons_threshold = EARLY_HORIZON_THRESHOLD\n",
    "    local_one_third_thresh = ONE_THIRD_THRESHOLD\n",
    "    # if train_months <= 12:\n",
    "    #     local_early_horizons_threshold = 25\n",
    "    #     local_one_third_thresh = 27\n",
    "    if LONG_TRAIN_FILTER and train_months >= 24:\n",
    "        local_early_horizons_threshold = 25\n",
    "        local_one_third_thresh = 27\n",
    "    early_horizon_cols = [f\"test_{h}_days\" for h in EARLY_HORIZONS if f\"test_{h}_days\" in df.columns]\n",
    "    mask_early_horizon = (df[early_horizon_cols] <= local_early_horizons_threshold).all(axis=1)\n",
    "\n",
    "    # 2. Ensure MAPE is good at 1/3 of train length\n",
    "    one_third_horizon = int(train_months * 30 // 3)\n",
    "    closest_horizon = min(HORIZONS, key=lambda x: abs(x - one_third_horizon))\n",
    "\n",
    "    if f\"test_{closest_horizon}_days\" in df.columns:\n",
    "        mask_one_third = df[f\"test_{closest_horizon}_days\"] <= local_one_third_thresh\n",
    "    else:\n",
    "        mask_one_third = True\n",
    "\n",
    "    # Apply filters\n",
    "    df_filtered = df[mask_early_horizon & mask_one_third].copy()\n",
    "    print(df_filtered.shape)\n",
    "    # --- (b) Extract best models and reconstruct `MLForecast` ---\n",
    "    # Use joblib to parallelize the processing of rows\n",
    "    results_list = Parallel(n_jobs=-1)(  # Use all available cores\n",
    "        delayed(process_row)(row, dataset_name, sensor_id, train_label, selected_sensors_df, scenarios_sensors, transform_mapping, model_mapping)\n",
    "        for _, row in df_filtered.iterrows()\n",
    "    )\n",
    "\n",
    "    # Filter out None results (failed models or models not meeting criteria)\n",
    "    good_winter_models.extend(r for r in results_list if r is not None)\n",
    "\n",
    "    for index, result in enumerate(results_list):\n",
    "        if result and 'plot_data' in result:\n",
    "            plot_data = result['plot_data']\n",
    "            test_df_copy = plot_data['test_df_copy']\n",
    "            model_name = plot_data['model_name']\n",
    "            dataset_name = plot_data['dataset_name']\n",
    "\n",
    "            # Generate the plot\n",
    "            print(result['sorted_mape'])\n",
    "            plt.figure()  # Create a new figure for each plot\n",
    "            plt.plot(test_df_copy['ds'], test_df_copy['y'], label='Actual')\n",
    "            plt.plot(test_df_copy['ds'], test_df_copy['forecast'], label='Forecast')\n",
    "            plt.title(f\"Forecast for {model_name}, {index}  in {dataset_name}\")\n",
    "            plt.legend()\n",
    "            plt.show()  \n",
    "\n",
    "\n",
    "# After processing all datasets, convert to DataFrame (optional)\n",
    "good_winter_models_df = pd.DataFrame(good_winter_models)\n",
    "print(good_winter_models_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_winter_models_df = pd.DataFrame(good_winter_models)\n",
    "\n",
    "def calculate_lags_name(lags_str):\n",
    "    try:\n",
    "        lags = eval(lags_str) if isinstance(lags_str, str) else lags_str\n",
    "        return f'{len(lags)}_{max(lags)}'\n",
    "    except:\n",
    "        return \"Error\"  # Handle potential errors, e.g., invalid string\n",
    "good_winter_models_df['lags_name'] = good_winter_models_df['lags'].apply(calculate_lags_name)\n",
    " \n",
    "good_winter_models_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
